{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict protein secondary structure with deep neural networks\n",
    "\n",
    "\n",
    "**Author**: YOUR_NAME\n",
    "\n",
    "**Due date**: March 30, 2025, 11:59 PM\n",
    "\n",
    "This assignment is similar to the [previous one](https://dinglab.io/chem193/homework/2-protein-secondary-structure/script/main.html#) but uses deep neural networks instead of simple linear models. It also uses larger datasets. The training and test datasets are provided in the text files [train.txt](https://tufts.box.com/s/y4t82o03hhf92zw6dik0x9r7v09qdyhs) and [test.txt](https://tufts.box.com/s/v4ypbippcsnifjlkd7vrp478bb7ue92l), respectively. The format of the datasets is the same as in the previous assignment.\n",
    "The task is to train a deep neural network model using the training dataset and predict the secondary structure of the proteins using their sequences in the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import equinox as eqx\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from sys import exit\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output of the neural network model\n",
    "Similary to the previous assignment, the model will predict the secondary structure of a protein using the sliding window approach. It predicts the secondary structure of a residue based on a window of residues centered at that residue. In the previous assignment, the window size was 15. In this assignment, the window size is a hyperparameter that you can choose. The default value is 31, but you can change it to any odd number.\n",
    "\n",
    "Assume that the window size is `k`. The input to the neural network model is a 1d array of `k` integers, each representing the index of that residue in the amino acid alphabet, `ACDEFGHIKLMNPQRSTVWY*`. For example, if `k = 5`, the sequence `ACACG` will be represented as `[0, 1, 0, 1, 5]`. The output of the model is a 1d array of 3 floats, each representing the logorihtm of the probability of the corresponding secondary structure, `helix`, `strand`, and `other`. For example, the output `[-1.0986123, -1.0986123, -1.0986123]` represents the probabilities `[0.333, 0.333, 0.333]` for the secondary structures `helix`, `strand`, and `other`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the training data\n",
    "Given the design of the model, the data in `train.txt` are not directly suitable for training. The data need to be processed to create the input and output pairs for the model. For each sequence in the training data, we extract all windows of size `k` and convert both the amino acid sequence and the secondary structure to the integer representation. The following two code cells show how it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you need to change `path_to_train` to the path of the train.txt file\n",
    "path_to_train = \"../data/train.txt\"\n",
    "\n",
    "## read data from train.txt\n",
    "train_data = {}\n",
    "\n",
    "## train_seq is a dictionary with the following structure:\n",
    "## train_seq[protein_name] = (sequence, secondary_structure),\n",
    "## where protein_name is the name of the protein, sequence is the amino acid sequence of the protein, and secondary_structure is the secondary structure of the protein as given in the train.txt file\n",
    "\n",
    "\n",
    "with open(path_to_train, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            name = line.strip()[1:]\n",
    "            train_data[name] = []\n",
    "        else:\n",
    "            train_data[name].append(line.strip())\n",
    "\n",
    "\n",
    "## here we split the data into training and validation data\n",
    "## we use 80% of the data for training and 20% for validation\n",
    "\n",
    "names = list(train_data.keys())\n",
    "names_validation = np.random.choice(names, int(len(names)*0.2), replace=False)\n",
    "\n",
    "valid_data = {name: train_data[name] for name in names_validation}\n",
    "for name in names_validation:\n",
    "    train_data.pop(name)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_per_seq(seq, secondary_structure, window_size=31):\n",
    "    \"\"\" Get windows for a single sequence\n",
    "\n",
    "    Args:\n",
    "        seq (str): amino acid sequence\n",
    "        secondary_structure (str): secondary structure\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        xs (np.array): input windows\n",
    "        ys (np.array): output windows\n",
    "\n",
    "    \"\"\"\n",
    "    ## amino acid order\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY*\"\n",
    "\n",
    "    seq = [amino_acids.index(s) for s in seq]\n",
    "    for _ in range(window_size // 2):\n",
    "        seq = [amino_acids.index(\"*\")] + seq + [amino_acids.index(\"*\")]\n",
    "\n",
    "    windows = []\n",
    "    for i in range(window_size//2, len(seq) - window_size//2):\n",
    "        x = np.array(seq[i - window_size//2 : i + window_size//2 + 1], dtype=np.int8)\n",
    "        y = int(secondary_structure[i - window_size//2])\n",
    "\n",
    "        windows.append((x, y))\n",
    "\n",
    "    xs = np.array([x for x, _ in windows], dtype=np.int8)\n",
    "    ys = np.array([y for _, y in windows], dtype=np.int8)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def get_windows(data, window_size=31):\n",
    "    \"\"\" Get windows for a dataset\"\n",
    "    \n",
    "    Args:\n",
    "        data (dict): dataset\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        xs (np.array): input windows\n",
    "        ys (np.array): output windows\n",
    "    \"\"\"\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for name in tqdm(data.keys()):\n",
    "        seq, ss = data[name]\n",
    "        x, y = get_windows_per_seq(seq, ss, window_size=window_size)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    xs = np.concatenate(xs)\n",
    "    ys = np.concatenate(ys)\n",
    "\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the default window size is 31 amino acids (15 on each side of the central amino acid)\n",
    "## you can change the window size by changing the value of the window_size variable\n",
    "window_size = 31\n",
    "\n",
    "## get windows for training and validation data\n",
    "train_xs, train_ys = get_windows(train_data, window_size)\n",
    "valid_xs, valid_ys = get_windows(valid_data, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network model\n",
    "You need to implement a deep neural network model using the `equinox` library. The input and output of the model are described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################  write your code for the following NeuralNetwork class (50 points)\n",
    "####################################################################################################\n",
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.layers = []\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\" Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (jnp.array): 1D array of integers representing amino acids in a window\n",
    "        \n",
    "        Returns:\n",
    "            logp (jnp.array): 1D array of 3 floats representing log-probabilities of secondary structure classes\n",
    "        \"\"\"\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To train the model, you need to finish the implementation of the `loss_fn` function. The function takes the model, a batch of input and output pairs, and returns the loss. The loss should be the same as the loss function used in the previous assignment. To monitor the training process, you also need to implement the three functions: `make_predictions`, and `compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that you could comment the @eqx.filter_jit decorator during the development of the loss function so that the error messages are more informative. Once the loss function is working, you can uncomment the decorator to speed up the training process. The same applies to the make_predictions function\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss_fn(model, xs, ys):\n",
    "    \"\"\" Loss function for a batch of windows\n",
    "\n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(batch_size, window_size)\n",
    "        ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        loss (jnp.array): the average loss over the batch\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################################################################\n",
    "    #### write your code loss function (10 points)\n",
    "    ###############################################################################################\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_predictions(model, xs):\n",
    "    \"\"\" Make predictions for a batch of windows\"\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(batch_size, window_size)\n",
    "\n",
    "    Returns:\n",
    "        predictions (jnp.array): 1D array of integers representing predicted secondary structure classes. Shape: (batch_size,)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ################################################\n",
    "    #### write your code loss function (10 points)\n",
    "    ################################################\n",
    "\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_average_loss(model, xs, ys, batch_size=1024*16):\n",
    "    \"\"\" Compute average loss for a dataset of windows by batching\n",
    "\n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(num_samples, window_size)\n",
    "        ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (num_samples,)\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        loss (float): average loss over the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_batches = len(xs) // batch_size + 1\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = xs[start_idx:end_idx]\n",
    "        batch_ys = ys[start_idx:end_idx]\n",
    "\n",
    "        loss = loss_fn(model, batch_xs, batch_ys)\n",
    "        total_loss += loss * batch_xs.shape[0]\n",
    "    loss = total_loss / len(xs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_accuracy(model, xs, ys, batch_size=1024*16):\n",
    "    \"\"\" Compute accuracy for a dataset of windows by batching\"\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(num_samples, window_size)\n",
    "        ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (num_samples,)\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): accuracy over the dataset\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ####  write your code loss function (10 points)  ####\n",
    "    #####################################################\n",
    "\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using stochastic gradient descent (SGD) with the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize the model \n",
    "key = jr.PRNGKey(0)\n",
    "model = NeuralNetwork(key)\n",
    "\n",
    "\n",
    "## initialize the optimizer\n",
    "## the learning rate is set to 0.001\n",
    "optim = optax.adamw(0.001)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "## training loop\n",
    "## Note that you could comment the @eqx.filter_jit decorator during the development so that the error messages are more informative. Once you are done, you can uncomment the decorator to speed up the training process\n",
    "@eqx.filter_jit\n",
    "def make_step(model, batch_xs, batch_ys, opt_state):\n",
    "    \"\"\" Make a single optimization step using a batch of windows\n",
    "\n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        batch_xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(batch_size, window_size)\n",
    "        batch_ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (batch_size,)\n",
    "        opt_state (optax.OptState): optimizer state\n",
    "\n",
    "    Returns:\n",
    "        model (NeuralNetwork): updated neural network model\n",
    "        opt_state (optax.OptState): updated optimizer state\n",
    "        loss_value (float): loss value for the batch    \n",
    "    \"\"\"\n",
    "\n",
    "    loss_value, grads = eqx.filter_value_and_grad(loss_fn)(model, batch_xs, batch_ys)\n",
    "    updates, opt_state = optim.update(\n",
    "        grads, opt_state, eqx.filter(model, eqx.is_array)\n",
    "    )    \n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell shows how to train the model using the Adam optimizer. We monitor the training process by computing the loss and accuracy on the training and validation datasets after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the number of epochs and batch size\n",
    "## you can change both as needed\n",
    "num_epochs = 50\n",
    "batch_size = 1024 * 16\n",
    "\n",
    "train_loss_record = []\n",
    "valid_loss_record = []\n",
    "train_accuracy_record = []\n",
    "valid_accuracy_record = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    idx = np.arange(len(train_xs))\n",
    "    np.random.shuffle(idx)\n",
    "    train_xs = train_xs[idx]\n",
    "    train_ys = train_ys[idx]\n",
    "\n",
    "    num_batches = len(train_xs) // batch_size\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = train_xs[start_idx:end_idx]\n",
    "        batch_ys = train_ys[start_idx:end_idx]\n",
    "\n",
    "        model, opt_state, loss_value = make_step(model, batch_xs, batch_ys, opt_state)\n",
    "\n",
    "        if idx_batch % 10 == 0:\n",
    "            print(f\"epoch {epoch:5>d}, batch {idx_batch:5>d}, train_loss {loss_value:5.3f}\")\n",
    "\n",
    "    \n",
    "    train_loss = compute_average_loss(model, train_xs, train_ys)\n",
    "    train_accuracy = compute_accuracy(model, train_xs, train_ys)\n",
    "\n",
    "    valid_loss = compute_average_loss(model, valid_xs, valid_ys)\n",
    "    valid_accuracy = compute_accuracy(model, valid_xs, valid_ys)\n",
    "    \n",
    "    print(f\"epoch {epoch:>5d}, train_loss {train_loss:5.3f}, valid_loss {valid_loss:5.3f}, train_accuracy {train_accuracy:7.2%}, valid_accuracy {valid_accuracy:7.2%}\")\n",
    "\n",
    "    train_loss_record.append(train_loss)\n",
    "    valid_loss_record.append(valid_loss)\n",
    "    train_accuracy_record.append(train_accuracy)\n",
    "    valid_accuracy_record.append(valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and accuracy curves during training\n",
    "To visualize the training process, we plot the loss and accuracy curves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(train_loss_record, label=\"train_loss\")\n",
    "plt.plot(valid_loss_record, label=\"valid_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(train_accuracy_record, label=\"train_accuracy\")\n",
    "plt.plot(valid_accuracy_record, label=\"valid_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the loss and accuracy curves, answer the following question**: (10 points)\n",
    "\n",
    "As the number of epochs increases, what happens to the loss and accuracy on the training and validation datasets? Why the behavior of the loss/accuracy curves is different on the training and validation datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer to the above question_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model with the highest validation accuracy\n",
    "To avoid overfitting, we save the model with the highest validation accuracy after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-initialize the model \n",
    "key = jr.PRNGKey(100)\n",
    "model = NeuralNetwork(key)\n",
    "\n",
    "\n",
    "## initialize the optimizer\n",
    "## the learning rate is set to 0.001\n",
    "optim = optax.adamw(0.001)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "\n",
    "## you can change the number of epochs and batch size as needed\n",
    "num_epochs = 50\n",
    "batch_size = 1024 * 16\n",
    "\n",
    "highest_valid_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    idx = np.arange(len(train_xs))\n",
    "    np.random.shuffle(idx)\n",
    "    train_xs = train_xs[idx]\n",
    "    train_ys = train_ys[idx]\n",
    "\n",
    "    num_batches = len(train_xs) // batch_size\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = train_xs[start_idx:end_idx]\n",
    "        batch_ys = train_ys[start_idx:end_idx]\n",
    "\n",
    "        model, opt_state, loss_value = make_step(model, batch_xs, batch_ys, opt_state)\n",
    "\n",
    "        if idx_batch % 10 == 0:\n",
    "            print(f\"epoch {epoch:5>d}, batch {idx_batch:5>d}, train_loss {loss_value:5.3f}\")\n",
    "\n",
    "    \n",
    "    train_loss = compute_average_loss(model, train_xs, train_ys)\n",
    "    train_accuracy = compute_accuracy(model, train_xs, train_ys)\n",
    "\n",
    "    valid_loss = compute_average_loss(model, valid_xs, valid_ys)\n",
    "    valid_accuracy = compute_accuracy(model, valid_xs, valid_ys)\n",
    "    \n",
    "    print(f\"epoch {epoch:5>d}, train_loss {train_loss:5.3f}, valid_loss {valid_loss:5.3f}, train_accuracy {train_accuracy:7.2%}, valid_accuracy {valid_accuracy:7.2%}\")\n",
    "\n",
    "    if valid_accuracy > highest_valid_accuracy:\n",
    "        highest_valid_accuracy = valid_accuracy        \n",
    "        eqx.tree_serialise_leaves(\"../output/model-with-highest-valid-accuracy.eqx\", model)\n",
    "        print(f\"model saved with highest valid accuracy: {highest_valid_accuracy:7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test data\n",
    "After the model is trained, you make predictions on the test sequences using saved model that has the highest validation accuracy. The test sequences are provided in the file `test.txt`. You need to use the trained model to predict the secondary structure of all residues in the test sequences. The predictions should be written to a text file named `predictions.txt` and its format should be the same as the file `train.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model with the highest validation accuracy\n",
    "key = jr.PRNGKey(100)\n",
    "model = NeuralNetwork(key)\n",
    "model = eqx.tree_deserialise_leaves(\"../output/model-with-highest-valid-accuracy.eqx\", model)\n",
    "\n",
    "###########################################################################\n",
    "#### write your code here for the task described above (10 points) ########\n",
    "###########################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "1. You need to create a folder named `assignment-3-protein-secondary-structure-nn` under the OneDrive folder that has been shared with you.\n",
    "2. Complete the code and answer the questions as described above in this Jupyter notebook. Make sure to save your work and name your Jupyter notebook as `main.ipynb`.\n",
    "3. Upload the `main.ipynb`, `pyproject.toml`, and `test_prediction.txt` files to the `assignment-3-protein-secondary-structure-nn` folder that you created in step 1.\n",
    "\n",
    "**Note:** Please make sure the names of the folder and files are exactly as instructed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
