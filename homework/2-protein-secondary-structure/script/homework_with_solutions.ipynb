{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict protein secondary structure\n",
    "\n",
    "In this assignment you will predict the secondary structure of proteins based on their amino acid sequences. The [secondary structure of a protein](https://en.wikipedia.org/wiki/Protein_secondary_structure) is the local spatial arrangement of its backbone atoms. It arises from the hydrogen bonds between the backbone atoms and is the building block of proteins' 3D structure. The most common types of secondary structures are alpha helices, beta sheets, and coils. Given a protein's 3D structure, the secondary structure can be determined from the coordinates of its atoms. Depending on the determination method, the secondary structure can be classified into more or fewer types.\n",
    "\n",
    "Here you will use a dataset of proteins with known secondary structures that were assigned by the [DSSP](https://pdb-redo.eu/dssp/about) method based on their 3D structures. Although DSSP assigns eight types of secondary structures, the assignment simplifies them into three types: helix (including types G, H, and I), strand (including types B and E) and coil (including types T, S, ., and P). The training and test datasets are provided in the text files [train.txt](../../../_static/protein-secondary-structure/train.txt) and [test.txt](../../../_static/protein-secondary-structure/test.txt), respectively. Each line starting with a greater-than symbol `>` contains the name of the protein. In the training dataset, the two lines following the name contain the amino acid sequence and the secondary structure, respectively. In the test dataset, only the amino acid sequence is provided. The amino acid sequence is represented by the one-letter code. The secondary structure is represented as a string of 0 (helix), 1 (strand), and 2 (other).\n",
    "\n",
    "The task is to train a machine learning model using the training dataset and predict the secondary structure of the proteins using their sequences in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Different proteins have different numbers of amino acids. If the model were to predict the secondary structure of all amino acids in a protein at once, it would have to handle sequences of different lengths. Although this is possible, this assignment will use a simpler approach. The model will predict the secondary structure of an amino acid based on the amino acids before it and after it. This way, the model will have a fixed-size input and output. Specifically, the model will use as input a sequence of 15 amino acids and predict the secondary structure of the central amino acid. The model will slide this window along the sequence to predict the secondary structure of all amino acids in a protein. For the amino acids near the beginning and end of the sequence where the window extends beyond the sequence, the model will use a special padding token `*` to complete the window. \n",
    "\n",
    "As an example, consider the amino acid sequence `SIVAGYEVVGSSSASELLSAIEHVAEKA`. To predict the secondary structure of the starting amino acid `S`, the model will use as input the window `*******SIVAGYEV`. For the second amino acid `I`, the model will use the window `******SIVAGYEVV`. For the residue `A` at position 14, the model will use the window \n",
    "`EVVGSSSASELLSAI`. This process will continue until the model predicts the secondary structure of the last amino acid `A` using the window `IEHVAEKA*******`\n",
    "\n",
    "Therefore the input to the model will be a sequence of 15 amino acids and the output will be the secondary structure of the central amino acid. The secondary structure of an amino acid is a categorical variable $y$ with three classes: 0 (helix), 1 (strand), and 2 (other). Therefore, the model will perform a multi-class classification. The input is a sequence of strings, so they need to be converted to numerical values. The simplest way to do this is to use [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics). In one-hot encoding, each amino acid is represented by a vector of zeros with a one at the position corresponding to the amino acid. For example, if all amino acids (including the padding token `*`) are represented by the list `['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '*']`, then the amino acid `A` is represented by the 21-dimensional vector `[1, 0, ..., 0]`, the amino acid `C` is represented by `[0, 1, 0, ..., 0]`, and so on. The padding token `*` is represented by `[0, 0, ..., 1]`. Because the input is a sequence of 15 amino acids, the input $x$ to the model is a vector of length $15 \\times 21 = 315$.\n",
    "\n",
    "The model will be a multi-class logistic regression model, as covered in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "import chex\n",
    "import optax.tree_utils as otu\n",
    "from typing import NamedTuple\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the training data\n",
    "Given the design of the model, the data in `train.txt` are not directly suitable for training. The data need to be processed to create the input and output pairs for the model. For each sequence in the training data, you need to extract all windows of 15 amino acids and the corresponding secondary structure of the central amino acid. Then you need to convert the amino acids to one-hot encoding. Take the sequence `SIVAGYEVVGSSSASELLSAIEHVAEKA` and its secondary structure `2111111111222000000000220000` as an example. The first window is `*******SIVAGYEV` and the corresponding secondary structure of the central amino acid `S` is 2. The second window is `******SIVAGYEVV` and the secondary structure of the central amino acid `I` is 1. This process will continue until the last window `IEHVAEKA*******` and the secondary structure of the central amino acid `A` is 0. Each window and the corresponding secondary structure will be an input-output pair for the model. You need to process all sequences in the training data to create the input-output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = '../data/train.txt'\n",
    "\n",
    "\n",
    "## read data from train.txt\n",
    "train_seq = {}\n",
    "\n",
    "## finish the code below such that train_seq is a dictionary with the following structure:\n",
    "## train_seq[protein_name] = (sequence, secondary_structure), \n",
    "## where protein_name is the name of the protein, sequence is the amino acid sequence of the protein, and secondary_structure is the secondary structure of the protein as given in the train.txt file\n",
    "\n",
    "\n",
    "with open(path_to_train, 'r') as f:\n",
    "    ############################################\n",
    "    #### write your code (5 points) #######\n",
    "    ############################################\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            name = line.strip()[1:]\n",
    "            train_seq[name] = []\n",
    "        else:\n",
    "            train_seq[name].append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(seq, secondary_structure):\n",
    "    \"\"\" Extract windows of size 15 from the sequence and secondary structure of a protein\n",
    "\n",
    "    Args:\n",
    "        seq: str, the amino acid sequence of the protein\n",
    "        secondary_structure: str, the secondary structure of the protein\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: each tuple contains (x, y), where x (15x21=315 dimensional) is the one-hot encoding of the amino acid sequence of length 15, and y is the secondary structure of the central amino acid in the window\n",
    "    \"\"\"\n",
    "\n",
    "    ## amino acid order for one-hot encoding\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY*\"\n",
    "\n",
    "    ############################################\n",
    "    #### write your code (5 points) ############\n",
    "    ############################################\n",
    "\n",
    "    def one_hot_encoding(residue):\n",
    "        return np.array([1 if residue == aa else 0 for aa in amino_acids], dtype=np.uint8)\n",
    "\n",
    "    windows = []\n",
    "    for i, r in enumerate(seq):\n",
    "        if i < 7:\n",
    "            segment = \"*\"*(7-i) + seq[:i+8]\n",
    "        elif i > len(seq) - 8:\n",
    "            segment = seq[i-7:] + \"*\"*(7-(len(seq)-1-i))\n",
    "        else:\n",
    "            segment = seq[i-7:i+8]        \n",
    "\n",
    "        x = np.array([one_hot_encoding(aa) for aa in segment])\n",
    "        x = x.reshape(-1)\n",
    "        y = secondary_structure[i]\n",
    "\n",
    "        windows.append((x, y))\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code in the cell below uses the function `get_windows` to process every sequence in the training data and stack both `x` and `y` in numpy arrays: `train_xs` and `train_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4776/4776 [00:43<00:00, 109.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#### The following code is provided to you and you need to understand it ####\n",
    "\n",
    "## get windows for all proteins in the training set\n",
    "## stack all windows in train_xs and train_ys\n",
    "\n",
    "train_xs = []\n",
    "train_ys = []\n",
    "\n",
    "for name, (seq, ss) in tqdm(train_seq.items()):\n",
    "    windows = get_windows(seq, ss)\n",
    "    for x, y in windows:\n",
    "        train_xs.append(x)\n",
    "        train_ys.append(y)\n",
    "\n",
    "train_xs = np.array(train_xs)\n",
    "train_ys = np.array(train_ys, dtype=np.int64)\n",
    "\n",
    "train_xs = np.hstack((np.ones((train_xs.shape[0], 1)), train_xs))\n",
    "\n",
    "assert train_xs.shape[0] == train_ys.shape[0]\n",
    "assert train_xs.shape[1] == 315 + 1\n",
    "assert np.all((train_xs == 0) | (train_xs == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code in this cell builds a optimizer, `fmin_lbfgs`, using the `optax` library. You do not need to modify this code. The optimization step will use this optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_opt(init_params, fun, opt, max_iter, tol):\n",
    "    value_and_grad_fun = optax.value_and_grad_from_state(fun)\n",
    "\n",
    "    def step(carry):\n",
    "        params, state = carry\n",
    "        value, grad = value_and_grad_fun(params, state=state)\n",
    "        updates, state = opt.update(\n",
    "            grad, state, params, value=value, grad=grad, value_fn=fun\n",
    "        )\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, state\n",
    "\n",
    "    def continuing_criterion(carry):\n",
    "        _, state = carry\n",
    "        iter_num = otu.tree_get(state, \"count\")\n",
    "        grad = otu.tree_get(state, \"grad\")\n",
    "        err = otu.tree_l2_norm(grad)\n",
    "        return (iter_num == 0) | ((iter_num < max_iter) & (err >= tol))\n",
    "\n",
    "    init_carry = (init_params, opt.init(init_params))\n",
    "    final_params, final_state = jax.lax.while_loop(\n",
    "        continuing_criterion, step, init_carry\n",
    "    )\n",
    "    return final_params, final_state\n",
    "\n",
    "\n",
    "class _InfoState(NamedTuple):\n",
    "    iter_num: chex.Numeric\n",
    "\n",
    "\n",
    "def _print_info():\n",
    "    def init_fn(params):\n",
    "        del params\n",
    "        return _InfoState(iter_num=0)\n",
    "\n",
    "    def update_fn(updates, state, params, *, value, grad, **extra_args):\n",
    "        del params, extra_args\n",
    "\n",
    "        jax.debug.print(\n",
    "            \"Iteration: {i:>5}, Value: {v:>6.3e}, Gradient norm: {e:>6.3e}\",\n",
    "            i=state.iter_num,\n",
    "            v=value,\n",
    "            e=otu.tree_l2_norm(grad),\n",
    "        )\n",
    "        return updates, _InfoState(iter_num=state.iter_num + 1)\n",
    "\n",
    "    return optax.GradientTransformationExtraArgs(init_fn, update_fn)\n",
    "\n",
    "def fmin_lbfgs(initial_params, fun, max_iter, tol):\n",
    "    opt = optax.chain(_print_info(), optax.lbfgs())\n",
    "    return _run_opt(initial_params, fun, opt, max_iter, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function for softmax regression\n",
    "In the following, you need to complete a few functions to compute the loss for the softmax regression model.\n",
    "The parameter of the loss function `loss_fn` is `theta` which has shape `(2, 316)`, although we have three classes. The first row of `theta` corresponds to the weights for the class 0 and the second row corresponds to the weights for the class 1. Why do we not have a row for class 2? This is because the degenaracy of the softmax function, i.e., the softmax function is invariant to adding a constant to the scores. Therefore, we can fix one of the scores to zero. In this case, we fix the score of the last class to zero by setting the `theta` parameter for the last class to zero. This is why the shape of `theta` is `(2, 316)` and not `(3, 316)`. Given a `theta` of shape `(2, 316)`, we could always add a row of zeros to make it `(3, 316)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_theta(theta):\n",
    "    \"\"\"Expand theta by appending a row of zeros at the end\n",
    "\n",
    "    Args:\n",
    "        theta: jnp.ndarray, the parameters of the model with shape (2, 316)\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: the expanded theta with shape (3, 316) where the last row is all zeros\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return jnp.vstack((theta, jnp.zeros_like(theta[0])))\n",
    "\n",
    "\n",
    "def log_likelihood_per_sample(theta, x, y):\n",
    "    \"\"\"Compute the log-likelihood of a single sample\n",
    "\n",
    "    Args:\n",
    "        theta: jnp.ndarray, the parameters of the model with shape (2, 316)\n",
    "        x: jnp.ndarray, the input features of one sample\n",
    "        y: int, the target label of the sample\n",
    "\n",
    "    Returns:\n",
    "        float: the log-likelihood of the sample\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    theta_expanded = expand_theta(theta)\n",
    "    logit = jnp.matmul(theta_expanded, x)\n",
    "    logprob = logit - jax.scipy.special.logsumexp(logit)\n",
    "    return logprob[y]\n",
    "\n",
    "\n",
    "def log_likelihood_data(theta, xs, ys):\n",
    "    \"\"\"Compute the log-likelihood of the data by using log_likelihood_per_sample and jax.vmap\n",
    "\n",
    "    Args:\n",
    "        theta: jnp.ndarray, the parameters of the model with shape (2, 316)\n",
    "        xs: jnp.ndarray, the input features of all samples with shape (n, 316)\n",
    "        ys: jnp.ndarray, the target labels of all samples with shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: the log-likelihood of all samples with shape (n,)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return jnp.sum(\n",
    "        jax.vmap(log_likelihood_per_sample, in_axes=(None, 0, 0))(theta, xs, ys)\n",
    "    )\n",
    "\n",
    "\n",
    "def loss_fn(theta):\n",
    "    \"\"\"the loss function is defined as the mean negative log-likelihood of the samples,\n",
    "\n",
    "    Args:\n",
    "        theta: jnp.ndarray, the parameters of the model with shape (2, 316)\n",
    "\n",
    "    Returns:\n",
    "        float: the mean negative log-likelihood of the samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return -log_likelihood_data(theta, train_xs, train_ys) / train_xs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "After the data are processed properly and the loss function is defined, it is time to train the model. Thanks to the automatic differentiation provided by JAX, you do not need to write the function to compute the gradient of the loss function. You only need to provide the loss function to the optimizer along with the initial parameters and stopping criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value: 1.10e+00 Initial gradient norm: 1.93e-01\n",
      "Iteration:     0, Value: 1.099e+00, Gradient norm: 1.934e-01\n",
      "Iteration:     1, Value: 1.070e+00, Gradient norm: 1.148e-01\n",
      "Iteration:     2, Value: 1.040e+00, Gradient norm: 8.036e-02\n",
      "Iteration:     3, Value: 1.013e+00, Gradient norm: 8.489e-02\n",
      "Iteration:     4, Value: 9.647e-01, Gradient norm: 1.042e-01\n",
      "Iteration:     5, Value: 9.065e-01, Gradient norm: 6.281e-02\n",
      "Iteration:     6, Value: 8.863e-01, Gradient norm: 3.913e-02\n",
      "Iteration:     7, Value: 8.799e-01, Gradient norm: 2.339e-02\n",
      "Iteration:     8, Value: 8.691e-01, Gradient norm: 4.872e-02\n",
      "Iteration:     9, Value: 8.601e-01, Gradient norm: 5.991e-02\n",
      "Iteration:    10, Value: 8.491e-01, Gradient norm: 2.996e-02\n",
      "Iteration:    11, Value: 8.465e-01, Gradient norm: 8.120e-03\n",
      "Iteration:    12, Value: 8.458e-01, Gradient norm: 1.338e-02\n",
      "Iteration:    13, Value: 8.450e-01, Gradient norm: 1.955e-02\n",
      "Iteration:    14, Value: 8.439e-01, Gradient norm: 1.869e-02\n",
      "Iteration:    15, Value: 8.433e-01, Gradient norm: 1.653e-02\n",
      "Iteration:    16, Value: 8.424e-01, Gradient norm: 5.731e-03\n",
      "Iteration:    17, Value: 8.423e-01, Gradient norm: 4.212e-03\n",
      "Iteration:    18, Value: 8.420e-01, Gradient norm: 2.600e-03\n",
      "Iteration:    19, Value: 8.419e-01, Gradient norm: 5.850e-03\n",
      "Iteration:    20, Value: 8.417e-01, Gradient norm: 2.304e-03\n",
      "Iteration:    21, Value: 8.416e-01, Gradient norm: 1.766e-03\n",
      "Iteration:    22, Value: 8.415e-01, Gradient norm: 1.747e-03\n",
      "Iteration:    23, Value: 8.414e-01, Gradient norm: 2.547e-03\n",
      "Iteration:    24, Value: 8.413e-01, Gradient norm: 3.240e-03\n",
      "Iteration:    25, Value: 8.413e-01, Gradient norm: 1.747e-03\n",
      "Iteration:    26, Value: 8.412e-01, Gradient norm: 1.528e-03\n",
      "Iteration:    27, Value: 8.412e-01, Gradient norm: 1.722e-03\n",
      "Iteration:    28, Value: 8.411e-01, Gradient norm: 2.073e-03\n",
      "Iteration:    29, Value: 8.411e-01, Gradient norm: 2.958e-03\n",
      "Iteration:    30, Value: 8.410e-01, Gradient norm: 1.153e-03\n",
      "Iteration:    31, Value: 8.410e-01, Gradient norm: 8.344e-04\n",
      "Iteration:    32, Value: 8.410e-01, Gradient norm: 1.142e-03\n",
      "Iteration:    33, Value: 8.409e-01, Gradient norm: 1.602e-03\n",
      "Iteration:    34, Value: 8.409e-01, Gradient norm: 1.012e-03\n",
      "Iteration:    35, Value: 8.409e-01, Gradient norm: 9.810e-04\n",
      "Iteration:    36, Value: 8.408e-01, Gradient norm: 2.066e-03\n",
      "Iteration:    37, Value: 8.408e-01, Gradient norm: 1.594e-03\n",
      "Iteration:    38, Value: 8.408e-01, Gradient norm: 1.433e-03\n",
      "Iteration:    39, Value: 8.408e-01, Gradient norm: 1.083e-03\n",
      "Iteration:    40, Value: 8.407e-01, Gradient norm: 7.730e-04\n",
      "Iteration:    41, Value: 8.407e-01, Gradient norm: 7.597e-04\n",
      "Iteration:    42, Value: 8.407e-01, Gradient norm: 1.602e-03\n",
      "Iteration:    43, Value: 8.407e-01, Gradient norm: 1.092e-03\n",
      "Iteration:    44, Value: 8.406e-01, Gradient norm: 6.678e-04\n",
      "Iteration:    45, Value: 8.406e-01, Gradient norm: 1.844e-03\n",
      "Iteration:    46, Value: 8.406e-01, Gradient norm: 6.048e-04\n",
      "Iteration:    47, Value: 8.406e-01, Gradient norm: 4.934e-04\n",
      "Iteration:    48, Value: 8.406e-01, Gradient norm: 8.211e-04\n",
      "Iteration:    49, Value: 8.406e-01, Gradient norm: 1.796e-03\n",
      "Iteration:    50, Value: 8.406e-01, Gradient norm: 6.606e-04\n",
      "Iteration:    51, Value: 8.406e-01, Gradient norm: 5.772e-04\n",
      "Iteration:    52, Value: 8.406e-01, Gradient norm: 6.650e-04\n",
      "Iteration:    53, Value: 8.406e-01, Gradient norm: 6.779e-04\n",
      "Iteration:    54, Value: 8.406e-01, Gradient norm: 2.070e-03\n",
      "Iteration:    55, Value: 8.406e-01, Gradient norm: 8.210e-04\n",
      "Iteration:    56, Value: 8.406e-01, Gradient norm: 6.236e-04\n",
      "Iteration:    57, Value: 8.405e-01, Gradient norm: 7.373e-04\n",
      "Iteration:    58, Value: 8.405e-01, Gradient norm: 1.274e-03\n",
      "Iteration:    59, Value: 8.405e-01, Gradient norm: 7.354e-04\n",
      "Iteration:    60, Value: 8.405e-01, Gradient norm: 5.904e-04\n",
      "Iteration:    61, Value: 8.405e-01, Gradient norm: 1.367e-03\n",
      "Iteration:    62, Value: 8.405e-01, Gradient norm: 9.027e-04\n",
      "Iteration:    63, Value: 8.405e-01, Gradient norm: 6.328e-04\n",
      "Iteration:    64, Value: 8.405e-01, Gradient norm: 6.074e-04\n",
      "Iteration:    65, Value: 8.404e-01, Gradient norm: 6.899e-04\n",
      "Iteration:    66, Value: 8.404e-01, Gradient norm: 1.670e-03\n",
      "Iteration:    67, Value: 8.404e-01, Gradient norm: 1.566e-03\n",
      "Iteration:    68, Value: 8.404e-01, Gradient norm: 4.747e-04\n",
      "Iteration:    69, Value: 8.404e-01, Gradient norm: 4.076e-04\n",
      "Iteration:    70, Value: 8.404e-01, Gradient norm: 9.100e-04\n",
      "Iteration:    71, Value: 8.404e-01, Gradient norm: 6.234e-04\n",
      "Iteration:    72, Value: 8.404e-01, Gradient norm: 2.556e-04\n",
      "Iteration:    73, Value: 8.404e-01, Gradient norm: 2.770e-04\n",
      "Iteration:    74, Value: 8.404e-01, Gradient norm: 7.407e-04\n",
      "Iteration:    75, Value: 8.404e-01, Gradient norm: 2.578e-04\n",
      "Iteration:    76, Value: 8.404e-01, Gradient norm: 2.797e-04\n",
      "Iteration:    77, Value: 8.404e-01, Gradient norm: 2.941e-04\n",
      "Iteration:    78, Value: 8.404e-01, Gradient norm: 2.657e-04\n",
      "Iteration:    79, Value: 8.404e-01, Gradient norm: 8.648e-04\n",
      "Iteration:    80, Value: 8.404e-01, Gradient norm: 4.692e-04\n",
      "Iteration:    81, Value: 8.404e-01, Gradient norm: 3.789e-04\n",
      "Iteration:    82, Value: 8.404e-01, Gradient norm: 6.242e-04\n",
      "Iteration:    83, Value: 8.404e-01, Gradient norm: 8.296e-04\n",
      "Iteration:    84, Value: 8.404e-01, Gradient norm: 1.321e-03\n",
      "Iteration:    85, Value: 8.403e-01, Gradient norm: 7.191e-04\n",
      "Iteration:    86, Value: 8.403e-01, Gradient norm: 4.739e-04\n",
      "Iteration:    87, Value: 8.403e-01, Gradient norm: 8.479e-04\n",
      "Iteration:    88, Value: 8.403e-01, Gradient norm: 6.893e-04\n",
      "Iteration:    89, Value: 8.403e-01, Gradient norm: 9.595e-04\n",
      "Iteration:    90, Value: 8.403e-01, Gradient norm: 1.622e-03\n",
      "Iteration:    91, Value: 8.403e-01, Gradient norm: 5.327e-04\n",
      "Iteration:    92, Value: 8.403e-01, Gradient norm: 3.796e-04\n",
      "Iteration:    93, Value: 8.403e-01, Gradient norm: 4.027e-04\n",
      "Iteration:    94, Value: 8.403e-01, Gradient norm: 5.908e-04\n",
      "Iteration:    95, Value: 8.403e-01, Gradient norm: 8.341e-04\n",
      "Iteration:    96, Value: 8.403e-01, Gradient norm: 5.092e-04\n",
      "Iteration:    97, Value: 8.403e-01, Gradient norm: 4.405e-04\n",
      "Iteration:    98, Value: 8.403e-01, Gradient norm: 3.326e-04\n",
      "Iteration:    99, Value: 8.403e-01, Gradient norm: 7.549e-04\n",
      "Iteration:   100, Value: 8.403e-01, Gradient norm: 3.595e-04\n",
      "Iteration:   101, Value: 8.403e-01, Gradient norm: 2.821e-04\n",
      "Iteration:   102, Value: 8.403e-01, Gradient norm: 2.886e-04\n",
      "Iteration:   103, Value: 8.403e-01, Gradient norm: 6.521e-04\n",
      "Iteration:   104, Value: 8.403e-01, Gradient norm: 4.798e-04\n",
      "Iteration:   105, Value: 8.403e-01, Gradient norm: 2.308e-04\n",
      "Iteration:   106, Value: 8.403e-01, Gradient norm: 2.211e-04\n",
      "Iteration:   107, Value: 8.403e-01, Gradient norm: 2.727e-04\n",
      "Iteration:   108, Value: 8.403e-01, Gradient norm: 3.806e-04\n",
      "Iteration:   109, Value: 8.402e-01, Gradient norm: 1.798e-04\n",
      "Iteration:   110, Value: 8.402e-01, Gradient norm: 1.750e-04\n",
      "Iteration:   111, Value: 8.402e-01, Gradient norm: 2.204e-04\n",
      "Iteration:   112, Value: 8.402e-01, Gradient norm: 4.646e-04\n",
      "Iteration:   113, Value: 8.402e-01, Gradient norm: 2.720e-04\n",
      "Iteration:   114, Value: 8.402e-01, Gradient norm: 6.739e-04\n",
      "Iteration:   115, Value: 8.402e-01, Gradient norm: 1.430e-04\n",
      "Iteration:   116, Value: 8.402e-01, Gradient norm: 1.932e-04\n",
      "Iteration:   117, Value: 8.402e-01, Gradient norm: 2.705e-04\n",
      "Iteration:   118, Value: 8.402e-01, Gradient norm: 3.183e-04\n",
      "Iteration:   119, Value: 8.402e-01, Gradient norm: 3.111e-04\n",
      "Iteration:   120, Value: 8.402e-01, Gradient norm: 6.686e-04\n",
      "Iteration:   121, Value: 8.402e-01, Gradient norm: 2.358e-04\n",
      "Iteration:   122, Value: 8.402e-01, Gradient norm: 2.504e-04\n",
      "Iteration:   123, Value: 8.402e-01, Gradient norm: 3.144e-04\n",
      "Iteration:   124, Value: 8.402e-01, Gradient norm: 2.847e-04\n",
      "Iteration:   125, Value: 8.402e-01, Gradient norm: 8.943e-04\n",
      "Iteration:   126, Value: 8.402e-01, Gradient norm: 5.044e-04\n",
      "Iteration:   127, Value: 8.402e-01, Gradient norm: 2.116e-04\n",
      "Iteration:   128, Value: 8.402e-01, Gradient norm: 2.605e-04\n",
      "Iteration:   129, Value: 8.402e-01, Gradient norm: 2.945e-04\n",
      "Iteration:   130, Value: 8.402e-01, Gradient norm: 4.409e-04\n",
      "Iteration:   131, Value: 8.402e-01, Gradient norm: 2.205e-04\n",
      "Iteration:   132, Value: 8.402e-01, Gradient norm: 3.207e-04\n",
      "Iteration:   133, Value: 8.402e-01, Gradient norm: 4.796e-04\n",
      "Iteration:   134, Value: 8.402e-01, Gradient norm: 1.999e-04\n",
      "Iteration:   135, Value: 8.402e-01, Gradient norm: 1.620e-04\n",
      "Iteration:   136, Value: 8.402e-01, Gradient norm: 2.043e-04\n",
      "Iteration:   137, Value: 8.402e-01, Gradient norm: 1.613e-04\n",
      "Iteration:   138, Value: 8.402e-01, Gradient norm: 3.057e-04\n",
      "Iteration:   139, Value: 8.402e-01, Gradient norm: 1.328e-04\n",
      "Iteration:   140, Value: 8.402e-01, Gradient norm: 1.500e-04\n",
      "Iteration:   141, Value: 8.402e-01, Gradient norm: 1.838e-04\n",
      "Iteration:   142, Value: 8.402e-01, Gradient norm: 1.782e-04\n",
      "Iteration:   143, Value: 8.402e-01, Gradient norm: 1.179e-04\n",
      "Iteration:   144, Value: 8.402e-01, Gradient norm: 2.297e-04\n",
      "Iteration:   145, Value: 8.402e-01, Gradient norm: 2.531e-04\n",
      "Iteration:   146, Value: 8.402e-01, Gradient norm: 1.432e-04\n",
      "Final value: 8.40e-01, Final gradient norm: 7.59e-05\n"
     ]
    }
   ],
   "source": [
    "## initialize theta with zeros\n",
    "m = 3\n",
    "theta_init = jnp.zeros((m-1, train_xs.shape[1]))\n",
    "\n",
    "## check the initial value of the loss function and the norm of the gradient at the initial point\n",
    "print(\n",
    "    f'Initial value: {loss_fn(theta_init):.2e} '\n",
    "    f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(loss_fn)(theta_init)):.2e}'\n",
    ")\n",
    "\n",
    "## optimize the loss function using L-BFGS \n",
    "final_theta, _ = fmin_lbfgs(theta_init, loss_fn, max_iter=300, tol=1e-4)\n",
    "print(\n",
    "    f'Final value: {loss_fn(final_theta):.2e}, '\n",
    "    f'Final gradient norm: {otu.tree_l2_norm(jax.grad(loss_fn)(final_theta)):.2e}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions and compute the accuracy on the training data\n",
    "After the model is trained, you need to make predictions on the training data and compute the accuracy of the predictions. For each residue in the training data, you need to compute the probability of each class and assign the class with the highest probability as the predicted class. Then you need to compare the predicted classes with the true classes and compute the accuracy. The accuracy is the number of correct predictions divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_sample(theta, x):\n",
    "    \"\"\"Predict the class of a single sample\n",
    "    \n",
    "    Args:\n",
    "        theta: jnp.ndarray, the parameters of the model with shape (2, 316)\n",
    "        x: jnp.ndarray, the input features of one sample\n",
    "\n",
    "    Returns:\n",
    "        int: the predicted class of the sample\n",
    "    \n",
    "    \"\"\"\n",
    "    theta = expand_theta(theta)\n",
    "    return jnp.argmax(jnp.matmul(theta, x))\n",
    "\n",
    "def predict(theta, xs):\n",
    "    \"\"\"Predict the classes of multiple samples by using predict_per_sample and jax.vmap\n",
    "\n",
    "    Args:\n",
    "        theta: jnp.ndarray, the parameters of the model with shape (2, 316)\n",
    "        xs: jnp.ndarray, the input features of multiple samples with shape (n, 316)\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: the predicted classes of the samples with shape (n,)        \n",
    "    \"\"\"\n",
    "\n",
    "    return jax.vmap(predict_per_sample, in_axes=(None, 0))(theta, xs)\n",
    "\n",
    "\n",
    "def compute_accuray(y_pred, y_true):\n",
    "    \"\"\"Compute the accuracy of the model\n",
    "\n",
    "    Args:\n",
    "        y_pred: jnp.ndarray, the predicted classes of the samples with shape (n,)\n",
    "        y_true: jnp.ndarray, the true classes of the samples with shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        float: the accuracy of the model\n",
    "    \"\"\"\n",
    "\n",
    "    return jnp.mean(y_pred == y_true)\n",
    "\n",
    "\n",
    "y_pred = predict(final_theta, train_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model on the training set using the initial theta and the final theta. The accuracy with the initial theta should be around 33% because the initial theta is zero and the model is making random predictions. The accuracy with the final theta should be significantly higher because the model has learned the weights that minimize the loss function on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model with initial theta: 0.38\n",
      "Accuracy of the model with final theta: 0.62\n"
     ]
    }
   ],
   "source": [
    "y_pred_init = predict(theta_init, train_xs)\n",
    "accuracy_init = compute_accuray(y_pred_init, train_ys)\n",
    "\n",
    "y_pred_final = predict(final_theta, train_xs)\n",
    "accuracy_final = compute_accuray(y_pred_final, train_ys)\n",
    "\n",
    "print(f'Accuracy of the model with initial theta: {accuracy_init:.2f}')\n",
    "print(f'Accuracy of the model with final theta: {accuracy_final:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test data\n",
    "After the model is trained, you need to make predictions on the test sequences. The test sequences are provided in the file `test.txt`. You need to use the trained model to predict the secondary structure of all residues in the test sequences. The predictions should be written to a text file named `predictions.txt` and its format should be the same as the file `train.txt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
