{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict protein secondary structure with deep neural networks\n",
    "\n",
    "\n",
    "**Author**: YOUR_NAME\n",
    "\n",
    "**Due date**: March 30, 2025, 11:59 PM\n",
    "\n",
    "This assignment is similar to the [previous one](https://dinglab.io/chem193/homework/2-protein-secondary-structure/script/main.html#) but uses deep neural networks instead of simple linear models. It also uses larger datasets. The training and test datasets are provided in the text files [train.txt](https://tufts.box.com/s/y4t82o03hhf92zw6dik0x9r7v09qdyhs) and [test.txt](https://tufts.box.com/s/v4ypbippcsnifjlkd7vrp478bb7ue92l), respectively. The format of the datasets is the same as in the previous assignment.\n",
    "The task is to train a deep neural network model using the training dataset and predict the secondary structure of the proteins using their sequences in the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import equinox as eqx\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from sys import exit\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output of the neural network model\n",
    "Similary to the previous assignment, the model will predict the secondary structure of a protein using the sliding window approach. It predicts the secondary structure of a residue based on a window of residues centered at that residue. In the previous assignment, the window size was 15. In this assignment, the window size is a hyperparameter that you can choose. The default value is 31, but you can change it to any odd number.\n",
    "\n",
    "Assume that the window size is `k`. The input to the neural network model is a 1d array of `k` integers, each representing the index of that residue in the amino acid alphabet, `ACDEFGHIKLMNPQRSTVWY*`. For example, if `k = 5`, the sequence `ACACG` will be represented as `[0, 1, 0, 1, 5]`. The output of the model is a 1d array of 3 floats, each representing the logorihtm of the probability of the corresponding secondary structure, `helix`, `strand`, and `other`. For example, the output `[-1.0986123, -1.0986123, -1.0986123]` represents the probabilities `[0.333, 0.333, 0.333]` for the secondary structures `helix`, `strand`, and `other`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the training data\n",
    "Given the design of the model, the data in `train.txt` are not directly suitable for training. The data need to be processed to create the input and output pairs for the model. For each sequence in the training data, we extract all windows of size `k` and convert both the amino acid sequence and the secondary structure to the integer representation. The following two code cells show how it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 34331\n",
      "Number of validation samples: 8582\n"
     ]
    }
   ],
   "source": [
    "## you need to change `path_to_train` to the path of the train.txt file\n",
    "path_to_train = \"../data/train.txt\"\n",
    "\n",
    "## read data from train.txt\n",
    "train_data = {}\n",
    "\n",
    "## train_seq is a dictionary with the following structure:\n",
    "## train_seq[protein_name] = (sequence, secondary_structure),\n",
    "## where protein_name is the name of the protein, sequence is the amino acid sequence of the protein, and secondary_structure is the secondary structure of the protein as given in the train.txt file\n",
    "\n",
    "\n",
    "with open(path_to_train, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            name = line.strip()[1:]\n",
    "            train_data[name] = []\n",
    "        else:\n",
    "            train_data[name].append(line.strip())\n",
    "\n",
    "\n",
    "## here we split the data into training and validation data\n",
    "## we use 80% of the data for training and 20% for validation\n",
    "\n",
    "names = list(train_data.keys())\n",
    "names_validation = np.random.choice(names, int(len(names)*0.2), replace=False)\n",
    "\n",
    "valid_data = {name: train_data[name] for name in names_validation}\n",
    "for name in names_validation:\n",
    "    train_data.pop(name)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_per_seq(seq, secondary_structure, window_size=31):\n",
    "    \"\"\" Get windows for a single sequence\n",
    "\n",
    "    Args:\n",
    "        seq (str): amino acid sequence\n",
    "        secondary_structure (str): secondary structure\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        xs (np.array): input windows\n",
    "        ys (np.array): output windows\n",
    "\n",
    "    \"\"\"\n",
    "    ## amino acid order\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY*\"\n",
    "\n",
    "    seq = [amino_acids.index(s) for s in seq]\n",
    "    for _ in range(window_size // 2):\n",
    "        seq = [amino_acids.index(\"*\")] + seq + [amino_acids.index(\"*\")]\n",
    "\n",
    "    windows = []\n",
    "    for i in range(window_size//2, len(seq) - window_size//2):\n",
    "        x = np.array(seq[i - window_size//2 : i + window_size//2 + 1], dtype=np.int8)\n",
    "        y = int(secondary_structure[i - window_size//2])\n",
    "\n",
    "        windows.append((x, y))\n",
    "\n",
    "    xs = np.array([x for x, _ in windows], dtype=np.int8)\n",
    "    ys = np.array([y for _, y in windows], dtype=np.int8)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def get_windows(data, window_size=31):\n",
    "    \"\"\" Get windows for a dataset\"\n",
    "    \n",
    "    Args:\n",
    "        data (dict): dataset\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        xs (np.array): input windows\n",
    "        ys (np.array): output windows\n",
    "    \"\"\"\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for name in tqdm(data.keys()):\n",
    "        seq, ss = data[name]\n",
    "        x, y = get_windows_per_seq(seq, ss, window_size=window_size)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    xs = np.concatenate(xs)\n",
    "    ys = np.concatenate(ys)\n",
    "\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34331/34331 [00:51<00:00, 663.31it/s]\n",
      "100%|██████████| 8582/8582 [00:11<00:00, 722.15it/s]\n"
     ]
    }
   ],
   "source": [
    "## the default window size is 31 amino acids (15 on each side of the central amino acid)\n",
    "## you can change the window size by changing the value of the window_size variable\n",
    "window_size = 31\n",
    "\n",
    "## get windows for training and validation data\n",
    "train_xs, train_ys = get_windows(train_data, window_size)\n",
    "valid_xs, valid_ys = get_windows(valid_data, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network model\n",
    "You need to implement a deep neural network model using the `equinox` library. The input and output of the model are described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################  write your code for the following NeuralNetwork class (50 points)\n",
    "####################################################################################################\n",
    "class NeuralNetwork(eqx.Module):\n",
    "    emb: eqx.nn.Embedding\n",
    "    emb_layer: list\n",
    "    res_layer_1: list\n",
    "    res_layer_2: list\n",
    "    res_layer_3: list\n",
    "    output_layer: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        subkey, key = jr.split(key)\n",
    "        self.emb = eqx.nn.Embedding(num_embeddings=21, embedding_size=16, key=subkey)\n",
    "        emb_size = 16*window_size\n",
    "        res_size = 32\n",
    "\n",
    "        subkey, key = jr.split(key)\n",
    "        self.emb_layer = [\n",
    "            eqx.nn.Linear(emb_size, res_size, key=subkey),\n",
    "            jax.nn.relu,\n",
    "        ]\n",
    "\n",
    "        subkey1, subkey2, key = jr.split(key, 3)\n",
    "        self.res_layer_1 = [\n",
    "            eqx.nn.Linear(res_size, 64, key=subkey1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, res_size, key=subkey2),\n",
    "            jax.nn.relu,\n",
    "        ]\n",
    "\n",
    "        subkey1, subkey2, key = jr.split(key, 3)\n",
    "        self.res_layer_2 = [\n",
    "            eqx.nn.Linear(res_size, 64, key=subkey1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, res_size, key=subkey2),\n",
    "            jax.nn.relu,\n",
    "        ]\n",
    "\n",
    "        subkey1, subkey2, key = jr.split(key, 3)\n",
    "        self.res_layer_3 = [\n",
    "            eqx.nn.Linear(res_size, 64, key=subkey1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, res_size, key=subkey2),\n",
    "            jax.nn.relu,\n",
    "        ]\n",
    "\n",
    "        subkey1, subkey2, key = jr.split(key, 3)\n",
    "        self.output_layer = [\n",
    "            eqx.nn.Linear(res_size, 3, key=subkey1),\n",
    "            jax.nn.log_softmax,\n",
    "        ]\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\" Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (jnp.array): 1D array of integers representing amino acids in a window\n",
    "        \n",
    "        Returns:\n",
    "            logp (jnp.array): 1D array of 3 floats representing log-probabilities of secondary structure classes\n",
    "        \"\"\"\n",
    "\n",
    "        x = jax.vmap(self.emb)(x)\n",
    "        x = x.flatten()\n",
    "\n",
    "        for layer in self.emb_layer:\n",
    "            x = layer(x)\n",
    "\n",
    "        x_ini = x\n",
    "        for layer in self.res_layer_1:\n",
    "            x = layer(x)\n",
    "        x = x + x_ini\n",
    "\n",
    "        x_ini = x\n",
    "        for layer in self.res_layer_2:\n",
    "            x = layer(x)\n",
    "        x = x + x_ini\n",
    "\n",
    "        x_ini = x\n",
    "        for layer in self.res_layer_3:\n",
    "            x = layer(x)\n",
    "        x = x + x_ini\n",
    "        \n",
    "        for layer in self.output_layer:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To train the model, you need to finish the implementation of the `loss_fn` function. The function takes the model, a batch of input and output pairs, and returns the loss. The loss should be the same as the loss function used in the previous assignment. To monitor the training process, you also need to implement the three functions: `make_predictions`, and `compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that you could comment the @eqx.filter_jit decorator during the development of the loss function so that the error messages are more informative. Once the loss function is working, you can uncomment the decorator to speed up the training process. The same applies to the make_predictions function\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss_fn(model, xs, ys):\n",
    "    \"\"\" Loss function for a batch of windows\n",
    "\n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(batch_size, window_size)\n",
    "        ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        loss (jnp.array): the average loss over the batch\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################################################################\n",
    "    #### write your code loss function (15 points)\n",
    "    ###############################################################################################\n",
    "    log_prob = jax.vmap(model)(xs)\n",
    "    loss = -jnp.mean(jnp.sum(log_prob * jax.nn.one_hot(ys, 3), axis=1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_predictions(model, xs):\n",
    "    \"\"\" Make predictions for a batch of windows\"\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(batch_size, window_size)\n",
    "\n",
    "    Returns:\n",
    "        predictions (jnp.array): 1D array of integers representing predicted secondary structure classes. Shape: (batch_size,)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ################################################\n",
    "    #### write your code loss function (15 points)\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    log_prob = jax.vmap(model)(xs)\n",
    "    predictions = jnp.argmax(log_prob, axis=1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_average_loss(model, xs, ys, batch_size=1024*16):\n",
    "    \"\"\" Compute average loss for a dataset of windows by batching\n",
    "\n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(num_samples, window_size)\n",
    "        ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (num_samples,)\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        loss (float): average loss over the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_batches = len(xs) // batch_size + 1\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = xs[start_idx:end_idx]\n",
    "        batch_ys = ys[start_idx:end_idx]\n",
    "\n",
    "        loss = loss_fn(model, batch_xs, batch_ys)\n",
    "        total_loss += loss * batch_xs.shape[0]\n",
    "    loss = total_loss / len(xs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_accuracy(model, xs, ys, batch_size=1024*16):\n",
    "    \"\"\" Compute accuracy for a dataset of windows by batching\"\n",
    "    \n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(num_samples, window_size)\n",
    "        ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (num_samples,)\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): accuracy over the dataset\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ####  write your code loss function (10 points)  ####\n",
    "    #####################################################\n",
    "\n",
    "\n",
    "    num_batches = len(xs) // batch_size + 1\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = xs[start_idx:end_idx]\n",
    "        batch_ys = ys[start_idx:end_idx]\n",
    "        predictions = make_predictions(model, batch_xs)\n",
    "        correct += jnp.sum(predictions == batch_ys)\n",
    "        total += len(batch_ys)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using stochastic gradient descent (SGD) with the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize the model \n",
    "key = jr.PRNGKey(0)\n",
    "model = NeuralNetwork(key)\n",
    "\n",
    "\n",
    "## initialize the optimizer\n",
    "## the learning rate is set to 0.001\n",
    "optim = optax.adamw(0.001)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "## training loop\n",
    "## Note that you could comment the @eqx.filter_jit decorator during the development so that the error messages are more informative. Once you are done, you can uncomment the decorator to speed up the training process\n",
    "@eqx.filter_jit\n",
    "def make_step(model, batch_xs, batch_ys, opt_state):\n",
    "    \"\"\" Make a single optimization step using a batch of windows\n",
    "\n",
    "    Args:\n",
    "        model (NeuralNetwork): neural network model\n",
    "        batch_xs (jnp.array): 2D array of integers representing amino acids in windows. Shape:(batch_size, window_size)\n",
    "        batch_ys (jnp.array): 1D array of integers representing secondary structure classes. Shape: (batch_size,)\n",
    "        opt_state (optax.OptState): optimizer state\n",
    "\n",
    "    Returns:\n",
    "        model (NeuralNetwork): updated neural network model\n",
    "        opt_state (optax.OptState): updated optimizer state\n",
    "        loss_value (float): loss value for the batch    \n",
    "    \"\"\"\n",
    "\n",
    "    loss_value, grads = eqx.filter_value_and_grad(loss_fn)(model, batch_xs, batch_ys)\n",
    "    updates, opt_state = optim.update(\n",
    "        grads, opt_state, eqx.filter(model, eqx.is_array)\n",
    "    )    \n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell shows how to train the model using the Adam optimizer. We monitor the training process by computing the loss and accuracy on the training and validation datasets after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0, train_loss 1.090\n",
      "epoch 0, batch 10, train_loss 1.006\n",
      "epoch 0, batch 20, train_loss 0.951\n",
      "epoch 0, batch 30, train_loss 0.926\n",
      "epoch 0, batch 40, train_loss 0.902\n",
      "epoch 0, batch 50, train_loss 0.890\n",
      "epoch 0, batch 60, train_loss 0.868\n",
      "epoch 0, batch 70, train_loss 0.865\n",
      "epoch 0, batch 80, train_loss 0.854\n",
      "epoch 0, batch 90, train_loss 0.852\n",
      "epoch 0, batch 100, train_loss 0.849\n",
      "epoch 0, batch 110, train_loss 0.842\n",
      "epoch 0, batch 120, train_loss 0.836\n",
      "epoch 0, batch 130, train_loss 0.841\n",
      "epoch 0, batch 140, train_loss 0.831\n",
      "epoch 0, batch 150, train_loss 0.825\n",
      "epoch 0, batch 160, train_loss 0.817\n",
      "epoch 0, batch 170, train_loss 0.813\n",
      "epoch 0, batch 180, train_loss 0.805\n",
      "epoch 0, batch 190, train_loss 0.804\n",
      "epoch 0, batch 200, train_loss 0.809\n",
      "epoch 0, batch 210, train_loss 0.797\n",
      "epoch 0, batch 220, train_loss 0.789\n",
      "epoch 0, batch 230, train_loss 0.786\n",
      "epoch 0, batch 240, train_loss 0.776\n",
      "epoch 0, batch 250, train_loss 0.773\n",
      "epoch 0, batch 260, train_loss 0.774\n",
      "epoch 0, batch 270, train_loss 0.773\n",
      "epoch 0, batch 280, train_loss 0.765\n",
      "epoch 0, batch 290, train_loss 0.765\n",
      "epoch 0, batch 300, train_loss 0.775\n",
      "epoch 0, batch 310, train_loss 0.763\n",
      "epoch 0, batch 320, train_loss 0.770\n",
      "epoch 0, batch 330, train_loss 0.762\n",
      "epoch 0, batch 340, train_loss 0.764\n",
      "epoch 0, batch 350, train_loss 0.767\n",
      "epoch 0, batch 360, train_loss 0.757\n",
      "epoch 0, batch 370, train_loss 0.764\n",
      "epoch 0, batch 380, train_loss 0.757\n",
      "epoch 0, batch 390, train_loss 0.755\n",
      "epoch 0, batch 400, train_loss 0.746\n",
      "epoch 0, batch 410, train_loss 0.757\n",
      "epoch 0, batch 420, train_loss 0.755\n",
      "epoch 0, batch 430, train_loss 0.757\n",
      "epoch 0, batch 440, train_loss 0.750\n",
      "epoch 0, batch 450, train_loss 0.757\n",
      "epoch 0, batch 460, train_loss 0.748\n",
      "epoch 0, batch 470, train_loss 0.755\n",
      "epoch 0, batch 480, train_loss 0.755\n",
      "epoch 0, batch 490, train_loss 0.753\n",
      "epoch 0, batch 500, train_loss 0.754\n",
      "epoch 0, batch 510, train_loss 0.747\n",
      "epoch 0, batch 520, train_loss 0.740\n",
      "epoch 0, batch 530, train_loss 0.748\n",
      "epoch 0, batch 540, train_loss 0.743\n",
      "epoch 0, batch 550, train_loss 0.750\n",
      "epoch 0, batch 560, train_loss 0.742\n",
      "epoch 0, batch 570, train_loss 0.741\n",
      "epoch 0, batch 580, train_loss 0.752\n",
      "epoch 0, batch 590, train_loss 0.747\n",
      "epoch 0, batch 600, train_loss 0.748\n",
      "epoch 0, batch 610, train_loss 0.739\n",
      "epoch 0, batch 620, train_loss 0.737\n",
      "epoch 0, batch 630, train_loss 0.745\n",
      "epoch 0, batch 640, train_loss 0.743\n",
      "epoch 0, batch 650, train_loss 0.743\n",
      "epoch 0, batch 660, train_loss 0.742\n",
      "epoch 0, batch 670, train_loss 0.738\n",
      "epoch 0, batch 680, train_loss 0.738\n",
      "epoch 0, batch 690, train_loss 0.742\n",
      "epoch 0, batch 700, train_loss 0.746\n",
      "epoch 0, batch 710, train_loss 0.753\n",
      "epoch 0, batch 720, train_loss 0.747\n",
      "epoch 0, batch 730, train_loss 0.733\n",
      "epoch 0, batch 740, train_loss 0.733\n",
      "epoch 0, batch 750, train_loss 0.739\n",
      "epoch 0, batch 760, train_loss 0.733\n",
      "epoch 0, batch 770, train_loss 0.734\n",
      "epoch 0, batch 780, train_loss 0.739\n",
      "epoch 0, batch 790, train_loss 0.734\n",
      "epoch 0, batch 800, train_loss 0.734\n",
      "epoch 0, batch 810, train_loss 0.741\n",
      "epoch 0, batch 820, train_loss 0.731\n",
      "epoch 0, batch 830, train_loss 0.733\n",
      "epoch 0, batch 840, train_loss 0.724\n",
      "epoch 0, batch 850, train_loss 0.745\n",
      "epoch 0, batch 860, train_loss 0.733\n",
      "epoch 0, batch 870, train_loss 0.747\n",
      "epoch 0, batch 880, train_loss 0.720\n",
      "epoch 0, batch 890, train_loss 0.736\n",
      "epoch 0, batch 900, train_loss 0.743\n",
      "epoch 0, batch 910, train_loss 0.736\n",
      "epoch 0, batch 920, train_loss 0.730\n",
      "epoch 0, batch 930, train_loss 0.737\n",
      "epoch 0, batch 940, train_loss 0.735\n",
      "epoch 0, batch 950, train_loss 0.740\n",
      "epoch 0, batch 960, train_loss 0.738\n",
      "epoch 0, batch 970, train_loss 0.732\n",
      "epoch 0, batch 980, train_loss 0.730\n",
      "epoch 0, batch 990, train_loss 0.729\n",
      "epoch 0, batch 1000, train_loss 0.729\n",
      "epoch 0, batch 1010, train_loss 0.734\n",
      "epoch 0, batch 1020, train_loss 0.727\n",
      "epoch 0, batch 1030, train_loss 0.736\n",
      "epoch 0, batch 1040, train_loss 0.728\n",
      "epoch 0, batch 1050, train_loss 0.730\n",
      "epoch 0, batch 1060, train_loss 0.726\n",
      "epoch 0, batch 1070, train_loss 0.732\n",
      "epoch 0, batch 1080, train_loss 0.739\n",
      "epoch 0, batch 1090, train_loss 0.736\n",
      "epoch 0, batch 1100, train_loss 0.731\n",
      "epoch 0, batch 1110, train_loss 0.728\n",
      "epoch 0, batch 1120, train_loss 0.735\n",
      "epoch 0, batch 1130, train_loss 0.741\n",
      "epoch 0, batch 1140, train_loss 0.740\n",
      "epoch 0, batch 1150, train_loss 0.731\n",
      "epoch 0, batch 1160, train_loss 0.734\n",
      "epoch 0, batch 1170, train_loss 0.728\n",
      "epoch 0, batch 1180, train_loss 0.729\n",
      "epoch 0, batch 1190, train_loss 0.731\n",
      "epoch     0, train_loss 0.730, valid_loss 0.740, train_accuracy  68.26%, valid_accuracy  67.72%\n",
      "epoch 1, batch 0, train_loss 0.737\n",
      "epoch 1, batch 10, train_loss 0.730\n",
      "epoch 1, batch 20, train_loss 0.738\n",
      "epoch 1, batch 30, train_loss 0.729\n",
      "epoch 1, batch 40, train_loss 0.720\n",
      "epoch 1, batch 50, train_loss 0.737\n",
      "epoch 1, batch 60, train_loss 0.727\n",
      "epoch 1, batch 70, train_loss 0.731\n",
      "epoch 1, batch 80, train_loss 0.729\n",
      "epoch 1, batch 90, train_loss 0.730\n",
      "epoch 1, batch 100, train_loss 0.723\n",
      "epoch 1, batch 110, train_loss 0.724\n",
      "epoch 1, batch 120, train_loss 0.731\n",
      "epoch 1, batch 130, train_loss 0.721\n",
      "epoch 1, batch 140, train_loss 0.727\n",
      "epoch 1, batch 150, train_loss 0.727\n",
      "epoch 1, batch 160, train_loss 0.729\n",
      "epoch 1, batch 170, train_loss 0.715\n",
      "epoch 1, batch 180, train_loss 0.720\n",
      "epoch 1, batch 190, train_loss 0.732\n",
      "epoch 1, batch 200, train_loss 0.733\n",
      "epoch 1, batch 210, train_loss 0.720\n",
      "epoch 1, batch 220, train_loss 0.732\n",
      "epoch 1, batch 230, train_loss 0.717\n",
      "epoch 1, batch 240, train_loss 0.736\n",
      "epoch 1, batch 250, train_loss 0.727\n",
      "epoch 1, batch 260, train_loss 0.726\n",
      "epoch 1, batch 270, train_loss 0.715\n",
      "epoch 1, batch 280, train_loss 0.718\n",
      "epoch 1, batch 290, train_loss 0.721\n",
      "epoch 1, batch 300, train_loss 0.731\n",
      "epoch 1, batch 310, train_loss 0.730\n",
      "epoch 1, batch 320, train_loss 0.734\n",
      "epoch 1, batch 330, train_loss 0.730\n",
      "epoch 1, batch 340, train_loss 0.727\n",
      "epoch 1, batch 350, train_loss 0.739\n",
      "epoch 1, batch 360, train_loss 0.728\n",
      "epoch 1, batch 370, train_loss 0.718\n",
      "epoch 1, batch 380, train_loss 0.728\n",
      "epoch 1, batch 390, train_loss 0.717\n",
      "epoch 1, batch 400, train_loss 0.732\n",
      "epoch 1, batch 410, train_loss 0.715\n",
      "epoch 1, batch 420, train_loss 0.724\n",
      "epoch 1, batch 430, train_loss 0.719\n",
      "epoch 1, batch 440, train_loss 0.720\n",
      "epoch 1, batch 450, train_loss 0.720\n",
      "epoch 1, batch 460, train_loss 0.731\n",
      "epoch 1, batch 470, train_loss 0.732\n",
      "epoch 1, batch 480, train_loss 0.725\n",
      "epoch 1, batch 490, train_loss 0.727\n",
      "epoch 1, batch 500, train_loss 0.726\n",
      "epoch 1, batch 510, train_loss 0.723\n",
      "epoch 1, batch 520, train_loss 0.717\n",
      "epoch 1, batch 530, train_loss 0.719\n",
      "epoch 1, batch 540, train_loss 0.738\n",
      "epoch 1, batch 550, train_loss 0.721\n",
      "epoch 1, batch 560, train_loss 0.717\n",
      "epoch 1, batch 570, train_loss 0.720\n",
      "epoch 1, batch 580, train_loss 0.737\n",
      "epoch 1, batch 590, train_loss 0.721\n",
      "epoch 1, batch 600, train_loss 0.718\n",
      "epoch 1, batch 610, train_loss 0.721\n",
      "epoch 1, batch 620, train_loss 0.730\n",
      "epoch 1, batch 630, train_loss 0.725\n",
      "epoch 1, batch 640, train_loss 0.726\n",
      "epoch 1, batch 650, train_loss 0.720\n",
      "epoch 1, batch 660, train_loss 0.725\n",
      "epoch 1, batch 670, train_loss 0.716\n",
      "epoch 1, batch 680, train_loss 0.726\n",
      "epoch 1, batch 690, train_loss 0.724\n",
      "epoch 1, batch 700, train_loss 0.733\n",
      "epoch 1, batch 710, train_loss 0.722\n",
      "epoch 1, batch 720, train_loss 0.718\n",
      "epoch 1, batch 730, train_loss 0.729\n",
      "epoch 1, batch 740, train_loss 0.719\n",
      "epoch 1, batch 750, train_loss 0.724\n",
      "epoch 1, batch 760, train_loss 0.724\n",
      "epoch 1, batch 770, train_loss 0.729\n",
      "epoch 1, batch 780, train_loss 0.726\n",
      "epoch 1, batch 790, train_loss 0.721\n",
      "epoch 1, batch 800, train_loss 0.713\n",
      "epoch 1, batch 810, train_loss 0.727\n",
      "epoch 1, batch 820, train_loss 0.720\n",
      "epoch 1, batch 830, train_loss 0.729\n",
      "epoch 1, batch 840, train_loss 0.731\n",
      "epoch 1, batch 850, train_loss 0.726\n",
      "epoch 1, batch 860, train_loss 0.715\n",
      "epoch 1, batch 870, train_loss 0.729\n",
      "epoch 1, batch 880, train_loss 0.722\n",
      "epoch 1, batch 890, train_loss 0.721\n",
      "epoch 1, batch 900, train_loss 0.725\n",
      "epoch 1, batch 910, train_loss 0.723\n",
      "epoch 1, batch 920, train_loss 0.717\n",
      "epoch 1, batch 930, train_loss 0.731\n",
      "epoch 1, batch 940, train_loss 0.727\n",
      "epoch 1, batch 950, train_loss 0.721\n",
      "epoch 1, batch 960, train_loss 0.715\n",
      "epoch 1, batch 970, train_loss 0.720\n",
      "epoch 1, batch 980, train_loss 0.723\n",
      "epoch 1, batch 990, train_loss 0.722\n",
      "epoch 1, batch 1000, train_loss 0.716\n",
      "epoch 1, batch 1010, train_loss 0.726\n",
      "epoch 1, batch 1020, train_loss 0.720\n",
      "epoch 1, batch 1030, train_loss 0.723\n",
      "epoch 1, batch 1040, train_loss 0.728\n",
      "epoch 1, batch 1050, train_loss 0.725\n",
      "epoch 1, batch 1060, train_loss 0.720\n",
      "epoch 1, batch 1070, train_loss 0.715\n",
      "epoch 1, batch 1080, train_loss 0.709\n",
      "epoch 1, batch 1090, train_loss 0.714\n",
      "epoch 1, batch 1100, train_loss 0.724\n",
      "epoch 1, batch 1110, train_loss 0.713\n",
      "epoch 1, batch 1120, train_loss 0.718\n",
      "epoch 1, batch 1130, train_loss 0.726\n",
      "epoch 1, batch 1140, train_loss 0.718\n",
      "epoch 1, batch 1150, train_loss 0.723\n",
      "epoch 1, batch 1160, train_loss 0.712\n",
      "epoch 1, batch 1170, train_loss 0.719\n",
      "epoch 1, batch 1180, train_loss 0.725\n",
      "epoch 1, batch 1190, train_loss 0.714\n",
      "epoch     1, train_loss 0.718, valid_loss 0.731, train_accuracy  68.86%, valid_accuracy  68.22%\n",
      "epoch 2, batch 0, train_loss 0.723\n",
      "epoch 2, batch 10, train_loss 0.720\n",
      "epoch 2, batch 20, train_loss 0.708\n",
      "epoch 2, batch 30, train_loss 0.720\n",
      "epoch 2, batch 40, train_loss 0.724\n",
      "epoch 2, batch 50, train_loss 0.713\n",
      "epoch 2, batch 60, train_loss 0.711\n",
      "epoch 2, batch 70, train_loss 0.709\n",
      "epoch 2, batch 80, train_loss 0.720\n",
      "epoch 2, batch 90, train_loss 0.723\n",
      "epoch 2, batch 100, train_loss 0.721\n",
      "epoch 2, batch 110, train_loss 0.717\n",
      "epoch 2, batch 120, train_loss 0.713\n",
      "epoch 2, batch 130, train_loss 0.719\n",
      "epoch 2, batch 140, train_loss 0.705\n",
      "epoch 2, batch 150, train_loss 0.713\n",
      "epoch 2, batch 160, train_loss 0.710\n",
      "epoch 2, batch 170, train_loss 0.728\n",
      "epoch 2, batch 180, train_loss 0.719\n",
      "epoch 2, batch 190, train_loss 0.724\n",
      "epoch 2, batch 200, train_loss 0.712\n",
      "epoch 2, batch 210, train_loss 0.720\n",
      "epoch 2, batch 220, train_loss 0.717\n",
      "epoch 2, batch 230, train_loss 0.722\n",
      "epoch 2, batch 240, train_loss 0.723\n",
      "epoch 2, batch 250, train_loss 0.720\n",
      "epoch 2, batch 260, train_loss 0.724\n",
      "epoch 2, batch 270, train_loss 0.718\n",
      "epoch 2, batch 280, train_loss 0.714\n",
      "epoch 2, batch 290, train_loss 0.709\n",
      "epoch 2, batch 300, train_loss 0.719\n",
      "epoch 2, batch 310, train_loss 0.725\n",
      "epoch 2, batch 320, train_loss 0.725\n",
      "epoch 2, batch 330, train_loss 0.724\n",
      "epoch 2, batch 340, train_loss 0.713\n",
      "epoch 2, batch 350, train_loss 0.719\n",
      "epoch 2, batch 360, train_loss 0.721\n",
      "epoch 2, batch 370, train_loss 0.723\n",
      "epoch 2, batch 380, train_loss 0.709\n",
      "epoch 2, batch 390, train_loss 0.713\n",
      "epoch 2, batch 400, train_loss 0.712\n",
      "epoch 2, batch 410, train_loss 0.716\n",
      "epoch 2, batch 420, train_loss 0.711\n",
      "epoch 2, batch 430, train_loss 0.722\n",
      "epoch 2, batch 440, train_loss 0.730\n",
      "epoch 2, batch 450, train_loss 0.719\n",
      "epoch 2, batch 460, train_loss 0.717\n",
      "epoch 2, batch 470, train_loss 0.722\n",
      "epoch 2, batch 480, train_loss 0.720\n",
      "epoch 2, batch 490, train_loss 0.718\n",
      "epoch 2, batch 500, train_loss 0.727\n",
      "epoch 2, batch 510, train_loss 0.716\n",
      "epoch 2, batch 520, train_loss 0.719\n",
      "epoch 2, batch 530, train_loss 0.713\n",
      "epoch 2, batch 540, train_loss 0.710\n",
      "epoch 2, batch 550, train_loss 0.718\n",
      "epoch 2, batch 560, train_loss 0.724\n",
      "epoch 2, batch 570, train_loss 0.718\n",
      "epoch 2, batch 580, train_loss 0.724\n",
      "epoch 2, batch 590, train_loss 0.716\n",
      "epoch 2, batch 600, train_loss 0.729\n",
      "epoch 2, batch 610, train_loss 0.710\n",
      "epoch 2, batch 620, train_loss 0.719\n",
      "epoch 2, batch 630, train_loss 0.715\n",
      "epoch 2, batch 640, train_loss 0.715\n",
      "epoch 2, batch 650, train_loss 0.717\n",
      "epoch 2, batch 660, train_loss 0.716\n",
      "epoch 2, batch 670, train_loss 0.717\n",
      "epoch 2, batch 680, train_loss 0.708\n",
      "epoch 2, batch 690, train_loss 0.716\n",
      "epoch 2, batch 700, train_loss 0.710\n",
      "epoch 2, batch 710, train_loss 0.724\n",
      "epoch 2, batch 720, train_loss 0.727\n",
      "epoch 2, batch 730, train_loss 0.722\n",
      "epoch 2, batch 740, train_loss 0.720\n",
      "epoch 2, batch 750, train_loss 0.719\n",
      "epoch 2, batch 760, train_loss 0.704\n",
      "epoch 2, batch 770, train_loss 0.713\n",
      "epoch 2, batch 780, train_loss 0.724\n",
      "epoch 2, batch 790, train_loss 0.724\n",
      "epoch 2, batch 800, train_loss 0.712\n",
      "epoch 2, batch 810, train_loss 0.708\n",
      "epoch 2, batch 820, train_loss 0.721\n",
      "epoch 2, batch 830, train_loss 0.732\n",
      "epoch 2, batch 840, train_loss 0.718\n",
      "epoch 2, batch 850, train_loss 0.715\n",
      "epoch 2, batch 860, train_loss 0.719\n",
      "epoch 2, batch 870, train_loss 0.715\n",
      "epoch 2, batch 880, train_loss 0.719\n",
      "epoch 2, batch 890, train_loss 0.716\n",
      "epoch 2, batch 900, train_loss 0.721\n",
      "epoch 2, batch 910, train_loss 0.720\n",
      "epoch 2, batch 920, train_loss 0.711\n",
      "epoch 2, batch 930, train_loss 0.715\n",
      "epoch 2, batch 940, train_loss 0.718\n",
      "epoch 2, batch 950, train_loss 0.720\n",
      "epoch 2, batch 960, train_loss 0.719\n",
      "epoch 2, batch 970, train_loss 0.714\n",
      "epoch 2, batch 980, train_loss 0.719\n",
      "epoch 2, batch 990, train_loss 0.718\n",
      "epoch 2, batch 1000, train_loss 0.704\n",
      "epoch 2, batch 1010, train_loss 0.726\n",
      "epoch 2, batch 1020, train_loss 0.715\n",
      "epoch 2, batch 1030, train_loss 0.714\n",
      "epoch 2, batch 1040, train_loss 0.723\n",
      "epoch 2, batch 1050, train_loss 0.718\n",
      "epoch 2, batch 1060, train_loss 0.719\n",
      "epoch 2, batch 1070, train_loss 0.712\n",
      "epoch 2, batch 1080, train_loss 0.709\n",
      "epoch 2, batch 1090, train_loss 0.716\n",
      "epoch 2, batch 1100, train_loss 0.714\n",
      "epoch 2, batch 1110, train_loss 0.714\n",
      "epoch 2, batch 1120, train_loss 0.718\n",
      "epoch 2, batch 1130, train_loss 0.712\n",
      "epoch 2, batch 1140, train_loss 0.714\n",
      "epoch 2, batch 1150, train_loss 0.718\n",
      "epoch 2, batch 1160, train_loss 0.718\n",
      "epoch 2, batch 1170, train_loss 0.712\n",
      "epoch 2, batch 1180, train_loss 0.707\n",
      "epoch 2, batch 1190, train_loss 0.713\n",
      "epoch     2, train_loss 0.713, valid_loss 0.727, train_accuracy  69.11%, valid_accuracy  68.36%\n",
      "epoch 3, batch 0, train_loss 0.710\n",
      "epoch 3, batch 10, train_loss 0.713\n",
      "epoch 3, batch 20, train_loss 0.706\n",
      "epoch 3, batch 30, train_loss 0.717\n",
      "epoch 3, batch 40, train_loss 0.715\n",
      "epoch 3, batch 50, train_loss 0.711\n",
      "epoch 3, batch 60, train_loss 0.703\n",
      "epoch 3, batch 70, train_loss 0.717\n",
      "epoch 3, batch 80, train_loss 0.708\n",
      "epoch 3, batch 90, train_loss 0.714\n",
      "epoch 3, batch 100, train_loss 0.711\n",
      "epoch 3, batch 110, train_loss 0.705\n",
      "epoch 3, batch 120, train_loss 0.719\n",
      "epoch 3, batch 130, train_loss 0.708\n",
      "epoch 3, batch 140, train_loss 0.711\n",
      "epoch 3, batch 150, train_loss 0.715\n",
      "epoch 3, batch 160, train_loss 0.730\n",
      "epoch 3, batch 170, train_loss 0.719\n",
      "epoch 3, batch 180, train_loss 0.726\n",
      "epoch 3, batch 190, train_loss 0.714\n",
      "epoch 3, batch 200, train_loss 0.713\n",
      "epoch 3, batch 210, train_loss 0.717\n",
      "epoch 3, batch 220, train_loss 0.717\n",
      "epoch 3, batch 230, train_loss 0.720\n",
      "epoch 3, batch 240, train_loss 0.716\n",
      "epoch 3, batch 250, train_loss 0.703\n",
      "epoch 3, batch 260, train_loss 0.714\n",
      "epoch 3, batch 270, train_loss 0.715\n",
      "epoch 3, batch 280, train_loss 0.717\n",
      "epoch 3, batch 290, train_loss 0.712\n",
      "epoch 3, batch 300, train_loss 0.723\n",
      "epoch 3, batch 310, train_loss 0.717\n",
      "epoch 3, batch 320, train_loss 0.722\n",
      "epoch 3, batch 330, train_loss 0.715\n",
      "epoch 3, batch 340, train_loss 0.719\n",
      "epoch 3, batch 350, train_loss 0.705\n",
      "epoch 3, batch 360, train_loss 0.707\n",
      "epoch 3, batch 370, train_loss 0.720\n",
      "epoch 3, batch 380, train_loss 0.711\n",
      "epoch 3, batch 390, train_loss 0.712\n",
      "epoch 3, batch 400, train_loss 0.711\n",
      "epoch 3, batch 410, train_loss 0.716\n",
      "epoch 3, batch 420, train_loss 0.716\n",
      "epoch 3, batch 430, train_loss 0.712\n",
      "epoch 3, batch 440, train_loss 0.713\n",
      "epoch 3, batch 450, train_loss 0.714\n",
      "epoch 3, batch 460, train_loss 0.712\n",
      "epoch 3, batch 470, train_loss 0.706\n",
      "epoch 3, batch 480, train_loss 0.714\n",
      "epoch 3, batch 490, train_loss 0.714\n",
      "epoch 3, batch 500, train_loss 0.708\n",
      "epoch 3, batch 510, train_loss 0.718\n",
      "epoch 3, batch 520, train_loss 0.724\n",
      "epoch 3, batch 530, train_loss 0.709\n",
      "epoch 3, batch 540, train_loss 0.711\n",
      "epoch 3, batch 550, train_loss 0.704\n",
      "epoch 3, batch 560, train_loss 0.711\n",
      "epoch 3, batch 570, train_loss 0.705\n",
      "epoch 3, batch 580, train_loss 0.712\n",
      "epoch 3, batch 590, train_loss 0.716\n",
      "epoch 3, batch 600, train_loss 0.706\n",
      "epoch 3, batch 610, train_loss 0.718\n",
      "epoch 3, batch 620, train_loss 0.713\n",
      "epoch 3, batch 630, train_loss 0.706\n",
      "epoch 3, batch 640, train_loss 0.715\n",
      "epoch 3, batch 650, train_loss 0.700\n",
      "epoch 3, batch 660, train_loss 0.700\n",
      "epoch 3, batch 670, train_loss 0.721\n",
      "epoch 3, batch 680, train_loss 0.717\n",
      "epoch 3, batch 690, train_loss 0.715\n",
      "epoch 3, batch 700, train_loss 0.717\n",
      "epoch 3, batch 710, train_loss 0.717\n",
      "epoch 3, batch 720, train_loss 0.703\n",
      "epoch 3, batch 730, train_loss 0.705\n",
      "epoch 3, batch 740, train_loss 0.714\n",
      "epoch 3, batch 750, train_loss 0.715\n",
      "epoch 3, batch 760, train_loss 0.708\n",
      "epoch 3, batch 770, train_loss 0.713\n",
      "epoch 3, batch 780, train_loss 0.709\n",
      "epoch 3, batch 790, train_loss 0.704\n",
      "epoch 3, batch 800, train_loss 0.714\n",
      "epoch 3, batch 810, train_loss 0.706\n",
      "epoch 3, batch 820, train_loss 0.715\n",
      "epoch 3, batch 830, train_loss 0.717\n",
      "epoch 3, batch 840, train_loss 0.720\n",
      "epoch 3, batch 850, train_loss 0.716\n",
      "epoch 3, batch 860, train_loss 0.720\n",
      "epoch 3, batch 870, train_loss 0.704\n",
      "epoch 3, batch 880, train_loss 0.718\n",
      "epoch 3, batch 890, train_loss 0.709\n",
      "epoch 3, batch 900, train_loss 0.712\n",
      "epoch 3, batch 910, train_loss 0.712\n",
      "epoch 3, batch 920, train_loss 0.714\n",
      "epoch 3, batch 930, train_loss 0.721\n",
      "epoch 3, batch 940, train_loss 0.710\n",
      "epoch 3, batch 950, train_loss 0.720\n",
      "epoch 3, batch 960, train_loss 0.705\n",
      "epoch 3, batch 970, train_loss 0.709\n",
      "epoch 3, batch 980, train_loss 0.712\n",
      "epoch 3, batch 990, train_loss 0.709\n",
      "epoch 3, batch 1000, train_loss 0.709\n",
      "epoch 3, batch 1010, train_loss 0.707\n",
      "epoch 3, batch 1020, train_loss 0.709\n",
      "epoch 3, batch 1030, train_loss 0.721\n",
      "epoch 3, batch 1040, train_loss 0.720\n",
      "epoch 3, batch 1050, train_loss 0.713\n",
      "epoch 3, batch 1060, train_loss 0.711\n",
      "epoch 3, batch 1070, train_loss 0.712\n",
      "epoch 3, batch 1080, train_loss 0.708\n",
      "epoch 3, batch 1090, train_loss 0.702\n",
      "epoch 3, batch 1100, train_loss 0.720\n",
      "epoch 3, batch 1110, train_loss 0.712\n",
      "epoch 3, batch 1120, train_loss 0.707\n",
      "epoch 3, batch 1130, train_loss 0.713\n",
      "epoch 3, batch 1140, train_loss 0.711\n",
      "epoch 3, batch 1150, train_loss 0.710\n",
      "epoch 3, batch 1160, train_loss 0.721\n",
      "epoch 3, batch 1170, train_loss 0.713\n",
      "epoch 3, batch 1180, train_loss 0.714\n",
      "epoch 3, batch 1190, train_loss 0.711\n",
      "epoch     3, train_loss 0.711, valid_loss 0.726, train_accuracy  69.25%, valid_accuracy  68.40%\n",
      "epoch 4, batch 0, train_loss 0.714\n",
      "epoch 4, batch 10, train_loss 0.707\n",
      "epoch 4, batch 20, train_loss 0.713\n",
      "epoch 4, batch 30, train_loss 0.701\n",
      "epoch 4, batch 40, train_loss 0.708\n",
      "epoch 4, batch 50, train_loss 0.713\n",
      "epoch 4, batch 60, train_loss 0.717\n",
      "epoch 4, batch 70, train_loss 0.711\n",
      "epoch 4, batch 80, train_loss 0.715\n",
      "epoch 4, batch 90, train_loss 0.714\n",
      "epoch 4, batch 100, train_loss 0.714\n",
      "epoch 4, batch 110, train_loss 0.709\n",
      "epoch 4, batch 120, train_loss 0.704\n",
      "epoch 4, batch 130, train_loss 0.718\n",
      "epoch 4, batch 140, train_loss 0.715\n",
      "epoch 4, batch 150, train_loss 0.706\n",
      "epoch 4, batch 160, train_loss 0.713\n",
      "epoch 4, batch 170, train_loss 0.721\n",
      "epoch 4, batch 180, train_loss 0.713\n",
      "epoch 4, batch 190, train_loss 0.724\n",
      "epoch 4, batch 200, train_loss 0.718\n",
      "epoch 4, batch 210, train_loss 0.713\n",
      "epoch 4, batch 220, train_loss 0.710\n",
      "epoch 4, batch 230, train_loss 0.715\n",
      "epoch 4, batch 240, train_loss 0.700\n",
      "epoch 4, batch 250, train_loss 0.713\n",
      "epoch 4, batch 260, train_loss 0.706\n",
      "epoch 4, batch 270, train_loss 0.715\n",
      "epoch 4, batch 280, train_loss 0.693\n",
      "epoch 4, batch 290, train_loss 0.714\n",
      "epoch 4, batch 300, train_loss 0.726\n",
      "epoch 4, batch 310, train_loss 0.709\n",
      "epoch 4, batch 320, train_loss 0.709\n",
      "epoch 4, batch 330, train_loss 0.701\n",
      "epoch 4, batch 340, train_loss 0.708\n",
      "epoch 4, batch 350, train_loss 0.712\n",
      "epoch 4, batch 360, train_loss 0.706\n",
      "epoch 4, batch 370, train_loss 0.709\n",
      "epoch 4, batch 380, train_loss 0.711\n",
      "epoch 4, batch 390, train_loss 0.707\n",
      "epoch 4, batch 400, train_loss 0.714\n",
      "epoch 4, batch 410, train_loss 0.711\n",
      "epoch 4, batch 420, train_loss 0.714\n",
      "epoch 4, batch 430, train_loss 0.716\n",
      "epoch 4, batch 440, train_loss 0.705\n",
      "epoch 4, batch 450, train_loss 0.711\n",
      "epoch 4, batch 460, train_loss 0.721\n",
      "epoch 4, batch 470, train_loss 0.703\n",
      "epoch 4, batch 480, train_loss 0.715\n",
      "epoch 4, batch 490, train_loss 0.710\n",
      "epoch 4, batch 500, train_loss 0.709\n",
      "epoch 4, batch 510, train_loss 0.712\n",
      "epoch 4, batch 520, train_loss 0.722\n",
      "epoch 4, batch 530, train_loss 0.707\n",
      "epoch 4, batch 540, train_loss 0.704\n",
      "epoch 4, batch 550, train_loss 0.703\n",
      "epoch 4, batch 560, train_loss 0.715\n",
      "epoch 4, batch 570, train_loss 0.709\n",
      "epoch 4, batch 580, train_loss 0.718\n",
      "epoch 4, batch 590, train_loss 0.711\n",
      "epoch 4, batch 600, train_loss 0.708\n",
      "epoch 4, batch 610, train_loss 0.716\n",
      "epoch 4, batch 620, train_loss 0.708\n",
      "epoch 4, batch 630, train_loss 0.710\n",
      "epoch 4, batch 640, train_loss 0.709\n",
      "epoch 4, batch 650, train_loss 0.713\n",
      "epoch 4, batch 660, train_loss 0.707\n",
      "epoch 4, batch 670, train_loss 0.715\n",
      "epoch 4, batch 680, train_loss 0.706\n",
      "epoch 4, batch 690, train_loss 0.708\n",
      "epoch 4, batch 700, train_loss 0.711\n",
      "epoch 4, batch 710, train_loss 0.704\n",
      "epoch 4, batch 720, train_loss 0.704\n",
      "epoch 4, batch 730, train_loss 0.706\n",
      "epoch 4, batch 740, train_loss 0.700\n",
      "epoch 4, batch 750, train_loss 0.707\n",
      "epoch 4, batch 760, train_loss 0.709\n",
      "epoch 4, batch 770, train_loss 0.710\n",
      "epoch 4, batch 780, train_loss 0.712\n",
      "epoch 4, batch 790, train_loss 0.707\n",
      "epoch 4, batch 800, train_loss 0.697\n",
      "epoch 4, batch 810, train_loss 0.702\n",
      "epoch 4, batch 820, train_loss 0.702\n",
      "epoch 4, batch 830, train_loss 0.709\n",
      "epoch 4, batch 840, train_loss 0.718\n",
      "epoch 4, batch 850, train_loss 0.712\n",
      "epoch 4, batch 860, train_loss 0.704\n",
      "epoch 4, batch 870, train_loss 0.712\n",
      "epoch 4, batch 880, train_loss 0.713\n",
      "epoch 4, batch 890, train_loss 0.711\n",
      "epoch 4, batch 900, train_loss 0.705\n",
      "epoch 4, batch 910, train_loss 0.709\n",
      "epoch 4, batch 920, train_loss 0.715\n",
      "epoch 4, batch 930, train_loss 0.714\n",
      "epoch 4, batch 940, train_loss 0.713\n",
      "epoch 4, batch 950, train_loss 0.711\n",
      "epoch 4, batch 960, train_loss 0.713\n",
      "epoch 4, batch 970, train_loss 0.713\n",
      "epoch 4, batch 980, train_loss 0.712\n",
      "epoch 4, batch 990, train_loss 0.707\n",
      "epoch 4, batch 1000, train_loss 0.714\n",
      "epoch 4, batch 1010, train_loss 0.706\n",
      "epoch 4, batch 1020, train_loss 0.714\n",
      "epoch 4, batch 1030, train_loss 0.701\n",
      "epoch 4, batch 1040, train_loss 0.707\n",
      "epoch 4, batch 1050, train_loss 0.719\n",
      "epoch 4, batch 1060, train_loss 0.712\n",
      "epoch 4, batch 1070, train_loss 0.708\n",
      "epoch 4, batch 1080, train_loss 0.712\n",
      "epoch 4, batch 1090, train_loss 0.701\n",
      "epoch 4, batch 1100, train_loss 0.700\n",
      "epoch 4, batch 1110, train_loss 0.711\n",
      "epoch 4, batch 1120, train_loss 0.700\n",
      "epoch 4, batch 1130, train_loss 0.706\n",
      "epoch 4, batch 1140, train_loss 0.709\n",
      "epoch 4, batch 1150, train_loss 0.713\n",
      "epoch 4, batch 1160, train_loss 0.716\n",
      "epoch 4, batch 1170, train_loss 0.700\n",
      "epoch 4, batch 1180, train_loss 0.708\n",
      "epoch 4, batch 1190, train_loss 0.697\n",
      "epoch     4, train_loss 0.708, valid_loss 0.725, train_accuracy  69.38%, valid_accuracy  68.50%\n",
      "epoch 5, batch 0, train_loss 0.711\n",
      "epoch 5, batch 10, train_loss 0.699\n",
      "epoch 5, batch 20, train_loss 0.704\n",
      "epoch 5, batch 30, train_loss 0.706\n",
      "epoch 5, batch 40, train_loss 0.713\n",
      "epoch 5, batch 50, train_loss 0.716\n",
      "epoch 5, batch 60, train_loss 0.712\n",
      "epoch 5, batch 70, train_loss 0.720\n",
      "epoch 5, batch 80, train_loss 0.705\n",
      "epoch 5, batch 90, train_loss 0.715\n",
      "epoch 5, batch 100, train_loss 0.706\n",
      "epoch 5, batch 110, train_loss 0.709\n",
      "epoch 5, batch 120, train_loss 0.709\n",
      "epoch 5, batch 130, train_loss 0.715\n",
      "epoch 5, batch 140, train_loss 0.705\n",
      "epoch 5, batch 150, train_loss 0.711\n",
      "epoch 5, batch 160, train_loss 0.709\n",
      "epoch 5, batch 170, train_loss 0.716\n",
      "epoch 5, batch 180, train_loss 0.702\n",
      "epoch 5, batch 190, train_loss 0.708\n",
      "epoch 5, batch 200, train_loss 0.712\n",
      "epoch 5, batch 210, train_loss 0.698\n",
      "epoch 5, batch 220, train_loss 0.708\n",
      "epoch 5, batch 230, train_loss 0.713\n",
      "epoch 5, batch 240, train_loss 0.716\n",
      "epoch 5, batch 250, train_loss 0.725\n",
      "epoch 5, batch 260, train_loss 0.710\n",
      "epoch 5, batch 270, train_loss 0.716\n",
      "epoch 5, batch 280, train_loss 0.700\n",
      "epoch 5, batch 290, train_loss 0.705\n",
      "epoch 5, batch 300, train_loss 0.712\n",
      "epoch 5, batch 310, train_loss 0.706\n",
      "epoch 5, batch 320, train_loss 0.715\n",
      "epoch 5, batch 330, train_loss 0.710\n",
      "epoch 5, batch 340, train_loss 0.705\n",
      "epoch 5, batch 350, train_loss 0.708\n",
      "epoch 5, batch 360, train_loss 0.724\n",
      "epoch 5, batch 370, train_loss 0.710\n",
      "epoch 5, batch 380, train_loss 0.705\n",
      "epoch 5, batch 390, train_loss 0.703\n",
      "epoch 5, batch 400, train_loss 0.710\n",
      "epoch 5, batch 410, train_loss 0.705\n",
      "epoch 5, batch 420, train_loss 0.706\n",
      "epoch 5, batch 430, train_loss 0.709\n",
      "epoch 5, batch 440, train_loss 0.715\n",
      "epoch 5, batch 450, train_loss 0.710\n",
      "epoch 5, batch 460, train_loss 0.715\n",
      "epoch 5, batch 470, train_loss 0.709\n",
      "epoch 5, batch 480, train_loss 0.709\n",
      "epoch 5, batch 490, train_loss 0.725\n",
      "epoch 5, batch 500, train_loss 0.714\n",
      "epoch 5, batch 510, train_loss 0.704\n",
      "epoch 5, batch 520, train_loss 0.704\n",
      "epoch 5, batch 530, train_loss 0.705\n",
      "epoch 5, batch 540, train_loss 0.702\n",
      "epoch 5, batch 550, train_loss 0.713\n",
      "epoch 5, batch 560, train_loss 0.708\n",
      "epoch 5, batch 570, train_loss 0.707\n",
      "epoch 5, batch 580, train_loss 0.705\n",
      "epoch 5, batch 590, train_loss 0.708\n",
      "epoch 5, batch 600, train_loss 0.697\n",
      "epoch 5, batch 610, train_loss 0.703\n",
      "epoch 5, batch 620, train_loss 0.712\n",
      "epoch 5, batch 630, train_loss 0.698\n",
      "epoch 5, batch 640, train_loss 0.713\n",
      "epoch 5, batch 650, train_loss 0.705\n",
      "epoch 5, batch 660, train_loss 0.706\n",
      "epoch 5, batch 670, train_loss 0.707\n",
      "epoch 5, batch 680, train_loss 0.704\n",
      "epoch 5, batch 690, train_loss 0.707\n",
      "epoch 5, batch 700, train_loss 0.706\n",
      "epoch 5, batch 710, train_loss 0.710\n",
      "epoch 5, batch 720, train_loss 0.706\n",
      "epoch 5, batch 730, train_loss 0.705\n",
      "epoch 5, batch 740, train_loss 0.697\n",
      "epoch 5, batch 750, train_loss 0.703\n",
      "epoch 5, batch 760, train_loss 0.714\n",
      "epoch 5, batch 770, train_loss 0.698\n",
      "epoch 5, batch 780, train_loss 0.709\n",
      "epoch 5, batch 790, train_loss 0.719\n",
      "epoch 5, batch 800, train_loss 0.706\n",
      "epoch 5, batch 810, train_loss 0.706\n",
      "epoch 5, batch 820, train_loss 0.703\n",
      "epoch 5, batch 830, train_loss 0.709\n",
      "epoch 5, batch 840, train_loss 0.700\n",
      "epoch 5, batch 850, train_loss 0.707\n",
      "epoch 5, batch 860, train_loss 0.711\n",
      "epoch 5, batch 870, train_loss 0.717\n",
      "epoch 5, batch 880, train_loss 0.704\n",
      "epoch 5, batch 890, train_loss 0.709\n",
      "epoch 5, batch 900, train_loss 0.706\n",
      "epoch 5, batch 910, train_loss 0.709\n",
      "epoch 5, batch 920, train_loss 0.701\n",
      "epoch 5, batch 930, train_loss 0.710\n",
      "epoch 5, batch 940, train_loss 0.707\n",
      "epoch 5, batch 950, train_loss 0.709\n",
      "epoch 5, batch 960, train_loss 0.700\n",
      "epoch 5, batch 970, train_loss 0.715\n",
      "epoch 5, batch 980, train_loss 0.702\n",
      "epoch 5, batch 990, train_loss 0.712\n",
      "epoch 5, batch 1000, train_loss 0.710\n",
      "epoch 5, batch 1010, train_loss 0.711\n",
      "epoch 5, batch 1020, train_loss 0.716\n",
      "epoch 5, batch 1030, train_loss 0.703\n",
      "epoch 5, batch 1040, train_loss 0.703\n",
      "epoch 5, batch 1050, train_loss 0.702\n",
      "epoch 5, batch 1060, train_loss 0.704\n",
      "epoch 5, batch 1070, train_loss 0.705\n",
      "epoch 5, batch 1080, train_loss 0.699\n",
      "epoch 5, batch 1090, train_loss 0.708\n",
      "epoch 5, batch 1100, train_loss 0.720\n",
      "epoch 5, batch 1110, train_loss 0.716\n",
      "epoch 5, batch 1120, train_loss 0.704\n",
      "epoch 5, batch 1130, train_loss 0.704\n",
      "epoch 5, batch 1140, train_loss 0.717\n",
      "epoch 5, batch 1150, train_loss 0.696\n",
      "epoch 5, batch 1160, train_loss 0.708\n",
      "epoch 5, batch 1170, train_loss 0.703\n",
      "epoch 5, batch 1180, train_loss 0.706\n",
      "epoch 5, batch 1190, train_loss 0.709\n",
      "epoch     5, train_loss 0.706, valid_loss 0.724, train_accuracy  69.46%, valid_accuracy  68.54%\n",
      "epoch 6, batch 0, train_loss 0.710\n",
      "epoch 6, batch 10, train_loss 0.708\n",
      "epoch 6, batch 20, train_loss 0.709\n",
      "epoch 6, batch 30, train_loss 0.695\n",
      "epoch 6, batch 40, train_loss 0.700\n",
      "epoch 6, batch 50, train_loss 0.703\n",
      "epoch 6, batch 60, train_loss 0.696\n",
      "epoch 6, batch 70, train_loss 0.717\n",
      "epoch 6, batch 80, train_loss 0.710\n",
      "epoch 6, batch 90, train_loss 0.702\n",
      "epoch 6, batch 100, train_loss 0.722\n",
      "epoch 6, batch 110, train_loss 0.699\n",
      "epoch 6, batch 120, train_loss 0.716\n",
      "epoch 6, batch 130, train_loss 0.704\n",
      "epoch 6, batch 140, train_loss 0.704\n",
      "epoch 6, batch 150, train_loss 0.706\n",
      "epoch 6, batch 160, train_loss 0.701\n",
      "epoch 6, batch 170, train_loss 0.703\n",
      "epoch 6, batch 180, train_loss 0.709\n",
      "epoch 6, batch 190, train_loss 0.707\n",
      "epoch 6, batch 200, train_loss 0.702\n",
      "epoch 6, batch 210, train_loss 0.714\n",
      "epoch 6, batch 220, train_loss 0.705\n",
      "epoch 6, batch 230, train_loss 0.692\n",
      "epoch 6, batch 240, train_loss 0.712\n",
      "epoch 6, batch 250, train_loss 0.708\n",
      "epoch 6, batch 260, train_loss 0.710\n",
      "epoch 6, batch 270, train_loss 0.710\n",
      "epoch 6, batch 280, train_loss 0.700\n",
      "epoch 6, batch 290, train_loss 0.706\n",
      "epoch 6, batch 300, train_loss 0.706\n",
      "epoch 6, batch 310, train_loss 0.708\n",
      "epoch 6, batch 320, train_loss 0.702\n",
      "epoch 6, batch 330, train_loss 0.704\n",
      "epoch 6, batch 340, train_loss 0.712\n",
      "epoch 6, batch 350, train_loss 0.708\n",
      "epoch 6, batch 360, train_loss 0.708\n",
      "epoch 6, batch 370, train_loss 0.707\n",
      "epoch 6, batch 380, train_loss 0.708\n",
      "epoch 6, batch 390, train_loss 0.712\n",
      "epoch 6, batch 400, train_loss 0.705\n",
      "epoch 6, batch 410, train_loss 0.710\n",
      "epoch 6, batch 420, train_loss 0.712\n",
      "epoch 6, batch 430, train_loss 0.705\n",
      "epoch 6, batch 440, train_loss 0.700\n",
      "epoch 6, batch 450, train_loss 0.708\n",
      "epoch 6, batch 460, train_loss 0.716\n",
      "epoch 6, batch 470, train_loss 0.704\n",
      "epoch 6, batch 480, train_loss 0.709\n",
      "epoch 6, batch 490, train_loss 0.711\n",
      "epoch 6, batch 500, train_loss 0.709\n",
      "epoch 6, batch 510, train_loss 0.696\n",
      "epoch 6, batch 520, train_loss 0.705\n",
      "epoch 6, batch 530, train_loss 0.713\n",
      "epoch 6, batch 540, train_loss 0.704\n",
      "epoch 6, batch 550, train_loss 0.701\n",
      "epoch 6, batch 560, train_loss 0.695\n",
      "epoch 6, batch 570, train_loss 0.702\n",
      "epoch 6, batch 580, train_loss 0.697\n",
      "epoch 6, batch 590, train_loss 0.705\n",
      "epoch 6, batch 600, train_loss 0.705\n",
      "epoch 6, batch 610, train_loss 0.701\n",
      "epoch 6, batch 620, train_loss 0.712\n",
      "epoch 6, batch 630, train_loss 0.702\n",
      "epoch 6, batch 640, train_loss 0.717\n",
      "epoch 6, batch 650, train_loss 0.700\n",
      "epoch 6, batch 660, train_loss 0.704\n",
      "epoch 6, batch 670, train_loss 0.702\n",
      "epoch 6, batch 680, train_loss 0.712\n",
      "epoch 6, batch 690, train_loss 0.703\n",
      "epoch 6, batch 700, train_loss 0.708\n",
      "epoch 6, batch 710, train_loss 0.714\n",
      "epoch 6, batch 720, train_loss 0.700\n",
      "epoch 6, batch 730, train_loss 0.711\n",
      "epoch 6, batch 740, train_loss 0.705\n",
      "epoch 6, batch 750, train_loss 0.708\n",
      "epoch 6, batch 760, train_loss 0.703\n",
      "epoch 6, batch 770, train_loss 0.711\n",
      "epoch 6, batch 780, train_loss 0.704\n",
      "epoch 6, batch 790, train_loss 0.706\n",
      "epoch 6, batch 800, train_loss 0.704\n",
      "epoch 6, batch 810, train_loss 0.705\n",
      "epoch 6, batch 820, train_loss 0.716\n",
      "epoch 6, batch 830, train_loss 0.695\n",
      "epoch 6, batch 840, train_loss 0.713\n",
      "epoch 6, batch 850, train_loss 0.710\n",
      "epoch 6, batch 860, train_loss 0.712\n",
      "epoch 6, batch 870, train_loss 0.704\n",
      "epoch 6, batch 880, train_loss 0.706\n",
      "epoch 6, batch 890, train_loss 0.698\n",
      "epoch 6, batch 900, train_loss 0.716\n",
      "epoch 6, batch 910, train_loss 0.706\n",
      "epoch 6, batch 920, train_loss 0.706\n",
      "epoch 6, batch 930, train_loss 0.707\n",
      "epoch 6, batch 940, train_loss 0.706\n",
      "epoch 6, batch 950, train_loss 0.705\n",
      "epoch 6, batch 960, train_loss 0.702\n",
      "epoch 6, batch 970, train_loss 0.700\n",
      "epoch 6, batch 980, train_loss 0.700\n",
      "epoch 6, batch 990, train_loss 0.699\n",
      "epoch 6, batch 1000, train_loss 0.714\n",
      "epoch 6, batch 1010, train_loss 0.710\n",
      "epoch 6, batch 1020, train_loss 0.709\n",
      "epoch 6, batch 1030, train_loss 0.710\n",
      "epoch 6, batch 1040, train_loss 0.707\n",
      "epoch 6, batch 1050, train_loss 0.704\n",
      "epoch 6, batch 1060, train_loss 0.699\n",
      "epoch 6, batch 1070, train_loss 0.713\n",
      "epoch 6, batch 1080, train_loss 0.697\n",
      "epoch 6, batch 1090, train_loss 0.713\n",
      "epoch 6, batch 1100, train_loss 0.702\n",
      "epoch 6, batch 1110, train_loss 0.703\n",
      "epoch 6, batch 1120, train_loss 0.717\n",
      "epoch 6, batch 1130, train_loss 0.710\n",
      "epoch 6, batch 1140, train_loss 0.718\n",
      "epoch 6, batch 1150, train_loss 0.713\n",
      "epoch 6, batch 1160, train_loss 0.713\n",
      "epoch 6, batch 1170, train_loss 0.694\n",
      "epoch 6, batch 1180, train_loss 0.707\n",
      "epoch 6, batch 1190, train_loss 0.697\n",
      "epoch     6, train_loss 0.706, valid_loss 0.724, train_accuracy  69.54%, valid_accuracy  68.49%\n",
      "epoch 7, batch 0, train_loss 0.712\n",
      "epoch 7, batch 10, train_loss 0.704\n",
      "epoch 7, batch 20, train_loss 0.696\n",
      "epoch 7, batch 30, train_loss 0.711\n",
      "epoch 7, batch 40, train_loss 0.700\n",
      "epoch 7, batch 50, train_loss 0.706\n",
      "epoch 7, batch 60, train_loss 0.698\n",
      "epoch 7, batch 70, train_loss 0.702\n",
      "epoch 7, batch 80, train_loss 0.701\n",
      "epoch 7, batch 90, train_loss 0.702\n",
      "epoch 7, batch 100, train_loss 0.704\n",
      "epoch 7, batch 110, train_loss 0.695\n",
      "epoch 7, batch 120, train_loss 0.707\n",
      "epoch 7, batch 130, train_loss 0.694\n",
      "epoch 7, batch 140, train_loss 0.704\n",
      "epoch 7, batch 150, train_loss 0.706\n",
      "epoch 7, batch 160, train_loss 0.711\n",
      "epoch 7, batch 170, train_loss 0.714\n",
      "epoch 7, batch 180, train_loss 0.699\n",
      "epoch 7, batch 190, train_loss 0.716\n",
      "epoch 7, batch 200, train_loss 0.714\n",
      "epoch 7, batch 210, train_loss 0.700\n",
      "epoch 7, batch 220, train_loss 0.706\n",
      "epoch 7, batch 230, train_loss 0.715\n",
      "epoch 7, batch 240, train_loss 0.710\n",
      "epoch 7, batch 250, train_loss 0.710\n",
      "epoch 7, batch 260, train_loss 0.709\n",
      "epoch 7, batch 270, train_loss 0.703\n",
      "epoch 7, batch 280, train_loss 0.707\n",
      "epoch 7, batch 290, train_loss 0.700\n",
      "epoch 7, batch 300, train_loss 0.706\n",
      "epoch 7, batch 310, train_loss 0.709\n",
      "epoch 7, batch 320, train_loss 0.702\n",
      "epoch 7, batch 330, train_loss 0.712\n",
      "epoch 7, batch 340, train_loss 0.708\n",
      "epoch 7, batch 350, train_loss 0.709\n",
      "epoch 7, batch 360, train_loss 0.710\n",
      "epoch 7, batch 370, train_loss 0.708\n",
      "epoch 7, batch 380, train_loss 0.706\n",
      "epoch 7, batch 390, train_loss 0.706\n",
      "epoch 7, batch 400, train_loss 0.701\n",
      "epoch 7, batch 410, train_loss 0.707\n",
      "epoch 7, batch 420, train_loss 0.711\n",
      "epoch 7, batch 430, train_loss 0.699\n",
      "epoch 7, batch 440, train_loss 0.699\n",
      "epoch 7, batch 450, train_loss 0.707\n",
      "epoch 7, batch 460, train_loss 0.698\n",
      "epoch 7, batch 470, train_loss 0.700\n",
      "epoch 7, batch 480, train_loss 0.714\n",
      "epoch 7, batch 490, train_loss 0.704\n",
      "epoch 7, batch 500, train_loss 0.702\n",
      "epoch 7, batch 510, train_loss 0.705\n",
      "epoch 7, batch 520, train_loss 0.702\n",
      "epoch 7, batch 530, train_loss 0.711\n",
      "epoch 7, batch 540, train_loss 0.705\n",
      "epoch 7, batch 550, train_loss 0.704\n",
      "epoch 7, batch 560, train_loss 0.699\n",
      "epoch 7, batch 570, train_loss 0.715\n",
      "epoch 7, batch 580, train_loss 0.707\n",
      "epoch 7, batch 590, train_loss 0.695\n",
      "epoch 7, batch 600, train_loss 0.711\n",
      "epoch 7, batch 610, train_loss 0.702\n",
      "epoch 7, batch 620, train_loss 0.712\n",
      "epoch 7, batch 630, train_loss 0.714\n",
      "epoch 7, batch 640, train_loss 0.709\n",
      "epoch 7, batch 650, train_loss 0.709\n",
      "epoch 7, batch 660, train_loss 0.700\n",
      "epoch 7, batch 670, train_loss 0.702\n",
      "epoch 7, batch 680, train_loss 0.703\n",
      "epoch 7, batch 690, train_loss 0.703\n",
      "epoch 7, batch 700, train_loss 0.705\n",
      "epoch 7, batch 710, train_loss 0.707\n",
      "epoch 7, batch 720, train_loss 0.703\n",
      "epoch 7, batch 730, train_loss 0.700\n",
      "epoch 7, batch 740, train_loss 0.699\n",
      "epoch 7, batch 750, train_loss 0.705\n",
      "epoch 7, batch 760, train_loss 0.703\n",
      "epoch 7, batch 770, train_loss 0.696\n",
      "epoch 7, batch 780, train_loss 0.697\n",
      "epoch 7, batch 790, train_loss 0.700\n",
      "epoch 7, batch 800, train_loss 0.707\n",
      "epoch 7, batch 810, train_loss 0.705\n",
      "epoch 7, batch 820, train_loss 0.703\n",
      "epoch 7, batch 830, train_loss 0.713\n",
      "epoch 7, batch 840, train_loss 0.702\n",
      "epoch 7, batch 850, train_loss 0.690\n",
      "epoch 7, batch 860, train_loss 0.702\n",
      "epoch 7, batch 870, train_loss 0.709\n",
      "epoch 7, batch 880, train_loss 0.701\n",
      "epoch 7, batch 890, train_loss 0.709\n",
      "epoch 7, batch 900, train_loss 0.712\n",
      "epoch 7, batch 910, train_loss 0.691\n",
      "epoch 7, batch 920, train_loss 0.702\n",
      "epoch 7, batch 930, train_loss 0.705\n",
      "epoch 7, batch 940, train_loss 0.711\n",
      "epoch 7, batch 950, train_loss 0.713\n",
      "epoch 7, batch 960, train_loss 0.711\n",
      "epoch 7, batch 970, train_loss 0.709\n",
      "epoch 7, batch 980, train_loss 0.702\n",
      "epoch 7, batch 990, train_loss 0.704\n",
      "epoch 7, batch 1000, train_loss 0.708\n",
      "epoch 7, batch 1010, train_loss 0.704\n",
      "epoch 7, batch 1020, train_loss 0.711\n",
      "epoch 7, batch 1030, train_loss 0.704\n",
      "epoch 7, batch 1040, train_loss 0.705\n",
      "epoch 7, batch 1050, train_loss 0.707\n",
      "epoch 7, batch 1060, train_loss 0.708\n",
      "epoch 7, batch 1070, train_loss 0.698\n",
      "epoch 7, batch 1080, train_loss 0.698\n",
      "epoch 7, batch 1090, train_loss 0.705\n",
      "epoch 7, batch 1100, train_loss 0.706\n",
      "epoch 7, batch 1110, train_loss 0.704\n",
      "epoch 7, batch 1120, train_loss 0.713\n",
      "epoch 7, batch 1130, train_loss 0.695\n",
      "epoch 7, batch 1140, train_loss 0.703\n",
      "epoch 7, batch 1150, train_loss 0.698\n",
      "epoch 7, batch 1160, train_loss 0.704\n",
      "epoch 7, batch 1170, train_loss 0.708\n",
      "epoch 7, batch 1180, train_loss 0.703\n",
      "epoch 7, batch 1190, train_loss 0.706\n",
      "epoch     7, train_loss 0.703, valid_loss 0.723, train_accuracy  69.63%, valid_accuracy  68.59%\n",
      "epoch 8, batch 0, train_loss 0.708\n",
      "epoch 8, batch 10, train_loss 0.700\n",
      "epoch 8, batch 20, train_loss 0.703\n",
      "epoch 8, batch 30, train_loss 0.715\n",
      "epoch 8, batch 40, train_loss 0.705\n",
      "epoch 8, batch 50, train_loss 0.704\n",
      "epoch 8, batch 60, train_loss 0.711\n",
      "epoch 8, batch 70, train_loss 0.704\n",
      "epoch 8, batch 80, train_loss 0.706\n",
      "epoch 8, batch 90, train_loss 0.700\n",
      "epoch 8, batch 100, train_loss 0.704\n",
      "epoch 8, batch 110, train_loss 0.702\n",
      "epoch 8, batch 120, train_loss 0.696\n",
      "epoch 8, batch 130, train_loss 0.702\n",
      "epoch 8, batch 140, train_loss 0.714\n",
      "epoch 8, batch 150, train_loss 0.702\n",
      "epoch 8, batch 160, train_loss 0.707\n",
      "epoch 8, batch 170, train_loss 0.704\n",
      "epoch 8, batch 180, train_loss 0.711\n",
      "epoch 8, batch 190, train_loss 0.705\n",
      "epoch 8, batch 200, train_loss 0.711\n",
      "epoch 8, batch 210, train_loss 0.707\n",
      "epoch 8, batch 220, train_loss 0.707\n",
      "epoch 8, batch 230, train_loss 0.699\n",
      "epoch 8, batch 240, train_loss 0.701\n",
      "epoch 8, batch 250, train_loss 0.695\n",
      "epoch 8, batch 260, train_loss 0.703\n",
      "epoch 8, batch 270, train_loss 0.704\n",
      "epoch 8, batch 280, train_loss 0.709\n",
      "epoch 8, batch 290, train_loss 0.705\n",
      "epoch 8, batch 300, train_loss 0.698\n",
      "epoch 8, batch 310, train_loss 0.702\n",
      "epoch 8, batch 320, train_loss 0.704\n",
      "epoch 8, batch 330, train_loss 0.699\n",
      "epoch 8, batch 340, train_loss 0.701\n",
      "epoch 8, batch 350, train_loss 0.698\n",
      "epoch 8, batch 360, train_loss 0.700\n",
      "epoch 8, batch 370, train_loss 0.704\n",
      "epoch 8, batch 380, train_loss 0.707\n",
      "epoch 8, batch 390, train_loss 0.688\n",
      "epoch 8, batch 400, train_loss 0.704\n",
      "epoch 8, batch 410, train_loss 0.701\n",
      "epoch 8, batch 420, train_loss 0.703\n",
      "epoch 8, batch 430, train_loss 0.705\n",
      "epoch 8, batch 440, train_loss 0.706\n",
      "epoch 8, batch 450, train_loss 0.704\n",
      "epoch 8, batch 460, train_loss 0.704\n",
      "epoch 8, batch 470, train_loss 0.706\n",
      "epoch 8, batch 480, train_loss 0.711\n",
      "epoch 8, batch 490, train_loss 0.704\n",
      "epoch 8, batch 500, train_loss 0.708\n",
      "epoch 8, batch 510, train_loss 0.698\n",
      "epoch 8, batch 520, train_loss 0.703\n",
      "epoch 8, batch 530, train_loss 0.704\n",
      "epoch 8, batch 540, train_loss 0.699\n",
      "epoch 8, batch 550, train_loss 0.708\n",
      "epoch 8, batch 560, train_loss 0.692\n",
      "epoch 8, batch 570, train_loss 0.703\n",
      "epoch 8, batch 580, train_loss 0.710\n",
      "epoch 8, batch 590, train_loss 0.701\n",
      "epoch 8, batch 600, train_loss 0.708\n",
      "epoch 8, batch 610, train_loss 0.707\n",
      "epoch 8, batch 620, train_loss 0.710\n",
      "epoch 8, batch 630, train_loss 0.707\n",
      "epoch 8, batch 640, train_loss 0.712\n",
      "epoch 8, batch 650, train_loss 0.706\n",
      "epoch 8, batch 660, train_loss 0.709\n",
      "epoch 8, batch 670, train_loss 0.709\n",
      "epoch 8, batch 680, train_loss 0.698\n",
      "epoch 8, batch 690, train_loss 0.709\n",
      "epoch 8, batch 700, train_loss 0.700\n",
      "epoch 8, batch 710, train_loss 0.704\n",
      "epoch 8, batch 720, train_loss 0.696\n",
      "epoch 8, batch 730, train_loss 0.707\n",
      "epoch 8, batch 740, train_loss 0.697\n",
      "epoch 8, batch 750, train_loss 0.706\n",
      "epoch 8, batch 760, train_loss 0.714\n",
      "epoch 8, batch 770, train_loss 0.711\n",
      "epoch 8, batch 780, train_loss 0.708\n",
      "epoch 8, batch 790, train_loss 0.717\n",
      "epoch 8, batch 800, train_loss 0.716\n",
      "epoch 8, batch 810, train_loss 0.709\n",
      "epoch 8, batch 820, train_loss 0.701\n",
      "epoch 8, batch 830, train_loss 0.702\n",
      "epoch 8, batch 840, train_loss 0.696\n",
      "epoch 8, batch 850, train_loss 0.706\n",
      "epoch 8, batch 860, train_loss 0.693\n",
      "epoch 8, batch 870, train_loss 0.712\n",
      "epoch 8, batch 880, train_loss 0.708\n",
      "epoch 8, batch 890, train_loss 0.697\n",
      "epoch 8, batch 900, train_loss 0.718\n",
      "epoch 8, batch 910, train_loss 0.701\n",
      "epoch 8, batch 920, train_loss 0.710\n",
      "epoch 8, batch 930, train_loss 0.709\n",
      "epoch 8, batch 940, train_loss 0.711\n",
      "epoch 8, batch 950, train_loss 0.721\n",
      "epoch 8, batch 960, train_loss 0.703\n",
      "epoch 8, batch 970, train_loss 0.704\n",
      "epoch 8, batch 980, train_loss 0.710\n",
      "epoch 8, batch 990, train_loss 0.710\n",
      "epoch 8, batch 1000, train_loss 0.712\n",
      "epoch 8, batch 1010, train_loss 0.709\n",
      "epoch 8, batch 1020, train_loss 0.700\n",
      "epoch 8, batch 1030, train_loss 0.701\n",
      "epoch 8, batch 1040, train_loss 0.695\n",
      "epoch 8, batch 1050, train_loss 0.709\n",
      "epoch 8, batch 1060, train_loss 0.714\n",
      "epoch 8, batch 1070, train_loss 0.701\n",
      "epoch 8, batch 1080, train_loss 0.708\n",
      "epoch 8, batch 1090, train_loss 0.700\n",
      "epoch 8, batch 1100, train_loss 0.700\n",
      "epoch 8, batch 1110, train_loss 0.704\n",
      "epoch 8, batch 1120, train_loss 0.701\n",
      "epoch 8, batch 1130, train_loss 0.705\n",
      "epoch 8, batch 1140, train_loss 0.703\n",
      "epoch 8, batch 1150, train_loss 0.697\n",
      "epoch 8, batch 1160, train_loss 0.693\n",
      "epoch 8, batch 1170, train_loss 0.706\n",
      "epoch 8, batch 1180, train_loss 0.699\n",
      "epoch 8, batch 1190, train_loss 0.713\n",
      "epoch     8, train_loss 0.702, valid_loss 0.722, train_accuracy  69.71%, valid_accuracy  68.57%\n",
      "epoch 9, batch 0, train_loss 0.701\n",
      "epoch 9, batch 10, train_loss 0.696\n",
      "epoch 9, batch 20, train_loss 0.703\n",
      "epoch 9, batch 30, train_loss 0.696\n",
      "epoch 9, batch 40, train_loss 0.701\n",
      "epoch 9, batch 50, train_loss 0.702\n",
      "epoch 9, batch 60, train_loss 0.703\n",
      "epoch 9, batch 70, train_loss 0.707\n",
      "epoch 9, batch 80, train_loss 0.705\n",
      "epoch 9, batch 90, train_loss 0.707\n",
      "epoch 9, batch 100, train_loss 0.702\n",
      "epoch 9, batch 110, train_loss 0.687\n",
      "epoch 9, batch 120, train_loss 0.711\n",
      "epoch 9, batch 130, train_loss 0.705\n",
      "epoch 9, batch 140, train_loss 0.692\n",
      "epoch 9, batch 150, train_loss 0.700\n",
      "epoch 9, batch 160, train_loss 0.698\n",
      "epoch 9, batch 170, train_loss 0.715\n",
      "epoch 9, batch 180, train_loss 0.707\n",
      "epoch 9, batch 190, train_loss 0.704\n",
      "epoch 9, batch 200, train_loss 0.713\n",
      "epoch 9, batch 210, train_loss 0.686\n",
      "epoch 9, batch 220, train_loss 0.698\n",
      "epoch 9, batch 230, train_loss 0.703\n",
      "epoch 9, batch 240, train_loss 0.703\n",
      "epoch 9, batch 250, train_loss 0.703\n",
      "epoch 9, batch 260, train_loss 0.696\n",
      "epoch 9, batch 270, train_loss 0.703\n",
      "epoch 9, batch 280, train_loss 0.716\n",
      "epoch 9, batch 290, train_loss 0.702\n",
      "epoch 9, batch 300, train_loss 0.703\n",
      "epoch 9, batch 310, train_loss 0.709\n",
      "epoch 9, batch 320, train_loss 0.704\n",
      "epoch 9, batch 330, train_loss 0.696\n",
      "epoch 9, batch 340, train_loss 0.698\n",
      "epoch 9, batch 350, train_loss 0.708\n",
      "epoch 9, batch 360, train_loss 0.694\n",
      "epoch 9, batch 370, train_loss 0.700\n",
      "epoch 9, batch 380, train_loss 0.707\n",
      "epoch 9, batch 390, train_loss 0.702\n",
      "epoch 9, batch 400, train_loss 0.697\n",
      "epoch 9, batch 410, train_loss 0.707\n",
      "epoch 9, batch 420, train_loss 0.703\n",
      "epoch 9, batch 430, train_loss 0.710\n",
      "epoch 9, batch 440, train_loss 0.695\n",
      "epoch 9, batch 450, train_loss 0.701\n",
      "epoch 9, batch 460, train_loss 0.699\n",
      "epoch 9, batch 470, train_loss 0.709\n",
      "epoch 9, batch 480, train_loss 0.698\n",
      "epoch 9, batch 490, train_loss 0.704\n",
      "epoch 9, batch 500, train_loss 0.707\n",
      "epoch 9, batch 510, train_loss 0.697\n",
      "epoch 9, batch 520, train_loss 0.700\n",
      "epoch 9, batch 530, train_loss 0.699\n",
      "epoch 9, batch 540, train_loss 0.708\n",
      "epoch 9, batch 550, train_loss 0.696\n",
      "epoch 9, batch 560, train_loss 0.704\n",
      "epoch 9, batch 570, train_loss 0.698\n",
      "epoch 9, batch 580, train_loss 0.701\n",
      "epoch 9, batch 590, train_loss 0.699\n",
      "epoch 9, batch 600, train_loss 0.695\n",
      "epoch 9, batch 610, train_loss 0.700\n",
      "epoch 9, batch 620, train_loss 0.706\n",
      "epoch 9, batch 630, train_loss 0.709\n",
      "epoch 9, batch 640, train_loss 0.693\n",
      "epoch 9, batch 650, train_loss 0.704\n",
      "epoch 9, batch 660, train_loss 0.706\n",
      "epoch 9, batch 670, train_loss 0.695\n",
      "epoch 9, batch 680, train_loss 0.698\n",
      "epoch 9, batch 690, train_loss 0.708\n",
      "epoch 9, batch 700, train_loss 0.711\n",
      "epoch 9, batch 710, train_loss 0.701\n",
      "epoch 9, batch 720, train_loss 0.697\n",
      "epoch 9, batch 730, train_loss 0.701\n",
      "epoch 9, batch 740, train_loss 0.713\n",
      "epoch 9, batch 750, train_loss 0.697\n",
      "epoch 9, batch 760, train_loss 0.709\n",
      "epoch 9, batch 770, train_loss 0.702\n",
      "epoch 9, batch 780, train_loss 0.700\n",
      "epoch 9, batch 790, train_loss 0.708\n",
      "epoch 9, batch 800, train_loss 0.701\n",
      "epoch 9, batch 810, train_loss 0.703\n",
      "epoch 9, batch 820, train_loss 0.704\n",
      "epoch 9, batch 830, train_loss 0.699\n",
      "epoch 9, batch 840, train_loss 0.699\n",
      "epoch 9, batch 850, train_loss 0.702\n",
      "epoch 9, batch 860, train_loss 0.706\n",
      "epoch 9, batch 870, train_loss 0.715\n",
      "epoch 9, batch 880, train_loss 0.702\n",
      "epoch 9, batch 890, train_loss 0.706\n",
      "epoch 9, batch 900, train_loss 0.700\n",
      "epoch 9, batch 910, train_loss 0.707\n",
      "epoch 9, batch 920, train_loss 0.708\n",
      "epoch 9, batch 930, train_loss 0.702\n",
      "epoch 9, batch 940, train_loss 0.698\n",
      "epoch 9, batch 950, train_loss 0.695\n",
      "epoch 9, batch 960, train_loss 0.704\n",
      "epoch 9, batch 970, train_loss 0.707\n",
      "epoch 9, batch 980, train_loss 0.698\n",
      "epoch 9, batch 990, train_loss 0.697\n",
      "epoch 9, batch 1000, train_loss 0.714\n",
      "epoch 9, batch 1010, train_loss 0.706\n",
      "epoch 9, batch 1020, train_loss 0.701\n",
      "epoch 9, batch 1030, train_loss 0.707\n",
      "epoch 9, batch 1040, train_loss 0.696\n",
      "epoch 9, batch 1050, train_loss 0.706\n",
      "epoch 9, batch 1060, train_loss 0.697\n",
      "epoch 9, batch 1070, train_loss 0.701\n",
      "epoch 9, batch 1080, train_loss 0.708\n",
      "epoch 9, batch 1090, train_loss 0.709\n",
      "epoch 9, batch 1100, train_loss 0.708\n",
      "epoch 9, batch 1110, train_loss 0.707\n",
      "epoch 9, batch 1120, train_loss 0.705\n",
      "epoch 9, batch 1130, train_loss 0.703\n",
      "epoch 9, batch 1140, train_loss 0.704\n",
      "epoch 9, batch 1150, train_loss 0.706\n",
      "epoch 9, batch 1160, train_loss 0.700\n",
      "epoch 9, batch 1170, train_loss 0.703\n",
      "epoch 9, batch 1180, train_loss 0.709\n",
      "epoch 9, batch 1190, train_loss 0.701\n",
      "epoch     9, train_loss 0.702, valid_loss 0.723, train_accuracy  69.70%, valid_accuracy  68.59%\n",
      "epoch 10, batch 0, train_loss 0.700\n",
      "epoch 10, batch 10, train_loss 0.700\n",
      "epoch 10, batch 20, train_loss 0.714\n",
      "epoch 10, batch 30, train_loss 0.700\n",
      "epoch 10, batch 40, train_loss 0.698\n",
      "epoch 10, batch 50, train_loss 0.703\n",
      "epoch 10, batch 60, train_loss 0.708\n",
      "epoch 10, batch 70, train_loss 0.706\n",
      "epoch 10, batch 80, train_loss 0.703\n",
      "epoch 10, batch 90, train_loss 0.703\n",
      "epoch 10, batch 100, train_loss 0.705\n",
      "epoch 10, batch 110, train_loss 0.704\n",
      "epoch 10, batch 120, train_loss 0.702\n",
      "epoch 10, batch 130, train_loss 0.692\n",
      "epoch 10, batch 140, train_loss 0.702\n",
      "epoch 10, batch 150, train_loss 0.712\n",
      "epoch 10, batch 160, train_loss 0.701\n",
      "epoch 10, batch 170, train_loss 0.705\n",
      "epoch 10, batch 180, train_loss 0.712\n",
      "epoch 10, batch 190, train_loss 0.700\n",
      "epoch 10, batch 200, train_loss 0.713\n",
      "epoch 10, batch 210, train_loss 0.701\n",
      "epoch 10, batch 220, train_loss 0.706\n",
      "epoch 10, batch 230, train_loss 0.709\n",
      "epoch 10, batch 240, train_loss 0.700\n",
      "epoch 10, batch 250, train_loss 0.704\n",
      "epoch 10, batch 260, train_loss 0.705\n",
      "epoch 10, batch 270, train_loss 0.700\n",
      "epoch 10, batch 280, train_loss 0.696\n",
      "epoch 10, batch 290, train_loss 0.702\n",
      "epoch 10, batch 300, train_loss 0.701\n",
      "epoch 10, batch 310, train_loss 0.703\n",
      "epoch 10, batch 320, train_loss 0.691\n",
      "epoch 10, batch 330, train_loss 0.703\n",
      "epoch 10, batch 340, train_loss 0.702\n",
      "epoch 10, batch 350, train_loss 0.702\n",
      "epoch 10, batch 360, train_loss 0.704\n",
      "epoch 10, batch 370, train_loss 0.705\n",
      "epoch 10, batch 380, train_loss 0.707\n",
      "epoch 10, batch 390, train_loss 0.697\n",
      "epoch 10, batch 400, train_loss 0.701\n",
      "epoch 10, batch 410, train_loss 0.699\n",
      "epoch 10, batch 420, train_loss 0.688\n",
      "epoch 10, batch 430, train_loss 0.697\n",
      "epoch 10, batch 440, train_loss 0.707\n",
      "epoch 10, batch 450, train_loss 0.716\n",
      "epoch 10, batch 460, train_loss 0.707\n",
      "epoch 10, batch 470, train_loss 0.697\n",
      "epoch 10, batch 480, train_loss 0.699\n",
      "epoch 10, batch 490, train_loss 0.706\n",
      "epoch 10, batch 500, train_loss 0.702\n",
      "epoch 10, batch 510, train_loss 0.694\n",
      "epoch 10, batch 520, train_loss 0.696\n",
      "epoch 10, batch 530, train_loss 0.695\n",
      "epoch 10, batch 540, train_loss 0.697\n",
      "epoch 10, batch 550, train_loss 0.699\n",
      "epoch 10, batch 560, train_loss 0.703\n",
      "epoch 10, batch 570, train_loss 0.708\n",
      "epoch 10, batch 580, train_loss 0.703\n",
      "epoch 10, batch 590, train_loss 0.694\n",
      "epoch 10, batch 600, train_loss 0.696\n",
      "epoch 10, batch 610, train_loss 0.702\n",
      "epoch 10, batch 620, train_loss 0.717\n",
      "epoch 10, batch 630, train_loss 0.698\n",
      "epoch 10, batch 640, train_loss 0.702\n",
      "epoch 10, batch 650, train_loss 0.703\n",
      "epoch 10, batch 660, train_loss 0.702\n",
      "epoch 10, batch 670, train_loss 0.705\n",
      "epoch 10, batch 680, train_loss 0.704\n",
      "epoch 10, batch 690, train_loss 0.713\n",
      "epoch 10, batch 700, train_loss 0.699\n",
      "epoch 10, batch 710, train_loss 0.703\n",
      "epoch 10, batch 720, train_loss 0.708\n",
      "epoch 10, batch 730, train_loss 0.694\n",
      "epoch 10, batch 740, train_loss 0.702\n",
      "epoch 10, batch 750, train_loss 0.706\n",
      "epoch 10, batch 760, train_loss 0.705\n",
      "epoch 10, batch 770, train_loss 0.698\n",
      "epoch 10, batch 780, train_loss 0.709\n",
      "epoch 10, batch 790, train_loss 0.706\n",
      "epoch 10, batch 800, train_loss 0.708\n",
      "epoch 10, batch 810, train_loss 0.700\n",
      "epoch 10, batch 820, train_loss 0.708\n",
      "epoch 10, batch 830, train_loss 0.701\n",
      "epoch 10, batch 840, train_loss 0.699\n",
      "epoch 10, batch 850, train_loss 0.700\n",
      "epoch 10, batch 860, train_loss 0.705\n",
      "epoch 10, batch 870, train_loss 0.701\n",
      "epoch 10, batch 880, train_loss 0.707\n",
      "epoch 10, batch 890, train_loss 0.701\n",
      "epoch 10, batch 900, train_loss 0.698\n",
      "epoch 10, batch 910, train_loss 0.708\n",
      "epoch 10, batch 920, train_loss 0.704\n",
      "epoch 10, batch 930, train_loss 0.695\n",
      "epoch 10, batch 940, train_loss 0.702\n",
      "epoch 10, batch 950, train_loss 0.700\n",
      "epoch 10, batch 960, train_loss 0.692\n",
      "epoch 10, batch 970, train_loss 0.696\n",
      "epoch 10, batch 980, train_loss 0.702\n",
      "epoch 10, batch 990, train_loss 0.708\n",
      "epoch 10, batch 1000, train_loss 0.708\n",
      "epoch 10, batch 1010, train_loss 0.709\n",
      "epoch 10, batch 1020, train_loss 0.707\n",
      "epoch 10, batch 1030, train_loss 0.705\n",
      "epoch 10, batch 1040, train_loss 0.708\n",
      "epoch 10, batch 1050, train_loss 0.692\n",
      "epoch 10, batch 1060, train_loss 0.695\n",
      "epoch 10, batch 1070, train_loss 0.706\n",
      "epoch 10, batch 1080, train_loss 0.703\n",
      "epoch 10, batch 1090, train_loss 0.693\n",
      "epoch 10, batch 1100, train_loss 0.710\n",
      "epoch 10, batch 1110, train_loss 0.707\n",
      "epoch 10, batch 1120, train_loss 0.693\n",
      "epoch 10, batch 1130, train_loss 0.706\n",
      "epoch 10, batch 1140, train_loss 0.708\n",
      "epoch 10, batch 1150, train_loss 0.705\n",
      "epoch 10, batch 1160, train_loss 0.707\n",
      "epoch 10, batch 1170, train_loss 0.698\n",
      "epoch 10, batch 1180, train_loss 0.708\n",
      "epoch 10, batch 1190, train_loss 0.713\n",
      "epoch    10, train_loss 0.701, valid_loss 0.722, train_accuracy  69.76%, valid_accuracy  68.59%\n",
      "epoch 11, batch 0, train_loss 0.695\n",
      "epoch 11, batch 10, train_loss 0.702\n",
      "epoch 11, batch 20, train_loss 0.708\n",
      "epoch 11, batch 30, train_loss 0.708\n",
      "epoch 11, batch 40, train_loss 0.702\n",
      "epoch 11, batch 50, train_loss 0.702\n",
      "epoch 11, batch 60, train_loss 0.700\n",
      "epoch 11, batch 70, train_loss 0.706\n",
      "epoch 11, batch 80, train_loss 0.698\n",
      "epoch 11, batch 90, train_loss 0.713\n",
      "epoch 11, batch 100, train_loss 0.700\n",
      "epoch 11, batch 110, train_loss 0.710\n",
      "epoch 11, batch 120, train_loss 0.691\n",
      "epoch 11, batch 130, train_loss 0.702\n",
      "epoch 11, batch 140, train_loss 0.703\n",
      "epoch 11, batch 150, train_loss 0.702\n",
      "epoch 11, batch 160, train_loss 0.708\n",
      "epoch 11, batch 170, train_loss 0.700\n",
      "epoch 11, batch 180, train_loss 0.704\n",
      "epoch 11, batch 190, train_loss 0.698\n",
      "epoch 11, batch 200, train_loss 0.704\n",
      "epoch 11, batch 210, train_loss 0.700\n",
      "epoch 11, batch 220, train_loss 0.704\n",
      "epoch 11, batch 230, train_loss 0.706\n",
      "epoch 11, batch 240, train_loss 0.699\n",
      "epoch 11, batch 250, train_loss 0.699\n",
      "epoch 11, batch 260, train_loss 0.694\n",
      "epoch 11, batch 270, train_loss 0.700\n",
      "epoch 11, batch 280, train_loss 0.697\n",
      "epoch 11, batch 290, train_loss 0.700\n",
      "epoch 11, batch 300, train_loss 0.687\n",
      "epoch 11, batch 310, train_loss 0.703\n",
      "epoch 11, batch 320, train_loss 0.713\n",
      "epoch 11, batch 330, train_loss 0.699\n",
      "epoch 11, batch 340, train_loss 0.706\n",
      "epoch 11, batch 350, train_loss 0.713\n",
      "epoch 11, batch 360, train_loss 0.705\n",
      "epoch 11, batch 370, train_loss 0.694\n",
      "epoch 11, batch 380, train_loss 0.702\n",
      "epoch 11, batch 390, train_loss 0.708\n",
      "epoch 11, batch 400, train_loss 0.701\n",
      "epoch 11, batch 410, train_loss 0.706\n",
      "epoch 11, batch 420, train_loss 0.696\n",
      "epoch 11, batch 430, train_loss 0.701\n",
      "epoch 11, batch 440, train_loss 0.693\n",
      "epoch 11, batch 450, train_loss 0.699\n",
      "epoch 11, batch 460, train_loss 0.702\n",
      "epoch 11, batch 470, train_loss 0.706\n",
      "epoch 11, batch 480, train_loss 0.704\n",
      "epoch 11, batch 490, train_loss 0.690\n",
      "epoch 11, batch 500, train_loss 0.698\n",
      "epoch 11, batch 510, train_loss 0.695\n",
      "epoch 11, batch 520, train_loss 0.704\n",
      "epoch 11, batch 530, train_loss 0.703\n",
      "epoch 11, batch 540, train_loss 0.703\n",
      "epoch 11, batch 550, train_loss 0.707\n",
      "epoch 11, batch 560, train_loss 0.701\n",
      "epoch 11, batch 570, train_loss 0.699\n",
      "epoch 11, batch 580, train_loss 0.695\n",
      "epoch 11, batch 590, train_loss 0.700\n",
      "epoch 11, batch 600, train_loss 0.696\n",
      "epoch 11, batch 610, train_loss 0.698\n",
      "epoch 11, batch 620, train_loss 0.703\n",
      "epoch 11, batch 630, train_loss 0.704\n",
      "epoch 11, batch 640, train_loss 0.707\n",
      "epoch 11, batch 650, train_loss 0.694\n",
      "epoch 11, batch 660, train_loss 0.703\n",
      "epoch 11, batch 670, train_loss 0.704\n",
      "epoch 11, batch 680, train_loss 0.706\n",
      "epoch 11, batch 690, train_loss 0.711\n",
      "epoch 11, batch 700, train_loss 0.701\n",
      "epoch 11, batch 710, train_loss 0.697\n",
      "epoch 11, batch 720, train_loss 0.711\n",
      "epoch 11, batch 730, train_loss 0.706\n",
      "epoch 11, batch 740, train_loss 0.703\n",
      "epoch 11, batch 750, train_loss 0.693\n",
      "epoch 11, batch 760, train_loss 0.705\n",
      "epoch 11, batch 770, train_loss 0.704\n",
      "epoch 11, batch 780, train_loss 0.699\n",
      "epoch 11, batch 790, train_loss 0.704\n",
      "epoch 11, batch 800, train_loss 0.691\n",
      "epoch 11, batch 810, train_loss 0.712\n",
      "epoch 11, batch 820, train_loss 0.712\n",
      "epoch 11, batch 830, train_loss 0.703\n",
      "epoch 11, batch 840, train_loss 0.698\n",
      "epoch 11, batch 850, train_loss 0.714\n",
      "epoch 11, batch 860, train_loss 0.704\n",
      "epoch 11, batch 870, train_loss 0.705\n",
      "epoch 11, batch 880, train_loss 0.707\n",
      "epoch 11, batch 890, train_loss 0.699\n",
      "epoch 11, batch 900, train_loss 0.704\n",
      "epoch 11, batch 910, train_loss 0.714\n",
      "epoch 11, batch 920, train_loss 0.693\n",
      "epoch 11, batch 930, train_loss 0.706\n",
      "epoch 11, batch 940, train_loss 0.702\n",
      "epoch 11, batch 950, train_loss 0.707\n",
      "epoch 11, batch 960, train_loss 0.699\n",
      "epoch 11, batch 970, train_loss 0.702\n",
      "epoch 11, batch 980, train_loss 0.703\n",
      "epoch 11, batch 990, train_loss 0.705\n",
      "epoch 11, batch 1000, train_loss 0.707\n",
      "epoch 11, batch 1010, train_loss 0.707\n",
      "epoch 11, batch 1020, train_loss 0.708\n",
      "epoch 11, batch 1030, train_loss 0.705\n",
      "epoch 11, batch 1040, train_loss 0.703\n",
      "epoch 11, batch 1050, train_loss 0.685\n",
      "epoch 11, batch 1060, train_loss 0.705\n",
      "epoch 11, batch 1070, train_loss 0.693\n",
      "epoch 11, batch 1080, train_loss 0.716\n",
      "epoch 11, batch 1090, train_loss 0.704\n",
      "epoch 11, batch 1100, train_loss 0.700\n",
      "epoch 11, batch 1110, train_loss 0.710\n",
      "epoch 11, batch 1120, train_loss 0.696\n",
      "epoch 11, batch 1130, train_loss 0.698\n",
      "epoch 11, batch 1140, train_loss 0.705\n",
      "epoch 11, batch 1150, train_loss 0.702\n",
      "epoch 11, batch 1160, train_loss 0.702\n",
      "epoch 11, batch 1170, train_loss 0.702\n",
      "epoch 11, batch 1180, train_loss 0.698\n",
      "epoch 11, batch 1190, train_loss 0.707\n",
      "epoch    11, train_loss 0.701, valid_loss 0.723, train_accuracy  69.76%, valid_accuracy  68.62%\n",
      "epoch 12, batch 0, train_loss 0.705\n",
      "epoch 12, batch 10, train_loss 0.701\n",
      "epoch 12, batch 20, train_loss 0.700\n",
      "epoch 12, batch 30, train_loss 0.709\n",
      "epoch 12, batch 40, train_loss 0.696\n",
      "epoch 12, batch 50, train_loss 0.695\n",
      "epoch 12, batch 60, train_loss 0.706\n",
      "epoch 12, batch 70, train_loss 0.700\n",
      "epoch 12, batch 80, train_loss 0.705\n",
      "epoch 12, batch 90, train_loss 0.693\n",
      "epoch 12, batch 100, train_loss 0.704\n",
      "epoch 12, batch 110, train_loss 0.693\n",
      "epoch 12, batch 120, train_loss 0.698\n",
      "epoch 12, batch 130, train_loss 0.709\n",
      "epoch 12, batch 140, train_loss 0.705\n",
      "epoch 12, batch 150, train_loss 0.704\n",
      "epoch 12, batch 160, train_loss 0.696\n",
      "epoch 12, batch 170, train_loss 0.703\n",
      "epoch 12, batch 180, train_loss 0.696\n",
      "epoch 12, batch 190, train_loss 0.697\n",
      "epoch 12, batch 200, train_loss 0.698\n",
      "epoch 12, batch 210, train_loss 0.705\n",
      "epoch 12, batch 220, train_loss 0.698\n",
      "epoch 12, batch 230, train_loss 0.704\n",
      "epoch 12, batch 240, train_loss 0.698\n",
      "epoch 12, batch 250, train_loss 0.689\n",
      "epoch 12, batch 260, train_loss 0.706\n",
      "epoch 12, batch 270, train_loss 0.696\n",
      "epoch 12, batch 280, train_loss 0.717\n",
      "epoch 12, batch 290, train_loss 0.702\n",
      "epoch 12, batch 300, train_loss 0.701\n",
      "epoch 12, batch 310, train_loss 0.698\n",
      "epoch 12, batch 320, train_loss 0.703\n",
      "epoch 12, batch 330, train_loss 0.707\n",
      "epoch 12, batch 340, train_loss 0.701\n",
      "epoch 12, batch 350, train_loss 0.705\n",
      "epoch 12, batch 360, train_loss 0.701\n",
      "epoch 12, batch 370, train_loss 0.700\n",
      "epoch 12, batch 380, train_loss 0.694\n",
      "epoch 12, batch 390, train_loss 0.695\n",
      "epoch 12, batch 400, train_loss 0.695\n",
      "epoch 12, batch 410, train_loss 0.699\n",
      "epoch 12, batch 420, train_loss 0.697\n",
      "epoch 12, batch 430, train_loss 0.705\n",
      "epoch 12, batch 440, train_loss 0.712\n",
      "epoch 12, batch 450, train_loss 0.706\n",
      "epoch 12, batch 460, train_loss 0.698\n",
      "epoch 12, batch 470, train_loss 0.702\n",
      "epoch 12, batch 480, train_loss 0.702\n",
      "epoch 12, batch 490, train_loss 0.695\n",
      "epoch 12, batch 500, train_loss 0.708\n",
      "epoch 12, batch 510, train_loss 0.698\n",
      "epoch 12, batch 520, train_loss 0.702\n",
      "epoch 12, batch 530, train_loss 0.705\n",
      "epoch 12, batch 540, train_loss 0.711\n",
      "epoch 12, batch 550, train_loss 0.704\n",
      "epoch 12, batch 560, train_loss 0.707\n",
      "epoch 12, batch 570, train_loss 0.691\n",
      "epoch 12, batch 580, train_loss 0.693\n",
      "epoch 12, batch 590, train_loss 0.704\n",
      "epoch 12, batch 600, train_loss 0.709\n",
      "epoch 12, batch 610, train_loss 0.695\n",
      "epoch 12, batch 620, train_loss 0.700\n",
      "epoch 12, batch 630, train_loss 0.711\n",
      "epoch 12, batch 640, train_loss 0.701\n",
      "epoch 12, batch 650, train_loss 0.702\n",
      "epoch 12, batch 660, train_loss 0.699\n",
      "epoch 12, batch 670, train_loss 0.701\n",
      "epoch 12, batch 680, train_loss 0.701\n",
      "epoch 12, batch 690, train_loss 0.693\n",
      "epoch 12, batch 700, train_loss 0.701\n",
      "epoch 12, batch 710, train_loss 0.696\n",
      "epoch 12, batch 720, train_loss 0.700\n",
      "epoch 12, batch 730, train_loss 0.693\n",
      "epoch 12, batch 740, train_loss 0.695\n",
      "epoch 12, batch 750, train_loss 0.705\n",
      "epoch 12, batch 760, train_loss 0.690\n",
      "epoch 12, batch 770, train_loss 0.710\n",
      "epoch 12, batch 780, train_loss 0.700\n",
      "epoch 12, batch 790, train_loss 0.694\n",
      "epoch 12, batch 800, train_loss 0.709\n",
      "epoch 12, batch 810, train_loss 0.700\n",
      "epoch 12, batch 820, train_loss 0.709\n",
      "epoch 12, batch 830, train_loss 0.694\n",
      "epoch 12, batch 840, train_loss 0.696\n",
      "epoch 12, batch 850, train_loss 0.697\n",
      "epoch 12, batch 860, train_loss 0.695\n",
      "epoch 12, batch 870, train_loss 0.697\n",
      "epoch 12, batch 880, train_loss 0.707\n",
      "epoch 12, batch 890, train_loss 0.706\n",
      "epoch 12, batch 900, train_loss 0.698\n",
      "epoch 12, batch 910, train_loss 0.701\n",
      "epoch 12, batch 920, train_loss 0.698\n",
      "epoch 12, batch 930, train_loss 0.701\n",
      "epoch 12, batch 940, train_loss 0.695\n",
      "epoch 12, batch 950, train_loss 0.716\n",
      "epoch 12, batch 960, train_loss 0.702\n",
      "epoch 12, batch 970, train_loss 0.703\n",
      "epoch 12, batch 980, train_loss 0.704\n",
      "epoch 12, batch 990, train_loss 0.701\n",
      "epoch 12, batch 1000, train_loss 0.704\n",
      "epoch 12, batch 1010, train_loss 0.695\n",
      "epoch 12, batch 1020, train_loss 0.695\n",
      "epoch 12, batch 1030, train_loss 0.706\n",
      "epoch 12, batch 1040, train_loss 0.708\n",
      "epoch 12, batch 1050, train_loss 0.696\n",
      "epoch 12, batch 1060, train_loss 0.696\n",
      "epoch 12, batch 1070, train_loss 0.702\n",
      "epoch 12, batch 1080, train_loss 0.698\n",
      "epoch 12, batch 1090, train_loss 0.702\n",
      "epoch 12, batch 1100, train_loss 0.694\n",
      "epoch 12, batch 1110, train_loss 0.695\n",
      "epoch 12, batch 1120, train_loss 0.693\n",
      "epoch 12, batch 1130, train_loss 0.701\n",
      "epoch 12, batch 1140, train_loss 0.709\n",
      "epoch 12, batch 1150, train_loss 0.699\n",
      "epoch 12, batch 1160, train_loss 0.715\n",
      "epoch 12, batch 1170, train_loss 0.711\n",
      "epoch 12, batch 1180, train_loss 0.705\n",
      "epoch 12, batch 1190, train_loss 0.706\n",
      "epoch    12, train_loss 0.700, valid_loss 0.722, train_accuracy  69.80%, valid_accuracy  68.56%\n",
      "epoch 13, batch 0, train_loss 0.708\n",
      "epoch 13, batch 10, train_loss 0.710\n",
      "epoch 13, batch 20, train_loss 0.690\n",
      "epoch 13, batch 30, train_loss 0.696\n",
      "epoch 13, batch 40, train_loss 0.694\n",
      "epoch 13, batch 50, train_loss 0.705\n",
      "epoch 13, batch 60, train_loss 0.703\n",
      "epoch 13, batch 70, train_loss 0.704\n",
      "epoch 13, batch 80, train_loss 0.704\n",
      "epoch 13, batch 90, train_loss 0.693\n",
      "epoch 13, batch 100, train_loss 0.704\n",
      "epoch 13, batch 110, train_loss 0.699\n",
      "epoch 13, batch 120, train_loss 0.689\n",
      "epoch 13, batch 130, train_loss 0.702\n",
      "epoch 13, batch 140, train_loss 0.700\n",
      "epoch 13, batch 150, train_loss 0.704\n",
      "epoch 13, batch 160, train_loss 0.703\n",
      "epoch 13, batch 170, train_loss 0.700\n",
      "epoch 13, batch 180, train_loss 0.701\n",
      "epoch 13, batch 190, train_loss 0.695\n",
      "epoch 13, batch 200, train_loss 0.706\n",
      "epoch 13, batch 210, train_loss 0.709\n",
      "epoch 13, batch 220, train_loss 0.694\n",
      "epoch 13, batch 230, train_loss 0.705\n",
      "epoch 13, batch 240, train_loss 0.693\n",
      "epoch 13, batch 250, train_loss 0.702\n",
      "epoch 13, batch 260, train_loss 0.700\n",
      "epoch 13, batch 270, train_loss 0.699\n",
      "epoch 13, batch 280, train_loss 0.697\n",
      "epoch 13, batch 290, train_loss 0.690\n",
      "epoch 13, batch 300, train_loss 0.695\n",
      "epoch 13, batch 310, train_loss 0.703\n",
      "epoch 13, batch 320, train_loss 0.702\n",
      "epoch 13, batch 330, train_loss 0.712\n",
      "epoch 13, batch 340, train_loss 0.706\n",
      "epoch 13, batch 350, train_loss 0.715\n",
      "epoch 13, batch 360, train_loss 0.699\n",
      "epoch 13, batch 370, train_loss 0.700\n",
      "epoch 13, batch 380, train_loss 0.705\n",
      "epoch 13, batch 390, train_loss 0.691\n",
      "epoch 13, batch 400, train_loss 0.707\n",
      "epoch 13, batch 410, train_loss 0.701\n",
      "epoch 13, batch 420, train_loss 0.700\n",
      "epoch 13, batch 430, train_loss 0.691\n",
      "epoch 13, batch 440, train_loss 0.694\n",
      "epoch 13, batch 450, train_loss 0.707\n",
      "epoch 13, batch 460, train_loss 0.705\n",
      "epoch 13, batch 470, train_loss 0.698\n",
      "epoch 13, batch 480, train_loss 0.704\n",
      "epoch 13, batch 490, train_loss 0.698\n",
      "epoch 13, batch 500, train_loss 0.699\n",
      "epoch 13, batch 510, train_loss 0.698\n",
      "epoch 13, batch 520, train_loss 0.708\n",
      "epoch 13, batch 530, train_loss 0.700\n",
      "epoch 13, batch 540, train_loss 0.698\n",
      "epoch 13, batch 550, train_loss 0.692\n",
      "epoch 13, batch 560, train_loss 0.710\n",
      "epoch 13, batch 570, train_loss 0.690\n",
      "epoch 13, batch 580, train_loss 0.707\n",
      "epoch 13, batch 590, train_loss 0.704\n",
      "epoch 13, batch 600, train_loss 0.700\n",
      "epoch 13, batch 610, train_loss 0.703\n",
      "epoch 13, batch 620, train_loss 0.708\n",
      "epoch 13, batch 630, train_loss 0.694\n",
      "epoch 13, batch 640, train_loss 0.693\n",
      "epoch 13, batch 650, train_loss 0.698\n",
      "epoch 13, batch 660, train_loss 0.701\n",
      "epoch 13, batch 670, train_loss 0.694\n",
      "epoch 13, batch 680, train_loss 0.690\n",
      "epoch 13, batch 690, train_loss 0.704\n",
      "epoch 13, batch 700, train_loss 0.698\n",
      "epoch 13, batch 710, train_loss 0.701\n",
      "epoch 13, batch 720, train_loss 0.706\n",
      "epoch 13, batch 730, train_loss 0.699\n",
      "epoch 13, batch 740, train_loss 0.705\n",
      "epoch 13, batch 750, train_loss 0.691\n",
      "epoch 13, batch 760, train_loss 0.705\n",
      "epoch 13, batch 770, train_loss 0.706\n",
      "epoch 13, batch 780, train_loss 0.699\n",
      "epoch 13, batch 790, train_loss 0.710\n",
      "epoch 13, batch 800, train_loss 0.693\n",
      "epoch 13, batch 810, train_loss 0.700\n",
      "epoch 13, batch 820, train_loss 0.703\n",
      "epoch 13, batch 830, train_loss 0.696\n",
      "epoch 13, batch 840, train_loss 0.703\n",
      "epoch 13, batch 850, train_loss 0.694\n",
      "epoch 13, batch 860, train_loss 0.691\n",
      "epoch 13, batch 870, train_loss 0.694\n",
      "epoch 13, batch 880, train_loss 0.702\n",
      "epoch 13, batch 890, train_loss 0.698\n",
      "epoch 13, batch 900, train_loss 0.708\n",
      "epoch 13, batch 910, train_loss 0.708\n",
      "epoch 13, batch 920, train_loss 0.695\n",
      "epoch 13, batch 930, train_loss 0.701\n",
      "epoch 13, batch 940, train_loss 0.706\n",
      "epoch 13, batch 950, train_loss 0.702\n",
      "epoch 13, batch 960, train_loss 0.701\n",
      "epoch 13, batch 970, train_loss 0.704\n",
      "epoch 13, batch 980, train_loss 0.699\n",
      "epoch 13, batch 990, train_loss 0.697\n",
      "epoch 13, batch 1000, train_loss 0.699\n",
      "epoch 13, batch 1010, train_loss 0.704\n",
      "epoch 13, batch 1020, train_loss 0.703\n",
      "epoch 13, batch 1030, train_loss 0.695\n",
      "epoch 13, batch 1040, train_loss 0.691\n",
      "epoch 13, batch 1050, train_loss 0.695\n",
      "epoch 13, batch 1060, train_loss 0.698\n",
      "epoch 13, batch 1070, train_loss 0.697\n",
      "epoch 13, batch 1080, train_loss 0.696\n",
      "epoch 13, batch 1090, train_loss 0.702\n",
      "epoch 13, batch 1100, train_loss 0.714\n",
      "epoch 13, batch 1110, train_loss 0.692\n",
      "epoch 13, batch 1120, train_loss 0.704\n",
      "epoch 13, batch 1130, train_loss 0.707\n",
      "epoch 13, batch 1140, train_loss 0.694\n",
      "epoch 13, batch 1150, train_loss 0.697\n",
      "epoch 13, batch 1160, train_loss 0.698\n",
      "epoch 13, batch 1170, train_loss 0.705\n",
      "epoch 13, batch 1180, train_loss 0.702\n",
      "epoch 13, batch 1190, train_loss 0.696\n",
      "epoch    13, train_loss 0.699, valid_loss 0.722, train_accuracy  69.87%, valid_accuracy  68.62%\n",
      "epoch 14, batch 0, train_loss 0.697\n",
      "epoch 14, batch 10, train_loss 0.697\n",
      "epoch 14, batch 20, train_loss 0.694\n",
      "epoch 14, batch 30, train_loss 0.700\n",
      "epoch 14, batch 40, train_loss 0.698\n",
      "epoch 14, batch 50, train_loss 0.695\n",
      "epoch 14, batch 60, train_loss 0.691\n",
      "epoch 14, batch 70, train_loss 0.702\n",
      "epoch 14, batch 80, train_loss 0.693\n",
      "epoch 14, batch 90, train_loss 0.699\n",
      "epoch 14, batch 100, train_loss 0.702\n",
      "epoch 14, batch 110, train_loss 0.696\n",
      "epoch 14, batch 120, train_loss 0.698\n",
      "epoch 14, batch 130, train_loss 0.696\n",
      "epoch 14, batch 140, train_loss 0.699\n",
      "epoch 14, batch 150, train_loss 0.698\n",
      "epoch 14, batch 160, train_loss 0.694\n",
      "epoch 14, batch 170, train_loss 0.697\n",
      "epoch 14, batch 180, train_loss 0.693\n",
      "epoch 14, batch 190, train_loss 0.692\n",
      "epoch 14, batch 200, train_loss 0.698\n",
      "epoch 14, batch 210, train_loss 0.711\n",
      "epoch 14, batch 220, train_loss 0.696\n",
      "epoch 14, batch 230, train_loss 0.705\n",
      "epoch 14, batch 240, train_loss 0.707\n",
      "epoch 14, batch 250, train_loss 0.711\n",
      "epoch 14, batch 260, train_loss 0.702\n",
      "epoch 14, batch 270, train_loss 0.701\n",
      "epoch 14, batch 280, train_loss 0.695\n",
      "epoch 14, batch 290, train_loss 0.711\n",
      "epoch 14, batch 300, train_loss 0.692\n",
      "epoch 14, batch 310, train_loss 0.694\n",
      "epoch 14, batch 320, train_loss 0.699\n",
      "epoch 14, batch 330, train_loss 0.699\n",
      "epoch 14, batch 340, train_loss 0.709\n",
      "epoch 14, batch 350, train_loss 0.699\n",
      "epoch 14, batch 360, train_loss 0.695\n",
      "epoch 14, batch 370, train_loss 0.701\n",
      "epoch 14, batch 380, train_loss 0.702\n",
      "epoch 14, batch 390, train_loss 0.698\n",
      "epoch 14, batch 400, train_loss 0.701\n",
      "epoch 14, batch 410, train_loss 0.690\n",
      "epoch 14, batch 420, train_loss 0.710\n",
      "epoch 14, batch 430, train_loss 0.699\n",
      "epoch 14, batch 440, train_loss 0.699\n",
      "epoch 14, batch 450, train_loss 0.697\n",
      "epoch 14, batch 460, train_loss 0.693\n",
      "epoch 14, batch 470, train_loss 0.703\n",
      "epoch 14, batch 480, train_loss 0.691\n",
      "epoch 14, batch 490, train_loss 0.700\n",
      "epoch 14, batch 500, train_loss 0.696\n",
      "epoch 14, batch 510, train_loss 0.698\n",
      "epoch 14, batch 520, train_loss 0.701\n",
      "epoch 14, batch 530, train_loss 0.697\n",
      "epoch 14, batch 540, train_loss 0.697\n",
      "epoch 14, batch 550, train_loss 0.711\n",
      "epoch 14, batch 560, train_loss 0.698\n",
      "epoch 14, batch 570, train_loss 0.697\n",
      "epoch 14, batch 580, train_loss 0.698\n",
      "epoch 14, batch 590, train_loss 0.703\n",
      "epoch 14, batch 600, train_loss 0.699\n",
      "epoch 14, batch 610, train_loss 0.699\n",
      "epoch 14, batch 620, train_loss 0.697\n",
      "epoch 14, batch 630, train_loss 0.702\n",
      "epoch 14, batch 640, train_loss 0.699\n",
      "epoch 14, batch 650, train_loss 0.695\n",
      "epoch 14, batch 660, train_loss 0.694\n",
      "epoch 14, batch 670, train_loss 0.701\n",
      "epoch 14, batch 680, train_loss 0.694\n",
      "epoch 14, batch 690, train_loss 0.697\n",
      "epoch 14, batch 700, train_loss 0.705\n",
      "epoch 14, batch 710, train_loss 0.701\n",
      "epoch 14, batch 720, train_loss 0.695\n",
      "epoch 14, batch 730, train_loss 0.696\n",
      "epoch 14, batch 740, train_loss 0.698\n",
      "epoch 14, batch 750, train_loss 0.707\n",
      "epoch 14, batch 760, train_loss 0.698\n",
      "epoch 14, batch 770, train_loss 0.691\n",
      "epoch 14, batch 780, train_loss 0.711\n",
      "epoch 14, batch 790, train_loss 0.701\n",
      "epoch 14, batch 800, train_loss 0.709\n",
      "epoch 14, batch 810, train_loss 0.699\n",
      "epoch 14, batch 820, train_loss 0.701\n",
      "epoch 14, batch 830, train_loss 0.706\n",
      "epoch 14, batch 840, train_loss 0.706\n",
      "epoch 14, batch 850, train_loss 0.706\n",
      "epoch 14, batch 860, train_loss 0.701\n",
      "epoch 14, batch 870, train_loss 0.700\n",
      "epoch 14, batch 880, train_loss 0.694\n",
      "epoch 14, batch 890, train_loss 0.701\n",
      "epoch 14, batch 900, train_loss 0.694\n",
      "epoch 14, batch 910, train_loss 0.705\n",
      "epoch 14, batch 920, train_loss 0.700\n",
      "epoch 14, batch 930, train_loss 0.694\n",
      "epoch 14, batch 940, train_loss 0.698\n",
      "epoch 14, batch 950, train_loss 0.698\n",
      "epoch 14, batch 960, train_loss 0.697\n",
      "epoch 14, batch 970, train_loss 0.702\n",
      "epoch 14, batch 980, train_loss 0.703\n",
      "epoch 14, batch 990, train_loss 0.702\n",
      "epoch 14, batch 1000, train_loss 0.700\n",
      "epoch 14, batch 1010, train_loss 0.708\n",
      "epoch 14, batch 1020, train_loss 0.684\n",
      "epoch 14, batch 1030, train_loss 0.707\n",
      "epoch 14, batch 1040, train_loss 0.706\n",
      "epoch 14, batch 1050, train_loss 0.697\n",
      "epoch 14, batch 1060, train_loss 0.700\n",
      "epoch 14, batch 1070, train_loss 0.694\n",
      "epoch 14, batch 1080, train_loss 0.705\n",
      "epoch 14, batch 1090, train_loss 0.705\n",
      "epoch 14, batch 1100, train_loss 0.702\n",
      "epoch 14, batch 1110, train_loss 0.700\n",
      "epoch 14, batch 1120, train_loss 0.698\n",
      "epoch 14, batch 1130, train_loss 0.699\n",
      "epoch 14, batch 1140, train_loss 0.706\n",
      "epoch 14, batch 1150, train_loss 0.694\n",
      "epoch 14, batch 1160, train_loss 0.697\n",
      "epoch 14, batch 1170, train_loss 0.706\n",
      "epoch 14, batch 1180, train_loss 0.705\n",
      "epoch 14, batch 1190, train_loss 0.693\n",
      "epoch    14, train_loss 0.699, valid_loss 0.722, train_accuracy  69.89%, valid_accuracy  68.62%\n",
      "epoch 15, batch 0, train_loss 0.692\n",
      "epoch 15, batch 10, train_loss 0.698\n",
      "epoch 15, batch 20, train_loss 0.691\n",
      "epoch 15, batch 30, train_loss 0.695\n",
      "epoch 15, batch 40, train_loss 0.694\n",
      "epoch 15, batch 50, train_loss 0.697\n",
      "epoch 15, batch 60, train_loss 0.698\n",
      "epoch 15, batch 70, train_loss 0.693\n",
      "epoch 15, batch 80, train_loss 0.697\n",
      "epoch 15, batch 90, train_loss 0.703\n",
      "epoch 15, batch 100, train_loss 0.700\n",
      "epoch 15, batch 110, train_loss 0.705\n",
      "epoch 15, batch 120, train_loss 0.709\n",
      "epoch 15, batch 130, train_loss 0.703\n",
      "epoch 15, batch 140, train_loss 0.699\n",
      "epoch 15, batch 150, train_loss 0.707\n",
      "epoch 15, batch 160, train_loss 0.701\n",
      "epoch 15, batch 170, train_loss 0.698\n",
      "epoch 15, batch 180, train_loss 0.702\n",
      "epoch 15, batch 190, train_loss 0.701\n",
      "epoch 15, batch 200, train_loss 0.694\n",
      "epoch 15, batch 210, train_loss 0.695\n",
      "epoch 15, batch 220, train_loss 0.698\n",
      "epoch 15, batch 230, train_loss 0.698\n",
      "epoch 15, batch 240, train_loss 0.697\n",
      "epoch 15, batch 250, train_loss 0.703\n",
      "epoch 15, batch 260, train_loss 0.700\n",
      "epoch 15, batch 270, train_loss 0.698\n",
      "epoch 15, batch 280, train_loss 0.698\n",
      "epoch 15, batch 290, train_loss 0.694\n",
      "epoch 15, batch 300, train_loss 0.708\n",
      "epoch 15, batch 310, train_loss 0.702\n",
      "epoch 15, batch 320, train_loss 0.694\n",
      "epoch 15, batch 330, train_loss 0.705\n",
      "epoch 15, batch 340, train_loss 0.689\n",
      "epoch 15, batch 350, train_loss 0.698\n",
      "epoch 15, batch 360, train_loss 0.691\n",
      "epoch 15, batch 370, train_loss 0.702\n",
      "epoch 15, batch 380, train_loss 0.706\n",
      "epoch 15, batch 390, train_loss 0.694\n",
      "epoch 15, batch 400, train_loss 0.692\n",
      "epoch 15, batch 410, train_loss 0.705\n",
      "epoch 15, batch 420, train_loss 0.692\n",
      "epoch 15, batch 430, train_loss 0.701\n",
      "epoch 15, batch 440, train_loss 0.696\n",
      "epoch 15, batch 450, train_loss 0.706\n",
      "epoch 15, batch 460, train_loss 0.703\n",
      "epoch 15, batch 470, train_loss 0.713\n",
      "epoch 15, batch 480, train_loss 0.693\n",
      "epoch 15, batch 490, train_loss 0.697\n",
      "epoch 15, batch 500, train_loss 0.699\n",
      "epoch 15, batch 510, train_loss 0.697\n",
      "epoch 15, batch 520, train_loss 0.708\n",
      "epoch 15, batch 530, train_loss 0.703\n",
      "epoch 15, batch 540, train_loss 0.710\n",
      "epoch 15, batch 550, train_loss 0.691\n",
      "epoch 15, batch 560, train_loss 0.694\n",
      "epoch 15, batch 570, train_loss 0.713\n",
      "epoch 15, batch 580, train_loss 0.709\n",
      "epoch 15, batch 590, train_loss 0.699\n",
      "epoch 15, batch 600, train_loss 0.698\n",
      "epoch 15, batch 610, train_loss 0.704\n",
      "epoch 15, batch 620, train_loss 0.695\n",
      "epoch 15, batch 630, train_loss 0.704\n",
      "epoch 15, batch 640, train_loss 0.703\n",
      "epoch 15, batch 650, train_loss 0.704\n",
      "epoch 15, batch 660, train_loss 0.691\n",
      "epoch 15, batch 670, train_loss 0.689\n",
      "epoch 15, batch 680, train_loss 0.695\n",
      "epoch 15, batch 690, train_loss 0.694\n",
      "epoch 15, batch 700, train_loss 0.697\n",
      "epoch 15, batch 710, train_loss 0.700\n",
      "epoch 15, batch 720, train_loss 0.708\n",
      "epoch 15, batch 730, train_loss 0.698\n",
      "epoch 15, batch 740, train_loss 0.700\n",
      "epoch 15, batch 750, train_loss 0.705\n",
      "epoch 15, batch 760, train_loss 0.702\n",
      "epoch 15, batch 770, train_loss 0.702\n",
      "epoch 15, batch 780, train_loss 0.701\n",
      "epoch 15, batch 790, train_loss 0.702\n",
      "epoch 15, batch 800, train_loss 0.700\n",
      "epoch 15, batch 810, train_loss 0.694\n",
      "epoch 15, batch 820, train_loss 0.706\n",
      "epoch 15, batch 830, train_loss 0.704\n",
      "epoch 15, batch 840, train_loss 0.702\n",
      "epoch 15, batch 850, train_loss 0.701\n",
      "epoch 15, batch 860, train_loss 0.692\n",
      "epoch 15, batch 870, train_loss 0.691\n",
      "epoch 15, batch 880, train_loss 0.696\n",
      "epoch 15, batch 890, train_loss 0.704\n",
      "epoch 15, batch 900, train_loss 0.698\n",
      "epoch 15, batch 910, train_loss 0.695\n",
      "epoch 15, batch 920, train_loss 0.700\n",
      "epoch 15, batch 930, train_loss 0.686\n",
      "epoch 15, batch 940, train_loss 0.695\n",
      "epoch 15, batch 950, train_loss 0.699\n",
      "epoch 15, batch 960, train_loss 0.696\n",
      "epoch 15, batch 970, train_loss 0.699\n",
      "epoch 15, batch 980, train_loss 0.694\n",
      "epoch 15, batch 990, train_loss 0.694\n",
      "epoch 15, batch 1000, train_loss 0.695\n",
      "epoch 15, batch 1010, train_loss 0.690\n",
      "epoch 15, batch 1020, train_loss 0.702\n",
      "epoch 15, batch 1030, train_loss 0.704\n",
      "epoch 15, batch 1040, train_loss 0.693\n",
      "epoch 15, batch 1050, train_loss 0.693\n",
      "epoch 15, batch 1060, train_loss 0.700\n",
      "epoch 15, batch 1070, train_loss 0.702\n",
      "epoch 15, batch 1080, train_loss 0.705\n",
      "epoch 15, batch 1090, train_loss 0.693\n",
      "epoch 15, batch 1100, train_loss 0.712\n",
      "epoch 15, batch 1110, train_loss 0.697\n",
      "epoch 15, batch 1120, train_loss 0.699\n",
      "epoch 15, batch 1130, train_loss 0.707\n",
      "epoch 15, batch 1140, train_loss 0.699\n",
      "epoch 15, batch 1150, train_loss 0.700\n",
      "epoch 15, batch 1160, train_loss 0.691\n",
      "epoch 15, batch 1170, train_loss 0.701\n",
      "epoch 15, batch 1180, train_loss 0.691\n",
      "epoch 15, batch 1190, train_loss 0.705\n",
      "epoch    15, train_loss 0.699, valid_loss 0.723, train_accuracy  69.87%, valid_accuracy  68.56%\n",
      "epoch 16, batch 0, train_loss 0.697\n",
      "epoch 16, batch 10, train_loss 0.701\n",
      "epoch 16, batch 20, train_loss 0.698\n",
      "epoch 16, batch 30, train_loss 0.703\n",
      "epoch 16, batch 40, train_loss 0.705\n",
      "epoch 16, batch 50, train_loss 0.703\n",
      "epoch 16, batch 60, train_loss 0.701\n",
      "epoch 16, batch 70, train_loss 0.702\n",
      "epoch 16, batch 80, train_loss 0.690\n",
      "epoch 16, batch 90, train_loss 0.700\n",
      "epoch 16, batch 100, train_loss 0.700\n",
      "epoch 16, batch 110, train_loss 0.695\n",
      "epoch 16, batch 120, train_loss 0.703\n",
      "epoch 16, batch 130, train_loss 0.689\n",
      "epoch 16, batch 140, train_loss 0.697\n",
      "epoch 16, batch 150, train_loss 0.702\n",
      "epoch 16, batch 160, train_loss 0.701\n",
      "epoch 16, batch 170, train_loss 0.699\n",
      "epoch 16, batch 180, train_loss 0.695\n",
      "epoch 16, batch 190, train_loss 0.702\n",
      "epoch 16, batch 200, train_loss 0.700\n",
      "epoch 16, batch 210, train_loss 0.692\n",
      "epoch 16, batch 220, train_loss 0.698\n",
      "epoch 16, batch 230, train_loss 0.696\n",
      "epoch 16, batch 240, train_loss 0.704\n",
      "epoch 16, batch 250, train_loss 0.700\n",
      "epoch 16, batch 260, train_loss 0.716\n",
      "epoch 16, batch 270, train_loss 0.689\n",
      "epoch 16, batch 280, train_loss 0.691\n",
      "epoch 16, batch 290, train_loss 0.696\n",
      "epoch 16, batch 300, train_loss 0.696\n",
      "epoch 16, batch 310, train_loss 0.694\n",
      "epoch 16, batch 320, train_loss 0.703\n",
      "epoch 16, batch 330, train_loss 0.696\n",
      "epoch 16, batch 340, train_loss 0.697\n",
      "epoch 16, batch 350, train_loss 0.705\n",
      "epoch 16, batch 360, train_loss 0.696\n",
      "epoch 16, batch 370, train_loss 0.693\n",
      "epoch 16, batch 380, train_loss 0.693\n",
      "epoch 16, batch 390, train_loss 0.690\n",
      "epoch 16, batch 400, train_loss 0.694\n",
      "epoch 16, batch 410, train_loss 0.703\n",
      "epoch 16, batch 420, train_loss 0.692\n",
      "epoch 16, batch 430, train_loss 0.694\n",
      "epoch 16, batch 440, train_loss 0.693\n",
      "epoch 16, batch 450, train_loss 0.689\n",
      "epoch 16, batch 460, train_loss 0.702\n",
      "epoch 16, batch 470, train_loss 0.701\n",
      "epoch 16, batch 480, train_loss 0.706\n",
      "epoch 16, batch 490, train_loss 0.703\n",
      "epoch 16, batch 500, train_loss 0.694\n",
      "epoch 16, batch 510, train_loss 0.702\n",
      "epoch 16, batch 520, train_loss 0.706\n",
      "epoch 16, batch 530, train_loss 0.696\n",
      "epoch 16, batch 540, train_loss 0.706\n",
      "epoch 16, batch 550, train_loss 0.700\n",
      "epoch 16, batch 560, train_loss 0.697\n",
      "epoch 16, batch 570, train_loss 0.698\n",
      "epoch 16, batch 580, train_loss 0.697\n",
      "epoch 16, batch 590, train_loss 0.702\n",
      "epoch 16, batch 600, train_loss 0.693\n",
      "epoch 16, batch 610, train_loss 0.701\n",
      "epoch 16, batch 620, train_loss 0.703\n",
      "epoch 16, batch 630, train_loss 0.693\n",
      "epoch 16, batch 640, train_loss 0.693\n",
      "epoch 16, batch 650, train_loss 0.687\n",
      "epoch 16, batch 660, train_loss 0.698\n",
      "epoch 16, batch 670, train_loss 0.691\n",
      "epoch 16, batch 680, train_loss 0.697\n",
      "epoch 16, batch 690, train_loss 0.693\n",
      "epoch 16, batch 700, train_loss 0.697\n",
      "epoch 16, batch 710, train_loss 0.695\n",
      "epoch 16, batch 720, train_loss 0.698\n",
      "epoch 16, batch 730, train_loss 0.691\n",
      "epoch 16, batch 740, train_loss 0.701\n",
      "epoch 16, batch 750, train_loss 0.704\n",
      "epoch 16, batch 760, train_loss 0.700\n",
      "epoch 16, batch 770, train_loss 0.701\n",
      "epoch 16, batch 780, train_loss 0.697\n",
      "epoch 16, batch 790, train_loss 0.700\n",
      "epoch 16, batch 800, train_loss 0.710\n",
      "epoch 16, batch 810, train_loss 0.696\n",
      "epoch 16, batch 820, train_loss 0.697\n",
      "epoch 16, batch 830, train_loss 0.707\n",
      "epoch 16, batch 840, train_loss 0.704\n",
      "epoch 16, batch 850, train_loss 0.697\n",
      "epoch 16, batch 860, train_loss 0.690\n",
      "epoch 16, batch 870, train_loss 0.700\n",
      "epoch 16, batch 880, train_loss 0.701\n",
      "epoch 16, batch 890, train_loss 0.696\n",
      "epoch 16, batch 900, train_loss 0.701\n",
      "epoch 16, batch 910, train_loss 0.701\n",
      "epoch 16, batch 920, train_loss 0.701\n",
      "epoch 16, batch 930, train_loss 0.692\n",
      "epoch 16, batch 940, train_loss 0.700\n",
      "epoch 16, batch 950, train_loss 0.699\n",
      "epoch 16, batch 960, train_loss 0.697\n",
      "epoch 16, batch 970, train_loss 0.690\n",
      "epoch 16, batch 980, train_loss 0.690\n",
      "epoch 16, batch 990, train_loss 0.699\n",
      "epoch 16, batch 1000, train_loss 0.705\n",
      "epoch 16, batch 1010, train_loss 0.702\n",
      "epoch 16, batch 1020, train_loss 0.696\n",
      "epoch 16, batch 1030, train_loss 0.692\n",
      "epoch 16, batch 1040, train_loss 0.702\n",
      "epoch 16, batch 1050, train_loss 0.701\n",
      "epoch 16, batch 1060, train_loss 0.695\n",
      "epoch 16, batch 1070, train_loss 0.698\n",
      "epoch 16, batch 1080, train_loss 0.698\n",
      "epoch 16, batch 1090, train_loss 0.700\n",
      "epoch 16, batch 1100, train_loss 0.701\n",
      "epoch 16, batch 1110, train_loss 0.697\n",
      "epoch 16, batch 1120, train_loss 0.700\n",
      "epoch 16, batch 1130, train_loss 0.698\n",
      "epoch 16, batch 1140, train_loss 0.696\n",
      "epoch 16, batch 1150, train_loss 0.699\n",
      "epoch 16, batch 1160, train_loss 0.700\n",
      "epoch 16, batch 1170, train_loss 0.691\n",
      "epoch 16, batch 1180, train_loss 0.695\n",
      "epoch 16, batch 1190, train_loss 0.701\n",
      "epoch    16, train_loss 0.699, valid_loss 0.723, train_accuracy  69.88%, valid_accuracy  68.58%\n",
      "epoch 17, batch 0, train_loss 0.695\n",
      "epoch 17, batch 10, train_loss 0.703\n",
      "epoch 17, batch 20, train_loss 0.709\n",
      "epoch 17, batch 30, train_loss 0.701\n",
      "epoch 17, batch 40, train_loss 0.706\n",
      "epoch 17, batch 50, train_loss 0.694\n",
      "epoch 17, batch 60, train_loss 0.695\n",
      "epoch 17, batch 70, train_loss 0.703\n",
      "epoch 17, batch 80, train_loss 0.704\n",
      "epoch 17, batch 90, train_loss 0.691\n",
      "epoch 17, batch 100, train_loss 0.699\n",
      "epoch 17, batch 110, train_loss 0.699\n",
      "epoch 17, batch 120, train_loss 0.693\n",
      "epoch 17, batch 130, train_loss 0.694\n",
      "epoch 17, batch 140, train_loss 0.704\n",
      "epoch 17, batch 150, train_loss 0.690\n",
      "epoch 17, batch 160, train_loss 0.692\n",
      "epoch 17, batch 170, train_loss 0.700\n",
      "epoch 17, batch 180, train_loss 0.698\n",
      "epoch 17, batch 190, train_loss 0.701\n",
      "epoch 17, batch 200, train_loss 0.703\n",
      "epoch 17, batch 210, train_loss 0.705\n",
      "epoch 17, batch 220, train_loss 0.693\n",
      "epoch 17, batch 230, train_loss 0.700\n",
      "epoch 17, batch 240, train_loss 0.697\n",
      "epoch 17, batch 250, train_loss 0.698\n",
      "epoch 17, batch 260, train_loss 0.700\n",
      "epoch 17, batch 270, train_loss 0.707\n",
      "epoch 17, batch 280, train_loss 0.701\n",
      "epoch 17, batch 290, train_loss 0.699\n",
      "epoch 17, batch 300, train_loss 0.695\n",
      "epoch 17, batch 310, train_loss 0.705\n",
      "epoch 17, batch 320, train_loss 0.692\n",
      "epoch 17, batch 330, train_loss 0.698\n",
      "epoch 17, batch 340, train_loss 0.695\n",
      "epoch 17, batch 350, train_loss 0.696\n",
      "epoch 17, batch 360, train_loss 0.714\n",
      "epoch 17, batch 370, train_loss 0.701\n",
      "epoch 17, batch 380, train_loss 0.702\n",
      "epoch 17, batch 390, train_loss 0.701\n",
      "epoch 17, batch 400, train_loss 0.691\n",
      "epoch 17, batch 410, train_loss 0.686\n",
      "epoch 17, batch 420, train_loss 0.707\n",
      "epoch 17, batch 430, train_loss 0.704\n",
      "epoch 17, batch 440, train_loss 0.699\n",
      "epoch 17, batch 450, train_loss 0.697\n",
      "epoch 17, batch 460, train_loss 0.697\n",
      "epoch 17, batch 470, train_loss 0.704\n",
      "epoch 17, batch 480, train_loss 0.699\n",
      "epoch 17, batch 490, train_loss 0.700\n",
      "epoch 17, batch 500, train_loss 0.695\n",
      "epoch 17, batch 510, train_loss 0.694\n",
      "epoch 17, batch 520, train_loss 0.699\n",
      "epoch 17, batch 530, train_loss 0.692\n",
      "epoch 17, batch 540, train_loss 0.703\n",
      "epoch 17, batch 550, train_loss 0.706\n",
      "epoch 17, batch 560, train_loss 0.709\n",
      "epoch 17, batch 570, train_loss 0.701\n",
      "epoch 17, batch 580, train_loss 0.681\n",
      "epoch 17, batch 590, train_loss 0.694\n",
      "epoch 17, batch 600, train_loss 0.700\n",
      "epoch 17, batch 610, train_loss 0.698\n",
      "epoch 17, batch 620, train_loss 0.701\n",
      "epoch 17, batch 630, train_loss 0.700\n",
      "epoch 17, batch 640, train_loss 0.705\n",
      "epoch 17, batch 650, train_loss 0.702\n",
      "epoch 17, batch 660, train_loss 0.701\n",
      "epoch 17, batch 670, train_loss 0.699\n",
      "epoch 17, batch 680, train_loss 0.697\n",
      "epoch 17, batch 690, train_loss 0.700\n",
      "epoch 17, batch 700, train_loss 0.700\n",
      "epoch 17, batch 710, train_loss 0.704\n",
      "epoch 17, batch 720, train_loss 0.710\n",
      "epoch 17, batch 730, train_loss 0.702\n",
      "epoch 17, batch 740, train_loss 0.695\n",
      "epoch 17, batch 750, train_loss 0.690\n",
      "epoch 17, batch 760, train_loss 0.704\n",
      "epoch 17, batch 770, train_loss 0.694\n",
      "epoch 17, batch 780, train_loss 0.695\n",
      "epoch 17, batch 790, train_loss 0.695\n",
      "epoch 17, batch 800, train_loss 0.694\n",
      "epoch 17, batch 810, train_loss 0.702\n",
      "epoch 17, batch 820, train_loss 0.693\n",
      "epoch 17, batch 830, train_loss 0.697\n",
      "epoch 17, batch 840, train_loss 0.700\n",
      "epoch 17, batch 850, train_loss 0.694\n",
      "epoch 17, batch 860, train_loss 0.700\n",
      "epoch 17, batch 870, train_loss 0.695\n",
      "epoch 17, batch 880, train_loss 0.694\n",
      "epoch 17, batch 890, train_loss 0.702\n",
      "epoch 17, batch 900, train_loss 0.700\n",
      "epoch 17, batch 910, train_loss 0.698\n",
      "epoch 17, batch 920, train_loss 0.702\n",
      "epoch 17, batch 930, train_loss 0.696\n",
      "epoch 17, batch 940, train_loss 0.706\n",
      "epoch 17, batch 950, train_loss 0.695\n",
      "epoch 17, batch 960, train_loss 0.699\n",
      "epoch 17, batch 970, train_loss 0.708\n",
      "epoch 17, batch 980, train_loss 0.700\n",
      "epoch 17, batch 990, train_loss 0.698\n",
      "epoch 17, batch 1000, train_loss 0.698\n",
      "epoch 17, batch 1010, train_loss 0.699\n",
      "epoch 17, batch 1020, train_loss 0.692\n",
      "epoch 17, batch 1030, train_loss 0.701\n",
      "epoch 17, batch 1040, train_loss 0.690\n",
      "epoch 17, batch 1050, train_loss 0.698\n",
      "epoch 17, batch 1060, train_loss 0.689\n",
      "epoch 17, batch 1070, train_loss 0.699\n",
      "epoch 17, batch 1080, train_loss 0.695\n",
      "epoch 17, batch 1090, train_loss 0.690\n",
      "epoch 17, batch 1100, train_loss 0.705\n",
      "epoch 17, batch 1110, train_loss 0.688\n",
      "epoch 17, batch 1120, train_loss 0.700\n",
      "epoch 17, batch 1130, train_loss 0.692\n",
      "epoch 17, batch 1140, train_loss 0.696\n",
      "epoch 17, batch 1150, train_loss 0.696\n",
      "epoch 17, batch 1160, train_loss 0.700\n",
      "epoch 17, batch 1170, train_loss 0.699\n",
      "epoch 17, batch 1180, train_loss 0.693\n",
      "epoch 17, batch 1190, train_loss 0.700\n",
      "epoch    17, train_loss 0.698, valid_loss 0.723, train_accuracy  69.92%, valid_accuracy  68.59%\n",
      "epoch 18, batch 0, train_loss 0.701\n",
      "epoch 18, batch 10, train_loss 0.699\n",
      "epoch 18, batch 20, train_loss 0.698\n",
      "epoch 18, batch 30, train_loss 0.700\n",
      "epoch 18, batch 40, train_loss 0.701\n",
      "epoch 18, batch 50, train_loss 0.696\n",
      "epoch 18, batch 60, train_loss 0.700\n",
      "epoch 18, batch 70, train_loss 0.686\n",
      "epoch 18, batch 80, train_loss 0.695\n",
      "epoch 18, batch 90, train_loss 0.702\n",
      "epoch 18, batch 100, train_loss 0.706\n",
      "epoch 18, batch 110, train_loss 0.698\n",
      "epoch 18, batch 120, train_loss 0.691\n",
      "epoch 18, batch 130, train_loss 0.690\n",
      "epoch 18, batch 140, train_loss 0.699\n",
      "epoch 18, batch 150, train_loss 0.698\n",
      "epoch 18, batch 160, train_loss 0.701\n",
      "epoch 18, batch 170, train_loss 0.696\n",
      "epoch 18, batch 180, train_loss 0.692\n",
      "epoch 18, batch 190, train_loss 0.707\n",
      "epoch 18, batch 200, train_loss 0.701\n",
      "epoch 18, batch 210, train_loss 0.690\n",
      "epoch 18, batch 220, train_loss 0.695\n",
      "epoch 18, batch 230, train_loss 0.694\n",
      "epoch 18, batch 240, train_loss 0.704\n",
      "epoch 18, batch 250, train_loss 0.701\n",
      "epoch 18, batch 260, train_loss 0.704\n",
      "epoch 18, batch 270, train_loss 0.690\n",
      "epoch 18, batch 280, train_loss 0.704\n",
      "epoch 18, batch 290, train_loss 0.692\n",
      "epoch 18, batch 300, train_loss 0.695\n",
      "epoch 18, batch 310, train_loss 0.694\n",
      "epoch 18, batch 320, train_loss 0.693\n",
      "epoch 18, batch 330, train_loss 0.709\n",
      "epoch 18, batch 340, train_loss 0.704\n",
      "epoch 18, batch 350, train_loss 0.700\n",
      "epoch 18, batch 360, train_loss 0.703\n",
      "epoch 18, batch 370, train_loss 0.695\n",
      "epoch 18, batch 380, train_loss 0.703\n",
      "epoch 18, batch 390, train_loss 0.695\n",
      "epoch 18, batch 400, train_loss 0.690\n",
      "epoch 18, batch 410, train_loss 0.701\n",
      "epoch 18, batch 420, train_loss 0.695\n",
      "epoch 18, batch 430, train_loss 0.699\n",
      "epoch 18, batch 440, train_loss 0.696\n",
      "epoch 18, batch 450, train_loss 0.692\n",
      "epoch 18, batch 460, train_loss 0.698\n",
      "epoch 18, batch 470, train_loss 0.705\n",
      "epoch 18, batch 480, train_loss 0.708\n",
      "epoch 18, batch 490, train_loss 0.699\n",
      "epoch 18, batch 500, train_loss 0.693\n",
      "epoch 18, batch 510, train_loss 0.704\n",
      "epoch 18, batch 520, train_loss 0.691\n",
      "epoch 18, batch 530, train_loss 0.700\n",
      "epoch 18, batch 540, train_loss 0.706\n",
      "epoch 18, batch 550, train_loss 0.693\n",
      "epoch 18, batch 560, train_loss 0.691\n",
      "epoch 18, batch 570, train_loss 0.711\n",
      "epoch 18, batch 580, train_loss 0.699\n",
      "epoch 18, batch 590, train_loss 0.695\n",
      "epoch 18, batch 600, train_loss 0.709\n",
      "epoch 18, batch 610, train_loss 0.689\n",
      "epoch 18, batch 620, train_loss 0.698\n",
      "epoch 18, batch 630, train_loss 0.701\n",
      "epoch 18, batch 640, train_loss 0.701\n",
      "epoch 18, batch 650, train_loss 0.708\n",
      "epoch 18, batch 660, train_loss 0.696\n",
      "epoch 18, batch 670, train_loss 0.699\n",
      "epoch 18, batch 680, train_loss 0.687\n",
      "epoch 18, batch 690, train_loss 0.694\n",
      "epoch 18, batch 700, train_loss 0.707\n",
      "epoch 18, batch 710, train_loss 0.697\n",
      "epoch 18, batch 720, train_loss 0.704\n",
      "epoch 18, batch 730, train_loss 0.700\n",
      "epoch 18, batch 740, train_loss 0.690\n",
      "epoch 18, batch 750, train_loss 0.703\n",
      "epoch 18, batch 760, train_loss 0.696\n",
      "epoch 18, batch 770, train_loss 0.700\n",
      "epoch 18, batch 780, train_loss 0.692\n",
      "epoch 18, batch 790, train_loss 0.696\n",
      "epoch 18, batch 800, train_loss 0.692\n",
      "epoch 18, batch 810, train_loss 0.687\n",
      "epoch 18, batch 820, train_loss 0.700\n",
      "epoch 18, batch 830, train_loss 0.700\n",
      "epoch 18, batch 840, train_loss 0.702\n",
      "epoch 18, batch 850, train_loss 0.704\n",
      "epoch 18, batch 860, train_loss 0.700\n",
      "epoch 18, batch 870, train_loss 0.692\n",
      "epoch 18, batch 880, train_loss 0.698\n",
      "epoch 18, batch 890, train_loss 0.709\n",
      "epoch 18, batch 900, train_loss 0.694\n",
      "epoch 18, batch 910, train_loss 0.696\n",
      "epoch 18, batch 920, train_loss 0.691\n",
      "epoch 18, batch 930, train_loss 0.695\n",
      "epoch 18, batch 940, train_loss 0.705\n",
      "epoch 18, batch 950, train_loss 0.697\n",
      "epoch 18, batch 960, train_loss 0.702\n",
      "epoch 18, batch 970, train_loss 0.697\n",
      "epoch 18, batch 980, train_loss 0.702\n",
      "epoch 18, batch 990, train_loss 0.693\n",
      "epoch 18, batch 1000, train_loss 0.696\n",
      "epoch 18, batch 1010, train_loss 0.693\n",
      "epoch 18, batch 1020, train_loss 0.701\n",
      "epoch 18, batch 1030, train_loss 0.697\n",
      "epoch 18, batch 1040, train_loss 0.695\n",
      "epoch 18, batch 1050, train_loss 0.698\n",
      "epoch 18, batch 1060, train_loss 0.700\n",
      "epoch 18, batch 1070, train_loss 0.702\n",
      "epoch 18, batch 1080, train_loss 0.694\n",
      "epoch 18, batch 1090, train_loss 0.705\n",
      "epoch 18, batch 1100, train_loss 0.695\n",
      "epoch 18, batch 1110, train_loss 0.701\n",
      "epoch 18, batch 1120, train_loss 0.706\n",
      "epoch 18, batch 1130, train_loss 0.693\n",
      "epoch 18, batch 1140, train_loss 0.695\n",
      "epoch 18, batch 1150, train_loss 0.701\n",
      "epoch 18, batch 1160, train_loss 0.708\n",
      "epoch 18, batch 1170, train_loss 0.699\n",
      "epoch 18, batch 1180, train_loss 0.692\n",
      "epoch 18, batch 1190, train_loss 0.688\n",
      "epoch    18, train_loss 0.697, valid_loss 0.722, train_accuracy  69.97%, valid_accuracy  68.63%\n",
      "epoch 19, batch 0, train_loss 0.703\n",
      "epoch 19, batch 10, train_loss 0.699\n",
      "epoch 19, batch 20, train_loss 0.702\n",
      "epoch 19, batch 30, train_loss 0.698\n",
      "epoch 19, batch 40, train_loss 0.700\n",
      "epoch 19, batch 50, train_loss 0.701\n",
      "epoch 19, batch 60, train_loss 0.697\n",
      "epoch 19, batch 70, train_loss 0.702\n",
      "epoch 19, batch 80, train_loss 0.699\n",
      "epoch 19, batch 90, train_loss 0.700\n",
      "epoch 19, batch 100, train_loss 0.703\n",
      "epoch 19, batch 110, train_loss 0.705\n",
      "epoch 19, batch 120, train_loss 0.698\n",
      "epoch 19, batch 130, train_loss 0.705\n",
      "epoch 19, batch 140, train_loss 0.692\n",
      "epoch 19, batch 150, train_loss 0.696\n",
      "epoch 19, batch 160, train_loss 0.699\n",
      "epoch 19, batch 170, train_loss 0.688\n",
      "epoch 19, batch 180, train_loss 0.693\n",
      "epoch 19, batch 190, train_loss 0.702\n",
      "epoch 19, batch 200, train_loss 0.693\n",
      "epoch 19, batch 210, train_loss 0.697\n",
      "epoch 19, batch 220, train_loss 0.689\n",
      "epoch 19, batch 230, train_loss 0.698\n",
      "epoch 19, batch 240, train_loss 0.696\n",
      "epoch 19, batch 250, train_loss 0.691\n",
      "epoch 19, batch 260, train_loss 0.704\n",
      "epoch 19, batch 270, train_loss 0.701\n",
      "epoch 19, batch 280, train_loss 0.693\n",
      "epoch 19, batch 290, train_loss 0.698\n",
      "epoch 19, batch 300, train_loss 0.706\n",
      "epoch 19, batch 310, train_loss 0.695\n",
      "epoch 19, batch 320, train_loss 0.694\n",
      "epoch 19, batch 330, train_loss 0.705\n",
      "epoch 19, batch 340, train_loss 0.703\n",
      "epoch 19, batch 350, train_loss 0.694\n",
      "epoch 19, batch 360, train_loss 0.686\n",
      "epoch 19, batch 370, train_loss 0.695\n",
      "epoch 19, batch 380, train_loss 0.690\n",
      "epoch 19, batch 390, train_loss 0.695\n",
      "epoch 19, batch 400, train_loss 0.704\n",
      "epoch 19, batch 410, train_loss 0.696\n",
      "epoch 19, batch 420, train_loss 0.704\n",
      "epoch 19, batch 430, train_loss 0.702\n",
      "epoch 19, batch 440, train_loss 0.693\n",
      "epoch 19, batch 450, train_loss 0.691\n",
      "epoch 19, batch 460, train_loss 0.697\n",
      "epoch 19, batch 470, train_loss 0.687\n",
      "epoch 19, batch 480, train_loss 0.691\n",
      "epoch 19, batch 490, train_loss 0.693\n",
      "epoch 19, batch 500, train_loss 0.700\n",
      "epoch 19, batch 510, train_loss 0.703\n",
      "epoch 19, batch 520, train_loss 0.693\n",
      "epoch 19, batch 530, train_loss 0.696\n",
      "epoch 19, batch 540, train_loss 0.697\n",
      "epoch 19, batch 550, train_loss 0.703\n",
      "epoch 19, batch 560, train_loss 0.699\n",
      "epoch 19, batch 570, train_loss 0.697\n",
      "epoch 19, batch 580, train_loss 0.694\n",
      "epoch 19, batch 590, train_loss 0.692\n",
      "epoch 19, batch 600, train_loss 0.691\n",
      "epoch 19, batch 610, train_loss 0.702\n",
      "epoch 19, batch 620, train_loss 0.698\n",
      "epoch 19, batch 630, train_loss 0.703\n",
      "epoch 19, batch 640, train_loss 0.697\n",
      "epoch 19, batch 650, train_loss 0.695\n",
      "epoch 19, batch 660, train_loss 0.698\n",
      "epoch 19, batch 670, train_loss 0.695\n",
      "epoch 19, batch 680, train_loss 0.705\n",
      "epoch 19, batch 690, train_loss 0.708\n",
      "epoch 19, batch 700, train_loss 0.706\n",
      "epoch 19, batch 710, train_loss 0.705\n",
      "epoch 19, batch 720, train_loss 0.695\n",
      "epoch 19, batch 730, train_loss 0.694\n",
      "epoch 19, batch 740, train_loss 0.710\n",
      "epoch 19, batch 750, train_loss 0.690\n",
      "epoch 19, batch 760, train_loss 0.702\n",
      "epoch 19, batch 770, train_loss 0.708\n",
      "epoch 19, batch 780, train_loss 0.697\n",
      "epoch 19, batch 790, train_loss 0.701\n",
      "epoch 19, batch 800, train_loss 0.693\n",
      "epoch 19, batch 810, train_loss 0.703\n",
      "epoch 19, batch 820, train_loss 0.704\n",
      "epoch 19, batch 830, train_loss 0.701\n",
      "epoch 19, batch 840, train_loss 0.696\n",
      "epoch 19, batch 850, train_loss 0.702\n",
      "epoch 19, batch 860, train_loss 0.691\n",
      "epoch 19, batch 870, train_loss 0.692\n",
      "epoch 19, batch 880, train_loss 0.703\n",
      "epoch 19, batch 890, train_loss 0.688\n",
      "epoch 19, batch 900, train_loss 0.692\n",
      "epoch 19, batch 910, train_loss 0.695\n",
      "epoch 19, batch 920, train_loss 0.695\n",
      "epoch 19, batch 930, train_loss 0.695\n",
      "epoch 19, batch 940, train_loss 0.705\n",
      "epoch 19, batch 950, train_loss 0.697\n",
      "epoch 19, batch 960, train_loss 0.688\n",
      "epoch 19, batch 970, train_loss 0.706\n",
      "epoch 19, batch 980, train_loss 0.706\n",
      "epoch 19, batch 990, train_loss 0.697\n",
      "epoch 19, batch 1000, train_loss 0.700\n",
      "epoch 19, batch 1010, train_loss 0.703\n",
      "epoch 19, batch 1020, train_loss 0.707\n",
      "epoch 19, batch 1030, train_loss 0.695\n",
      "epoch 19, batch 1040, train_loss 0.697\n",
      "epoch 19, batch 1050, train_loss 0.700\n",
      "epoch 19, batch 1060, train_loss 0.702\n",
      "epoch 19, batch 1070, train_loss 0.700\n",
      "epoch 19, batch 1080, train_loss 0.692\n",
      "epoch 19, batch 1090, train_loss 0.706\n",
      "epoch 19, batch 1100, train_loss 0.692\n",
      "epoch 19, batch 1110, train_loss 0.692\n",
      "epoch 19, batch 1120, train_loss 0.704\n",
      "epoch 19, batch 1130, train_loss 0.699\n",
      "epoch 19, batch 1140, train_loss 0.694\n",
      "epoch 19, batch 1150, train_loss 0.706\n",
      "epoch 19, batch 1160, train_loss 0.692\n",
      "epoch 19, batch 1170, train_loss 0.694\n",
      "epoch 19, batch 1180, train_loss 0.695\n",
      "epoch 19, batch 1190, train_loss 0.702\n",
      "epoch    19, train_loss 0.697, valid_loss 0.722, train_accuracy  69.99%, valid_accuracy  68.59%\n",
      "epoch 20, batch 0, train_loss 0.698\n",
      "epoch 20, batch 10, train_loss 0.699\n",
      "epoch 20, batch 20, train_loss 0.704\n",
      "epoch 20, batch 30, train_loss 0.702\n",
      "epoch 20, batch 40, train_loss 0.692\n",
      "epoch 20, batch 50, train_loss 0.691\n",
      "epoch 20, batch 60, train_loss 0.690\n",
      "epoch 20, batch 70, train_loss 0.701\n",
      "epoch 20, batch 80, train_loss 0.689\n",
      "epoch 20, batch 90, train_loss 0.701\n",
      "epoch 20, batch 100, train_loss 0.698\n",
      "epoch 20, batch 110, train_loss 0.701\n",
      "epoch 20, batch 120, train_loss 0.692\n",
      "epoch 20, batch 130, train_loss 0.704\n",
      "epoch 20, batch 140, train_loss 0.697\n",
      "epoch 20, batch 150, train_loss 0.699\n",
      "epoch 20, batch 160, train_loss 0.696\n",
      "epoch 20, batch 170, train_loss 0.703\n",
      "epoch 20, batch 180, train_loss 0.691\n",
      "epoch 20, batch 190, train_loss 0.700\n",
      "epoch 20, batch 200, train_loss 0.699\n",
      "epoch 20, batch 210, train_loss 0.697\n",
      "epoch 20, batch 220, train_loss 0.694\n",
      "epoch 20, batch 230, train_loss 0.695\n",
      "epoch 20, batch 240, train_loss 0.698\n",
      "epoch 20, batch 250, train_loss 0.694\n",
      "epoch 20, batch 260, train_loss 0.698\n",
      "epoch 20, batch 270, train_loss 0.703\n",
      "epoch 20, batch 280, train_loss 0.695\n",
      "epoch 20, batch 290, train_loss 0.697\n",
      "epoch 20, batch 300, train_loss 0.695\n",
      "epoch 20, batch 310, train_loss 0.693\n",
      "epoch 20, batch 320, train_loss 0.685\n",
      "epoch 20, batch 330, train_loss 0.706\n",
      "epoch 20, batch 340, train_loss 0.708\n",
      "epoch 20, batch 350, train_loss 0.687\n",
      "epoch 20, batch 360, train_loss 0.698\n",
      "epoch 20, batch 370, train_loss 0.688\n",
      "epoch 20, batch 380, train_loss 0.689\n",
      "epoch 20, batch 390, train_loss 0.703\n",
      "epoch 20, batch 400, train_loss 0.703\n",
      "epoch 20, batch 410, train_loss 0.703\n",
      "epoch 20, batch 420, train_loss 0.698\n",
      "epoch 20, batch 430, train_loss 0.700\n",
      "epoch 20, batch 440, train_loss 0.701\n",
      "epoch 20, batch 450, train_loss 0.697\n",
      "epoch 20, batch 460, train_loss 0.693\n",
      "epoch 20, batch 470, train_loss 0.700\n",
      "epoch 20, batch 480, train_loss 0.701\n",
      "epoch 20, batch 490, train_loss 0.694\n",
      "epoch 20, batch 500, train_loss 0.703\n",
      "epoch 20, batch 510, train_loss 0.702\n",
      "epoch 20, batch 520, train_loss 0.693\n",
      "epoch 20, batch 530, train_loss 0.696\n",
      "epoch 20, batch 540, train_loss 0.702\n",
      "epoch 20, batch 550, train_loss 0.697\n",
      "epoch 20, batch 560, train_loss 0.692\n",
      "epoch 20, batch 570, train_loss 0.705\n",
      "epoch 20, batch 580, train_loss 0.701\n",
      "epoch 20, batch 590, train_loss 0.700\n",
      "epoch 20, batch 600, train_loss 0.700\n",
      "epoch 20, batch 610, train_loss 0.709\n",
      "epoch 20, batch 620, train_loss 0.696\n",
      "epoch 20, batch 630, train_loss 0.694\n",
      "epoch 20, batch 640, train_loss 0.696\n",
      "epoch 20, batch 650, train_loss 0.696\n",
      "epoch 20, batch 660, train_loss 0.703\n",
      "epoch 20, batch 670, train_loss 0.700\n",
      "epoch 20, batch 680, train_loss 0.686\n",
      "epoch 20, batch 690, train_loss 0.701\n",
      "epoch 20, batch 700, train_loss 0.695\n",
      "epoch 20, batch 710, train_loss 0.699\n",
      "epoch 20, batch 720, train_loss 0.690\n",
      "epoch 20, batch 730, train_loss 0.690\n",
      "epoch 20, batch 740, train_loss 0.701\n",
      "epoch 20, batch 750, train_loss 0.698\n",
      "epoch 20, batch 760, train_loss 0.697\n",
      "epoch 20, batch 770, train_loss 0.698\n",
      "epoch 20, batch 780, train_loss 0.689\n",
      "epoch 20, batch 790, train_loss 0.697\n",
      "epoch 20, batch 800, train_loss 0.705\n",
      "epoch 20, batch 810, train_loss 0.688\n",
      "epoch 20, batch 820, train_loss 0.701\n",
      "epoch 20, batch 830, train_loss 0.701\n",
      "epoch 20, batch 840, train_loss 0.695\n",
      "epoch 20, batch 850, train_loss 0.703\n",
      "epoch 20, batch 860, train_loss 0.704\n",
      "epoch 20, batch 870, train_loss 0.708\n",
      "epoch 20, batch 880, train_loss 0.701\n",
      "epoch 20, batch 890, train_loss 0.694\n",
      "epoch 20, batch 900, train_loss 0.697\n",
      "epoch 20, batch 910, train_loss 0.698\n",
      "epoch 20, batch 920, train_loss 0.692\n",
      "epoch 20, batch 930, train_loss 0.692\n",
      "epoch 20, batch 940, train_loss 0.699\n",
      "epoch 20, batch 950, train_loss 0.697\n",
      "epoch 20, batch 960, train_loss 0.704\n",
      "epoch 20, batch 970, train_loss 0.705\n",
      "epoch 20, batch 980, train_loss 0.698\n",
      "epoch 20, batch 990, train_loss 0.687\n",
      "epoch 20, batch 1000, train_loss 0.694\n",
      "epoch 20, batch 1010, train_loss 0.696\n",
      "epoch 20, batch 1020, train_loss 0.690\n",
      "epoch 20, batch 1030, train_loss 0.705\n",
      "epoch 20, batch 1040, train_loss 0.699\n",
      "epoch 20, batch 1050, train_loss 0.691\n",
      "epoch 20, batch 1060, train_loss 0.693\n",
      "epoch 20, batch 1070, train_loss 0.685\n",
      "epoch 20, batch 1080, train_loss 0.703\n",
      "epoch 20, batch 1090, train_loss 0.698\n",
      "epoch 20, batch 1100, train_loss 0.695\n",
      "epoch 20, batch 1110, train_loss 0.690\n",
      "epoch 20, batch 1120, train_loss 0.692\n",
      "epoch 20, batch 1130, train_loss 0.697\n",
      "epoch 20, batch 1140, train_loss 0.688\n",
      "epoch 20, batch 1150, train_loss 0.684\n",
      "epoch 20, batch 1160, train_loss 0.694\n",
      "epoch 20, batch 1170, train_loss 0.700\n",
      "epoch 20, batch 1180, train_loss 0.699\n",
      "epoch 20, batch 1190, train_loss 0.685\n",
      "epoch    20, train_loss 0.696, valid_loss 0.722, train_accuracy  70.02%, valid_accuracy  68.66%\n",
      "epoch 21, batch 0, train_loss 0.685\n",
      "epoch 21, batch 10, train_loss 0.694\n",
      "epoch 21, batch 20, train_loss 0.708\n",
      "epoch 21, batch 30, train_loss 0.700\n",
      "epoch 21, batch 40, train_loss 0.697\n",
      "epoch 21, batch 50, train_loss 0.699\n",
      "epoch 21, batch 60, train_loss 0.683\n",
      "epoch 21, batch 70, train_loss 0.694\n",
      "epoch 21, batch 80, train_loss 0.701\n",
      "epoch 21, batch 90, train_loss 0.692\n",
      "epoch 21, batch 100, train_loss 0.697\n",
      "epoch 21, batch 110, train_loss 0.691\n",
      "epoch 21, batch 120, train_loss 0.710\n",
      "epoch 21, batch 130, train_loss 0.694\n",
      "epoch 21, batch 140, train_loss 0.697\n",
      "epoch 21, batch 150, train_loss 0.698\n",
      "epoch 21, batch 160, train_loss 0.696\n",
      "epoch 21, batch 170, train_loss 0.690\n",
      "epoch 21, batch 180, train_loss 0.692\n",
      "epoch 21, batch 190, train_loss 0.703\n",
      "epoch 21, batch 200, train_loss 0.696\n",
      "epoch 21, batch 210, train_loss 0.696\n",
      "epoch 21, batch 220, train_loss 0.693\n",
      "epoch 21, batch 230, train_loss 0.704\n",
      "epoch 21, batch 240, train_loss 0.695\n",
      "epoch 21, batch 250, train_loss 0.695\n",
      "epoch 21, batch 260, train_loss 0.709\n",
      "epoch 21, batch 270, train_loss 0.707\n",
      "epoch 21, batch 280, train_loss 0.681\n",
      "epoch 21, batch 290, train_loss 0.703\n",
      "epoch 21, batch 300, train_loss 0.704\n",
      "epoch 21, batch 310, train_loss 0.702\n",
      "epoch 21, batch 320, train_loss 0.697\n",
      "epoch 21, batch 330, train_loss 0.695\n",
      "epoch 21, batch 340, train_loss 0.699\n",
      "epoch 21, batch 350, train_loss 0.707\n",
      "epoch 21, batch 360, train_loss 0.697\n",
      "epoch 21, batch 370, train_loss 0.705\n",
      "epoch 21, batch 380, train_loss 0.692\n",
      "epoch 21, batch 390, train_loss 0.694\n",
      "epoch 21, batch 400, train_loss 0.701\n",
      "epoch 21, batch 410, train_loss 0.691\n",
      "epoch 21, batch 420, train_loss 0.704\n",
      "epoch 21, batch 430, train_loss 0.690\n",
      "epoch 21, batch 440, train_loss 0.694\n",
      "epoch 21, batch 450, train_loss 0.697\n",
      "epoch 21, batch 460, train_loss 0.697\n",
      "epoch 21, batch 470, train_loss 0.697\n",
      "epoch 21, batch 480, train_loss 0.694\n",
      "epoch 21, batch 490, train_loss 0.702\n",
      "epoch 21, batch 500, train_loss 0.700\n",
      "epoch 21, batch 510, train_loss 0.695\n",
      "epoch 21, batch 520, train_loss 0.697\n",
      "epoch 21, batch 530, train_loss 0.690\n",
      "epoch 21, batch 540, train_loss 0.696\n",
      "epoch 21, batch 550, train_loss 0.703\n",
      "epoch 21, batch 560, train_loss 0.701\n",
      "epoch 21, batch 570, train_loss 0.698\n",
      "epoch 21, batch 580, train_loss 0.691\n",
      "epoch 21, batch 590, train_loss 0.699\n",
      "epoch 21, batch 600, train_loss 0.690\n",
      "epoch 21, batch 610, train_loss 0.702\n",
      "epoch 21, batch 620, train_loss 0.702\n",
      "epoch 21, batch 630, train_loss 0.699\n",
      "epoch 21, batch 640, train_loss 0.698\n",
      "epoch 21, batch 650, train_loss 0.691\n",
      "epoch 21, batch 660, train_loss 0.703\n",
      "epoch 21, batch 670, train_loss 0.698\n",
      "epoch 21, batch 680, train_loss 0.697\n",
      "epoch 21, batch 690, train_loss 0.699\n",
      "epoch 21, batch 700, train_loss 0.700\n",
      "epoch 21, batch 710, train_loss 0.696\n",
      "epoch 21, batch 720, train_loss 0.689\n",
      "epoch 21, batch 730, train_loss 0.694\n",
      "epoch 21, batch 740, train_loss 0.688\n",
      "epoch 21, batch 750, train_loss 0.697\n",
      "epoch 21, batch 760, train_loss 0.708\n",
      "epoch 21, batch 770, train_loss 0.693\n",
      "epoch 21, batch 780, train_loss 0.704\n",
      "epoch 21, batch 790, train_loss 0.705\n",
      "epoch 21, batch 800, train_loss 0.688\n",
      "epoch 21, batch 810, train_loss 0.700\n",
      "epoch 21, batch 820, train_loss 0.704\n",
      "epoch 21, batch 830, train_loss 0.702\n",
      "epoch 21, batch 840, train_loss 0.701\n",
      "epoch 21, batch 850, train_loss 0.679\n",
      "epoch 21, batch 860, train_loss 0.710\n",
      "epoch 21, batch 870, train_loss 0.695\n",
      "epoch 21, batch 880, train_loss 0.705\n",
      "epoch 21, batch 890, train_loss 0.688\n",
      "epoch 21, batch 900, train_loss 0.705\n",
      "epoch 21, batch 910, train_loss 0.702\n",
      "epoch 21, batch 920, train_loss 0.713\n",
      "epoch 21, batch 930, train_loss 0.690\n",
      "epoch 21, batch 940, train_loss 0.701\n",
      "epoch 21, batch 950, train_loss 0.698\n",
      "epoch 21, batch 960, train_loss 0.698\n",
      "epoch 21, batch 970, train_loss 0.696\n",
      "epoch 21, batch 980, train_loss 0.696\n",
      "epoch 21, batch 990, train_loss 0.696\n",
      "epoch 21, batch 1000, train_loss 0.696\n",
      "epoch 21, batch 1010, train_loss 0.699\n",
      "epoch 21, batch 1020, train_loss 0.691\n",
      "epoch 21, batch 1030, train_loss 0.702\n",
      "epoch 21, batch 1040, train_loss 0.694\n",
      "epoch 21, batch 1050, train_loss 0.709\n",
      "epoch 21, batch 1060, train_loss 0.690\n",
      "epoch 21, batch 1070, train_loss 0.703\n",
      "epoch 21, batch 1080, train_loss 0.702\n",
      "epoch 21, batch 1090, train_loss 0.699\n",
      "epoch 21, batch 1100, train_loss 0.689\n",
      "epoch 21, batch 1110, train_loss 0.701\n",
      "epoch 21, batch 1120, train_loss 0.708\n",
      "epoch 21, batch 1130, train_loss 0.694\n",
      "epoch 21, batch 1140, train_loss 0.696\n",
      "epoch 21, batch 1150, train_loss 0.701\n",
      "epoch 21, batch 1160, train_loss 0.701\n",
      "epoch 21, batch 1170, train_loss 0.706\n",
      "epoch 21, batch 1180, train_loss 0.703\n",
      "epoch 21, batch 1190, train_loss 0.697\n",
      "epoch    21, train_loss 0.697, valid_loss 0.723, train_accuracy  69.97%, valid_accuracy  68.58%\n",
      "epoch 22, batch 0, train_loss 0.697\n",
      "epoch 22, batch 10, train_loss 0.697\n",
      "epoch 22, batch 20, train_loss 0.699\n",
      "epoch 22, batch 30, train_loss 0.697\n",
      "epoch 22, batch 40, train_loss 0.692\n",
      "epoch 22, batch 50, train_loss 0.701\n",
      "epoch 22, batch 60, train_loss 0.705\n",
      "epoch 22, batch 70, train_loss 0.688\n",
      "epoch 22, batch 80, train_loss 0.692\n",
      "epoch 22, batch 90, train_loss 0.699\n",
      "epoch 22, batch 100, train_loss 0.682\n",
      "epoch 22, batch 110, train_loss 0.704\n",
      "epoch 22, batch 120, train_loss 0.704\n",
      "epoch 22, batch 130, train_loss 0.697\n",
      "epoch 22, batch 140, train_loss 0.685\n",
      "epoch 22, batch 150, train_loss 0.697\n",
      "epoch 22, batch 160, train_loss 0.693\n",
      "epoch 22, batch 170, train_loss 0.691\n",
      "epoch 22, batch 180, train_loss 0.692\n",
      "epoch 22, batch 190, train_loss 0.697\n",
      "epoch 22, batch 200, train_loss 0.698\n",
      "epoch 22, batch 210, train_loss 0.698\n",
      "epoch 22, batch 220, train_loss 0.700\n",
      "epoch 22, batch 230, train_loss 0.700\n",
      "epoch 22, batch 240, train_loss 0.684\n",
      "epoch 22, batch 250, train_loss 0.708\n",
      "epoch 22, batch 260, train_loss 0.694\n",
      "epoch 22, batch 270, train_loss 0.695\n",
      "epoch 22, batch 280, train_loss 0.708\n",
      "epoch 22, batch 290, train_loss 0.697\n",
      "epoch 22, batch 300, train_loss 0.693\n",
      "epoch 22, batch 310, train_loss 0.699\n",
      "epoch 22, batch 320, train_loss 0.698\n",
      "epoch 22, batch 330, train_loss 0.695\n",
      "epoch 22, batch 340, train_loss 0.704\n",
      "epoch 22, batch 350, train_loss 0.697\n",
      "epoch 22, batch 360, train_loss 0.695\n",
      "epoch 22, batch 370, train_loss 0.700\n",
      "epoch 22, batch 380, train_loss 0.698\n",
      "epoch 22, batch 390, train_loss 0.693\n",
      "epoch 22, batch 400, train_loss 0.696\n",
      "epoch 22, batch 410, train_loss 0.701\n",
      "epoch 22, batch 420, train_loss 0.705\n",
      "epoch 22, batch 430, train_loss 0.698\n",
      "epoch 22, batch 440, train_loss 0.708\n",
      "epoch 22, batch 450, train_loss 0.696\n",
      "epoch 22, batch 460, train_loss 0.696\n",
      "epoch 22, batch 470, train_loss 0.692\n",
      "epoch 22, batch 480, train_loss 0.694\n",
      "epoch 22, batch 490, train_loss 0.698\n",
      "epoch 22, batch 500, train_loss 0.701\n",
      "epoch 22, batch 510, train_loss 0.699\n",
      "epoch 22, batch 520, train_loss 0.698\n",
      "epoch 22, batch 530, train_loss 0.693\n",
      "epoch 22, batch 540, train_loss 0.696\n",
      "epoch 22, batch 550, train_loss 0.703\n",
      "epoch 22, batch 560, train_loss 0.698\n",
      "epoch 22, batch 570, train_loss 0.700\n",
      "epoch 22, batch 580, train_loss 0.695\n",
      "epoch 22, batch 590, train_loss 0.696\n",
      "epoch 22, batch 600, train_loss 0.694\n",
      "epoch 22, batch 610, train_loss 0.688\n",
      "epoch 22, batch 620, train_loss 0.701\n",
      "epoch 22, batch 630, train_loss 0.692\n",
      "epoch 22, batch 640, train_loss 0.693\n",
      "epoch 22, batch 650, train_loss 0.685\n",
      "epoch 22, batch 660, train_loss 0.706\n",
      "epoch 22, batch 670, train_loss 0.694\n",
      "epoch 22, batch 680, train_loss 0.689\n",
      "epoch 22, batch 690, train_loss 0.699\n",
      "epoch 22, batch 700, train_loss 0.701\n",
      "epoch 22, batch 710, train_loss 0.695\n",
      "epoch 22, batch 720, train_loss 0.699\n",
      "epoch 22, batch 730, train_loss 0.695\n",
      "epoch 22, batch 740, train_loss 0.690\n",
      "epoch 22, batch 750, train_loss 0.685\n",
      "epoch 22, batch 760, train_loss 0.700\n",
      "epoch 22, batch 770, train_loss 0.695\n",
      "epoch 22, batch 780, train_loss 0.699\n",
      "epoch 22, batch 790, train_loss 0.706\n",
      "epoch 22, batch 800, train_loss 0.701\n",
      "epoch 22, batch 810, train_loss 0.687\n",
      "epoch 22, batch 820, train_loss 0.689\n",
      "epoch 22, batch 830, train_loss 0.698\n",
      "epoch 22, batch 840, train_loss 0.695\n",
      "epoch 22, batch 850, train_loss 0.702\n",
      "epoch 22, batch 860, train_loss 0.702\n",
      "epoch 22, batch 870, train_loss 0.692\n",
      "epoch 22, batch 880, train_loss 0.688\n",
      "epoch 22, batch 890, train_loss 0.704\n",
      "epoch 22, batch 900, train_loss 0.705\n",
      "epoch 22, batch 910, train_loss 0.692\n",
      "epoch 22, batch 920, train_loss 0.711\n",
      "epoch 22, batch 930, train_loss 0.704\n",
      "epoch 22, batch 940, train_loss 0.692\n",
      "epoch 22, batch 950, train_loss 0.703\n",
      "epoch 22, batch 960, train_loss 0.695\n",
      "epoch 22, batch 970, train_loss 0.697\n",
      "epoch 22, batch 980, train_loss 0.690\n",
      "epoch 22, batch 990, train_loss 0.707\n",
      "epoch 22, batch 1000, train_loss 0.699\n",
      "epoch 22, batch 1010, train_loss 0.694\n",
      "epoch 22, batch 1020, train_loss 0.697\n",
      "epoch 22, batch 1030, train_loss 0.699\n",
      "epoch 22, batch 1040, train_loss 0.693\n",
      "epoch 22, batch 1050, train_loss 0.708\n",
      "epoch 22, batch 1060, train_loss 0.692\n",
      "epoch 22, batch 1070, train_loss 0.704\n",
      "epoch 22, batch 1080, train_loss 0.697\n",
      "epoch 22, batch 1090, train_loss 0.691\n",
      "epoch 22, batch 1100, train_loss 0.700\n",
      "epoch 22, batch 1110, train_loss 0.698\n",
      "epoch 22, batch 1120, train_loss 0.697\n",
      "epoch 22, batch 1130, train_loss 0.698\n",
      "epoch 22, batch 1140, train_loss 0.702\n",
      "epoch 22, batch 1150, train_loss 0.694\n",
      "epoch 22, batch 1160, train_loss 0.703\n",
      "epoch 22, batch 1170, train_loss 0.695\n",
      "epoch 22, batch 1180, train_loss 0.703\n",
      "epoch 22, batch 1190, train_loss 0.697\n",
      "epoch    22, train_loss 0.696, valid_loss 0.721, train_accuracy  70.10%, valid_accuracy  68.62%\n",
      "epoch 23, batch 0, train_loss 0.696\n",
      "epoch 23, batch 10, train_loss 0.697\n",
      "epoch 23, batch 20, train_loss 0.693\n",
      "epoch 23, batch 30, train_loss 0.694\n",
      "epoch 23, batch 40, train_loss 0.691\n",
      "epoch 23, batch 50, train_loss 0.693\n",
      "epoch 23, batch 60, train_loss 0.697\n",
      "epoch 23, batch 70, train_loss 0.698\n",
      "epoch 23, batch 80, train_loss 0.689\n",
      "epoch 23, batch 90, train_loss 0.693\n",
      "epoch 23, batch 100, train_loss 0.697\n",
      "epoch 23, batch 110, train_loss 0.697\n",
      "epoch 23, batch 120, train_loss 0.691\n",
      "epoch 23, batch 130, train_loss 0.699\n",
      "epoch 23, batch 140, train_loss 0.695\n",
      "epoch 23, batch 150, train_loss 0.696\n",
      "epoch 23, batch 160, train_loss 0.687\n",
      "epoch 23, batch 170, train_loss 0.693\n",
      "epoch 23, batch 180, train_loss 0.682\n",
      "epoch 23, batch 190, train_loss 0.699\n",
      "epoch 23, batch 200, train_loss 0.704\n",
      "epoch 23, batch 210, train_loss 0.696\n",
      "epoch 23, batch 220, train_loss 0.683\n",
      "epoch 23, batch 230, train_loss 0.703\n",
      "epoch 23, batch 240, train_loss 0.702\n",
      "epoch 23, batch 250, train_loss 0.691\n",
      "epoch 23, batch 260, train_loss 0.702\n",
      "epoch 23, batch 270, train_loss 0.695\n",
      "epoch 23, batch 280, train_loss 0.691\n",
      "epoch 23, batch 290, train_loss 0.697\n",
      "epoch 23, batch 300, train_loss 0.693\n",
      "epoch 23, batch 310, train_loss 0.709\n",
      "epoch 23, batch 320, train_loss 0.699\n",
      "epoch 23, batch 330, train_loss 0.702\n",
      "epoch 23, batch 340, train_loss 0.702\n",
      "epoch 23, batch 350, train_loss 0.704\n",
      "epoch 23, batch 360, train_loss 0.694\n",
      "epoch 23, batch 370, train_loss 0.693\n",
      "epoch 23, batch 380, train_loss 0.713\n",
      "epoch 23, batch 390, train_loss 0.701\n",
      "epoch 23, batch 400, train_loss 0.693\n",
      "epoch 23, batch 410, train_loss 0.708\n",
      "epoch 23, batch 420, train_loss 0.701\n",
      "epoch 23, batch 430, train_loss 0.696\n",
      "epoch 23, batch 440, train_loss 0.695\n",
      "epoch 23, batch 450, train_loss 0.686\n",
      "epoch 23, batch 460, train_loss 0.700\n",
      "epoch 23, batch 470, train_loss 0.702\n",
      "epoch 23, batch 480, train_loss 0.688\n",
      "epoch 23, batch 490, train_loss 0.702\n",
      "epoch 23, batch 500, train_loss 0.701\n",
      "epoch 23, batch 510, train_loss 0.694\n",
      "epoch 23, batch 520, train_loss 0.693\n",
      "epoch 23, batch 530, train_loss 0.694\n",
      "epoch 23, batch 540, train_loss 0.701\n",
      "epoch 23, batch 550, train_loss 0.695\n",
      "epoch 23, batch 560, train_loss 0.693\n",
      "epoch 23, batch 570, train_loss 0.696\n",
      "epoch 23, batch 580, train_loss 0.699\n",
      "epoch 23, batch 590, train_loss 0.702\n",
      "epoch 23, batch 600, train_loss 0.696\n",
      "epoch 23, batch 610, train_loss 0.705\n",
      "epoch 23, batch 620, train_loss 0.695\n",
      "epoch 23, batch 630, train_loss 0.694\n",
      "epoch 23, batch 640, train_loss 0.702\n",
      "epoch 23, batch 650, train_loss 0.702\n",
      "epoch 23, batch 660, train_loss 0.697\n",
      "epoch 23, batch 670, train_loss 0.702\n",
      "epoch 23, batch 680, train_loss 0.699\n",
      "epoch 23, batch 690, train_loss 0.696\n",
      "epoch 23, batch 700, train_loss 0.698\n",
      "epoch 23, batch 710, train_loss 0.697\n",
      "epoch 23, batch 720, train_loss 0.692\n",
      "epoch 23, batch 730, train_loss 0.700\n",
      "epoch 23, batch 740, train_loss 0.694\n",
      "epoch 23, batch 750, train_loss 0.694\n",
      "epoch 23, batch 760, train_loss 0.710\n",
      "epoch 23, batch 770, train_loss 0.694\n",
      "epoch 23, batch 780, train_loss 0.701\n",
      "epoch 23, batch 790, train_loss 0.691\n",
      "epoch 23, batch 800, train_loss 0.702\n",
      "epoch 23, batch 810, train_loss 0.694\n",
      "epoch 23, batch 820, train_loss 0.704\n",
      "epoch 23, batch 830, train_loss 0.691\n",
      "epoch 23, batch 840, train_loss 0.705\n",
      "epoch 23, batch 850, train_loss 0.701\n",
      "epoch 23, batch 860, train_loss 0.690\n",
      "epoch 23, batch 870, train_loss 0.701\n",
      "epoch 23, batch 880, train_loss 0.702\n",
      "epoch 23, batch 890, train_loss 0.704\n",
      "epoch 23, batch 900, train_loss 0.690\n",
      "epoch 23, batch 910, train_loss 0.689\n",
      "epoch 23, batch 920, train_loss 0.703\n",
      "epoch 23, batch 930, train_loss 0.692\n",
      "epoch 23, batch 940, train_loss 0.700\n",
      "epoch 23, batch 950, train_loss 0.705\n",
      "epoch 23, batch 960, train_loss 0.691\n",
      "epoch 23, batch 970, train_loss 0.698\n",
      "epoch 23, batch 980, train_loss 0.688\n",
      "epoch 23, batch 990, train_loss 0.697\n",
      "epoch 23, batch 1000, train_loss 0.694\n",
      "epoch 23, batch 1010, train_loss 0.690\n",
      "epoch 23, batch 1020, train_loss 0.701\n",
      "epoch 23, batch 1030, train_loss 0.697\n",
      "epoch 23, batch 1040, train_loss 0.700\n",
      "epoch 23, batch 1050, train_loss 0.692\n",
      "epoch 23, batch 1060, train_loss 0.698\n",
      "epoch 23, batch 1070, train_loss 0.692\n",
      "epoch 23, batch 1080, train_loss 0.694\n",
      "epoch 23, batch 1090, train_loss 0.709\n",
      "epoch 23, batch 1100, train_loss 0.688\n",
      "epoch 23, batch 1110, train_loss 0.695\n",
      "epoch 23, batch 1120, train_loss 0.702\n",
      "epoch 23, batch 1130, train_loss 0.701\n",
      "epoch 23, batch 1140, train_loss 0.695\n",
      "epoch 23, batch 1150, train_loss 0.703\n",
      "epoch 23, batch 1160, train_loss 0.700\n",
      "epoch 23, batch 1170, train_loss 0.699\n",
      "epoch 23, batch 1180, train_loss 0.706\n",
      "epoch 23, batch 1190, train_loss 0.699\n",
      "epoch    23, train_loss 0.695, valid_loss 0.722, train_accuracy  70.08%, valid_accuracy  68.62%\n",
      "epoch 24, batch 0, train_loss 0.703\n",
      "epoch 24, batch 10, train_loss 0.699\n",
      "epoch 24, batch 20, train_loss 0.698\n",
      "epoch 24, batch 30, train_loss 0.684\n",
      "epoch 24, batch 40, train_loss 0.698\n",
      "epoch 24, batch 50, train_loss 0.694\n",
      "epoch 24, batch 60, train_loss 0.696\n",
      "epoch 24, batch 70, train_loss 0.688\n",
      "epoch 24, batch 80, train_loss 0.698\n",
      "epoch 24, batch 90, train_loss 0.706\n",
      "epoch 24, batch 100, train_loss 0.704\n",
      "epoch 24, batch 110, train_loss 0.694\n",
      "epoch 24, batch 120, train_loss 0.696\n",
      "epoch 24, batch 130, train_loss 0.699\n",
      "epoch 24, batch 140, train_loss 0.695\n",
      "epoch 24, batch 150, train_loss 0.689\n",
      "epoch 24, batch 160, train_loss 0.696\n",
      "epoch 24, batch 170, train_loss 0.694\n",
      "epoch 24, batch 180, train_loss 0.694\n",
      "epoch 24, batch 190, train_loss 0.707\n",
      "epoch 24, batch 200, train_loss 0.700\n",
      "epoch 24, batch 210, train_loss 0.687\n",
      "epoch 24, batch 220, train_loss 0.697\n",
      "epoch 24, batch 230, train_loss 0.692\n",
      "epoch 24, batch 240, train_loss 0.691\n",
      "epoch 24, batch 250, train_loss 0.696\n",
      "epoch 24, batch 260, train_loss 0.703\n",
      "epoch 24, batch 270, train_loss 0.703\n",
      "epoch 24, batch 280, train_loss 0.696\n",
      "epoch 24, batch 290, train_loss 0.694\n",
      "epoch 24, batch 300, train_loss 0.696\n",
      "epoch 24, batch 310, train_loss 0.689\n",
      "epoch 24, batch 320, train_loss 0.702\n",
      "epoch 24, batch 330, train_loss 0.690\n",
      "epoch 24, batch 340, train_loss 0.699\n",
      "epoch 24, batch 350, train_loss 0.697\n",
      "epoch 24, batch 360, train_loss 0.688\n",
      "epoch 24, batch 370, train_loss 0.703\n",
      "epoch 24, batch 380, train_loss 0.693\n",
      "epoch 24, batch 390, train_loss 0.694\n",
      "epoch 24, batch 400, train_loss 0.698\n",
      "epoch 24, batch 410, train_loss 0.694\n",
      "epoch 24, batch 420, train_loss 0.693\n",
      "epoch 24, batch 430, train_loss 0.710\n",
      "epoch 24, batch 440, train_loss 0.707\n",
      "epoch 24, batch 450, train_loss 0.710\n",
      "epoch 24, batch 460, train_loss 0.698\n",
      "epoch 24, batch 470, train_loss 0.692\n",
      "epoch 24, batch 480, train_loss 0.699\n",
      "epoch 24, batch 490, train_loss 0.696\n",
      "epoch 24, batch 500, train_loss 0.700\n",
      "epoch 24, batch 510, train_loss 0.691\n",
      "epoch 24, batch 520, train_loss 0.704\n",
      "epoch 24, batch 530, train_loss 0.699\n",
      "epoch 24, batch 540, train_loss 0.696\n",
      "epoch 24, batch 550, train_loss 0.696\n",
      "epoch 24, batch 560, train_loss 0.701\n",
      "epoch 24, batch 570, train_loss 0.694\n",
      "epoch 24, batch 580, train_loss 0.708\n",
      "epoch 24, batch 590, train_loss 0.689\n",
      "epoch 24, batch 600, train_loss 0.704\n",
      "epoch 24, batch 610, train_loss 0.695\n",
      "epoch 24, batch 620, train_loss 0.691\n",
      "epoch 24, batch 630, train_loss 0.693\n",
      "epoch 24, batch 640, train_loss 0.700\n",
      "epoch 24, batch 650, train_loss 0.694\n",
      "epoch 24, batch 660, train_loss 0.702\n",
      "epoch 24, batch 670, train_loss 0.698\n",
      "epoch 24, batch 680, train_loss 0.700\n",
      "epoch 24, batch 690, train_loss 0.692\n",
      "epoch 24, batch 700, train_loss 0.690\n",
      "epoch 24, batch 710, train_loss 0.696\n",
      "epoch 24, batch 720, train_loss 0.697\n",
      "epoch 24, batch 730, train_loss 0.693\n",
      "epoch 24, batch 740, train_loss 0.702\n",
      "epoch 24, batch 750, train_loss 0.698\n",
      "epoch 24, batch 760, train_loss 0.702\n",
      "epoch 24, batch 770, train_loss 0.699\n",
      "epoch 24, batch 780, train_loss 0.697\n",
      "epoch 24, batch 790, train_loss 0.703\n",
      "epoch 24, batch 800, train_loss 0.704\n",
      "epoch 24, batch 810, train_loss 0.700\n",
      "epoch 24, batch 820, train_loss 0.696\n",
      "epoch 24, batch 830, train_loss 0.695\n",
      "epoch 24, batch 840, train_loss 0.692\n",
      "epoch 24, batch 850, train_loss 0.694\n",
      "epoch 24, batch 860, train_loss 0.695\n",
      "epoch 24, batch 870, train_loss 0.696\n",
      "epoch 24, batch 880, train_loss 0.698\n",
      "epoch 24, batch 890, train_loss 0.701\n",
      "epoch 24, batch 900, train_loss 0.694\n",
      "epoch 24, batch 910, train_loss 0.698\n",
      "epoch 24, batch 920, train_loss 0.697\n",
      "epoch 24, batch 930, train_loss 0.697\n",
      "epoch 24, batch 940, train_loss 0.710\n",
      "epoch 24, batch 950, train_loss 0.697\n",
      "epoch 24, batch 960, train_loss 0.695\n",
      "epoch 24, batch 970, train_loss 0.693\n",
      "epoch 24, batch 980, train_loss 0.703\n",
      "epoch 24, batch 990, train_loss 0.703\n",
      "epoch 24, batch 1000, train_loss 0.701\n",
      "epoch 24, batch 1010, train_loss 0.688\n",
      "epoch 24, batch 1020, train_loss 0.693\n",
      "epoch 24, batch 1030, train_loss 0.697\n",
      "epoch 24, batch 1040, train_loss 0.700\n",
      "epoch 24, batch 1050, train_loss 0.690\n",
      "epoch 24, batch 1060, train_loss 0.690\n",
      "epoch 24, batch 1070, train_loss 0.695\n",
      "epoch 24, batch 1080, train_loss 0.685\n",
      "epoch 24, batch 1090, train_loss 0.689\n",
      "epoch 24, batch 1100, train_loss 0.690\n",
      "epoch 24, batch 1110, train_loss 0.692\n",
      "epoch 24, batch 1120, train_loss 0.698\n",
      "epoch 24, batch 1130, train_loss 0.702\n",
      "epoch 24, batch 1140, train_loss 0.696\n",
      "epoch 24, batch 1150, train_loss 0.708\n",
      "epoch 24, batch 1160, train_loss 0.700\n",
      "epoch 24, batch 1170, train_loss 0.695\n",
      "epoch 24, batch 1180, train_loss 0.696\n",
      "epoch 24, batch 1190, train_loss 0.696\n",
      "epoch    24, train_loss 0.695, valid_loss 0.722, train_accuracy  70.10%, valid_accuracy  68.61%\n",
      "epoch 25, batch 0, train_loss 0.693\n",
      "epoch 25, batch 10, train_loss 0.685\n",
      "epoch 25, batch 20, train_loss 0.701\n",
      "epoch 25, batch 30, train_loss 0.696\n",
      "epoch 25, batch 40, train_loss 0.691\n",
      "epoch 25, batch 50, train_loss 0.695\n",
      "epoch 25, batch 60, train_loss 0.692\n",
      "epoch 25, batch 70, train_loss 0.698\n",
      "epoch 25, batch 80, train_loss 0.695\n",
      "epoch 25, batch 90, train_loss 0.699\n",
      "epoch 25, batch 100, train_loss 0.689\n",
      "epoch 25, batch 110, train_loss 0.697\n",
      "epoch 25, batch 120, train_loss 0.697\n",
      "epoch 25, batch 130, train_loss 0.693\n",
      "epoch 25, batch 140, train_loss 0.697\n",
      "epoch 25, batch 150, train_loss 0.693\n",
      "epoch 25, batch 160, train_loss 0.690\n",
      "epoch 25, batch 170, train_loss 0.709\n",
      "epoch 25, batch 180, train_loss 0.693\n",
      "epoch 25, batch 190, train_loss 0.692\n",
      "epoch 25, batch 200, train_loss 0.689\n",
      "epoch 25, batch 210, train_loss 0.700\n",
      "epoch 25, batch 220, train_loss 0.701\n",
      "epoch 25, batch 230, train_loss 0.692\n",
      "epoch 25, batch 240, train_loss 0.696\n",
      "epoch 25, batch 250, train_loss 0.694\n",
      "epoch 25, batch 260, train_loss 0.697\n",
      "epoch 25, batch 270, train_loss 0.690\n",
      "epoch 25, batch 280, train_loss 0.695\n",
      "epoch 25, batch 290, train_loss 0.691\n",
      "epoch 25, batch 300, train_loss 0.695\n",
      "epoch 25, batch 310, train_loss 0.702\n",
      "epoch 25, batch 320, train_loss 0.697\n",
      "epoch 25, batch 330, train_loss 0.687\n",
      "epoch 25, batch 340, train_loss 0.700\n",
      "epoch 25, batch 350, train_loss 0.695\n",
      "epoch 25, batch 360, train_loss 0.702\n",
      "epoch 25, batch 370, train_loss 0.688\n",
      "epoch 25, batch 380, train_loss 0.699\n",
      "epoch 25, batch 390, train_loss 0.697\n",
      "epoch 25, batch 400, train_loss 0.694\n",
      "epoch 25, batch 410, train_loss 0.697\n",
      "epoch 25, batch 420, train_loss 0.691\n",
      "epoch 25, batch 430, train_loss 0.681\n",
      "epoch 25, batch 440, train_loss 0.696\n",
      "epoch 25, batch 450, train_loss 0.699\n",
      "epoch 25, batch 460, train_loss 0.700\n",
      "epoch 25, batch 470, train_loss 0.692\n",
      "epoch 25, batch 480, train_loss 0.696\n",
      "epoch 25, batch 490, train_loss 0.698\n",
      "epoch 25, batch 500, train_loss 0.700\n",
      "epoch 25, batch 510, train_loss 0.694\n",
      "epoch 25, batch 520, train_loss 0.702\n",
      "epoch 25, batch 530, train_loss 0.698\n",
      "epoch 25, batch 540, train_loss 0.701\n",
      "epoch 25, batch 550, train_loss 0.698\n",
      "epoch 25, batch 560, train_loss 0.705\n",
      "epoch 25, batch 570, train_loss 0.706\n",
      "epoch 25, batch 580, train_loss 0.690\n",
      "epoch 25, batch 590, train_loss 0.693\n",
      "epoch 25, batch 600, train_loss 0.706\n",
      "epoch 25, batch 610, train_loss 0.700\n",
      "epoch 25, batch 620, train_loss 0.689\n",
      "epoch 25, batch 630, train_loss 0.693\n",
      "epoch 25, batch 640, train_loss 0.696\n",
      "epoch 25, batch 650, train_loss 0.697\n",
      "epoch 25, batch 660, train_loss 0.691\n",
      "epoch 25, batch 670, train_loss 0.698\n",
      "epoch 25, batch 680, train_loss 0.696\n",
      "epoch 25, batch 690, train_loss 0.699\n",
      "epoch 25, batch 700, train_loss 0.698\n",
      "epoch 25, batch 710, train_loss 0.695\n",
      "epoch 25, batch 720, train_loss 0.686\n",
      "epoch 25, batch 730, train_loss 0.692\n",
      "epoch 25, batch 740, train_loss 0.708\n",
      "epoch 25, batch 750, train_loss 0.693\n",
      "epoch 25, batch 760, train_loss 0.693\n",
      "epoch 25, batch 770, train_loss 0.698\n",
      "epoch 25, batch 780, train_loss 0.697\n",
      "epoch 25, batch 790, train_loss 0.693\n",
      "epoch 25, batch 800, train_loss 0.701\n",
      "epoch 25, batch 810, train_loss 0.695\n",
      "epoch 25, batch 820, train_loss 0.702\n",
      "epoch 25, batch 830, train_loss 0.698\n",
      "epoch 25, batch 840, train_loss 0.690\n",
      "epoch 25, batch 850, train_loss 0.703\n",
      "epoch 25, batch 860, train_loss 0.704\n",
      "epoch 25, batch 870, train_loss 0.686\n",
      "epoch 25, batch 880, train_loss 0.698\n",
      "epoch 25, batch 890, train_loss 0.702\n",
      "epoch 25, batch 900, train_loss 0.703\n",
      "epoch 25, batch 910, train_loss 0.700\n",
      "epoch 25, batch 920, train_loss 0.702\n",
      "epoch 25, batch 930, train_loss 0.693\n",
      "epoch 25, batch 940, train_loss 0.700\n",
      "epoch 25, batch 950, train_loss 0.699\n",
      "epoch 25, batch 960, train_loss 0.701\n",
      "epoch 25, batch 970, train_loss 0.704\n",
      "epoch 25, batch 980, train_loss 0.692\n",
      "epoch 25, batch 990, train_loss 0.696\n",
      "epoch 25, batch 1000, train_loss 0.701\n",
      "epoch 25, batch 1010, train_loss 0.696\n",
      "epoch 25, batch 1020, train_loss 0.700\n",
      "epoch 25, batch 1030, train_loss 0.692\n",
      "epoch 25, batch 1040, train_loss 0.700\n",
      "epoch 25, batch 1050, train_loss 0.696\n",
      "epoch 25, batch 1060, train_loss 0.685\n",
      "epoch 25, batch 1070, train_loss 0.705\n",
      "epoch 25, batch 1080, train_loss 0.695\n",
      "epoch 25, batch 1090, train_loss 0.698\n",
      "epoch 25, batch 1100, train_loss 0.693\n",
      "epoch 25, batch 1110, train_loss 0.700\n",
      "epoch 25, batch 1120, train_loss 0.688\n",
      "epoch 25, batch 1130, train_loss 0.696\n",
      "epoch 25, batch 1140, train_loss 0.698\n",
      "epoch 25, batch 1150, train_loss 0.692\n",
      "epoch 25, batch 1160, train_loss 0.693\n",
      "epoch 25, batch 1170, train_loss 0.694\n",
      "epoch 25, batch 1180, train_loss 0.699\n",
      "epoch 25, batch 1190, train_loss 0.699\n",
      "epoch    25, train_loss 0.695, valid_loss 0.722, train_accuracy  70.11%, valid_accuracy  68.62%\n",
      "epoch 26, batch 0, train_loss 0.699\n",
      "epoch 26, batch 10, train_loss 0.697\n",
      "epoch 26, batch 20, train_loss 0.698\n",
      "epoch 26, batch 30, train_loss 0.692\n",
      "epoch 26, batch 40, train_loss 0.694\n",
      "epoch 26, batch 50, train_loss 0.699\n",
      "epoch 26, batch 60, train_loss 0.697\n",
      "epoch 26, batch 70, train_loss 0.699\n",
      "epoch 26, batch 80, train_loss 0.690\n",
      "epoch 26, batch 90, train_loss 0.694\n",
      "epoch 26, batch 100, train_loss 0.699\n",
      "epoch 26, batch 110, train_loss 0.697\n",
      "epoch 26, batch 120, train_loss 0.689\n",
      "epoch 26, batch 130, train_loss 0.693\n",
      "epoch 26, batch 140, train_loss 0.694\n",
      "epoch 26, batch 150, train_loss 0.687\n",
      "epoch 26, batch 160, train_loss 0.700\n",
      "epoch 26, batch 170, train_loss 0.699\n",
      "epoch 26, batch 180, train_loss 0.692\n",
      "epoch 26, batch 190, train_loss 0.688\n",
      "epoch 26, batch 200, train_loss 0.695\n",
      "epoch 26, batch 210, train_loss 0.695\n",
      "epoch 26, batch 220, train_loss 0.701\n",
      "epoch 26, batch 230, train_loss 0.702\n",
      "epoch 26, batch 240, train_loss 0.691\n",
      "epoch 26, batch 250, train_loss 0.708\n",
      "epoch 26, batch 260, train_loss 0.699\n",
      "epoch 26, batch 270, train_loss 0.700\n",
      "epoch 26, batch 280, train_loss 0.711\n",
      "epoch 26, batch 290, train_loss 0.689\n",
      "epoch 26, batch 300, train_loss 0.696\n",
      "epoch 26, batch 310, train_loss 0.692\n",
      "epoch 26, batch 320, train_loss 0.697\n",
      "epoch 26, batch 330, train_loss 0.703\n",
      "epoch 26, batch 340, train_loss 0.697\n",
      "epoch 26, batch 350, train_loss 0.689\n",
      "epoch 26, batch 360, train_loss 0.696\n",
      "epoch 26, batch 370, train_loss 0.697\n",
      "epoch 26, batch 380, train_loss 0.691\n",
      "epoch 26, batch 390, train_loss 0.688\n",
      "epoch 26, batch 400, train_loss 0.694\n",
      "epoch 26, batch 410, train_loss 0.692\n",
      "epoch 26, batch 420, train_loss 0.700\n",
      "epoch 26, batch 430, train_loss 0.704\n",
      "epoch 26, batch 440, train_loss 0.696\n",
      "epoch 26, batch 450, train_loss 0.700\n",
      "epoch 26, batch 460, train_loss 0.702\n",
      "epoch 26, batch 470, train_loss 0.697\n",
      "epoch 26, batch 480, train_loss 0.698\n",
      "epoch 26, batch 490, train_loss 0.693\n",
      "epoch 26, batch 500, train_loss 0.682\n",
      "epoch 26, batch 510, train_loss 0.700\n",
      "epoch 26, batch 520, train_loss 0.698\n",
      "epoch 26, batch 530, train_loss 0.702\n",
      "epoch 26, batch 540, train_loss 0.692\n",
      "epoch 26, batch 550, train_loss 0.700\n",
      "epoch 26, batch 560, train_loss 0.694\n",
      "epoch 26, batch 570, train_loss 0.687\n",
      "epoch 26, batch 580, train_loss 0.703\n",
      "epoch 26, batch 590, train_loss 0.697\n",
      "epoch 26, batch 600, train_loss 0.702\n",
      "epoch 26, batch 610, train_loss 0.707\n",
      "epoch 26, batch 620, train_loss 0.694\n",
      "epoch 26, batch 630, train_loss 0.699\n",
      "epoch 26, batch 640, train_loss 0.694\n",
      "epoch 26, batch 650, train_loss 0.690\n",
      "epoch 26, batch 660, train_loss 0.695\n",
      "epoch 26, batch 670, train_loss 0.708\n",
      "epoch 26, batch 680, train_loss 0.700\n",
      "epoch 26, batch 690, train_loss 0.694\n",
      "epoch 26, batch 700, train_loss 0.698\n",
      "epoch 26, batch 710, train_loss 0.704\n",
      "epoch 26, batch 720, train_loss 0.703\n",
      "epoch 26, batch 730, train_loss 0.691\n",
      "epoch 26, batch 740, train_loss 0.693\n",
      "epoch 26, batch 750, train_loss 0.690\n",
      "epoch 26, batch 760, train_loss 0.692\n",
      "epoch 26, batch 770, train_loss 0.702\n",
      "epoch 26, batch 780, train_loss 0.701\n",
      "epoch 26, batch 790, train_loss 0.696\n",
      "epoch 26, batch 800, train_loss 0.694\n",
      "epoch 26, batch 810, train_loss 0.690\n",
      "epoch 26, batch 820, train_loss 0.689\n",
      "epoch 26, batch 830, train_loss 0.687\n",
      "epoch 26, batch 840, train_loss 0.703\n",
      "epoch 26, batch 850, train_loss 0.704\n",
      "epoch 26, batch 860, train_loss 0.686\n",
      "epoch 26, batch 870, train_loss 0.691\n",
      "epoch 26, batch 880, train_loss 0.691\n",
      "epoch 26, batch 890, train_loss 0.692\n",
      "epoch 26, batch 900, train_loss 0.702\n",
      "epoch 26, batch 910, train_loss 0.703\n",
      "epoch 26, batch 920, train_loss 0.696\n",
      "epoch 26, batch 930, train_loss 0.699\n",
      "epoch 26, batch 940, train_loss 0.690\n",
      "epoch 26, batch 950, train_loss 0.691\n",
      "epoch 26, batch 960, train_loss 0.692\n",
      "epoch 26, batch 970, train_loss 0.689\n",
      "epoch 26, batch 980, train_loss 0.688\n",
      "epoch 26, batch 990, train_loss 0.685\n",
      "epoch 26, batch 1000, train_loss 0.693\n",
      "epoch 26, batch 1010, train_loss 0.680\n",
      "epoch 26, batch 1020, train_loss 0.696\n",
      "epoch 26, batch 1030, train_loss 0.698\n",
      "epoch 26, batch 1040, train_loss 0.697\n",
      "epoch 26, batch 1050, train_loss 0.691\n",
      "epoch 26, batch 1060, train_loss 0.689\n",
      "epoch 26, batch 1070, train_loss 0.694\n",
      "epoch 26, batch 1080, train_loss 0.700\n",
      "epoch 26, batch 1090, train_loss 0.695\n",
      "epoch 26, batch 1100, train_loss 0.692\n",
      "epoch 26, batch 1110, train_loss 0.692\n",
      "epoch 26, batch 1120, train_loss 0.698\n",
      "epoch 26, batch 1130, train_loss 0.704\n",
      "epoch 26, batch 1140, train_loss 0.696\n",
      "epoch 26, batch 1150, train_loss 0.685\n",
      "epoch 26, batch 1160, train_loss 0.692\n",
      "epoch 26, batch 1170, train_loss 0.705\n",
      "epoch 26, batch 1180, train_loss 0.692\n",
      "epoch 26, batch 1190, train_loss 0.691\n",
      "epoch    26, train_loss 0.695, valid_loss 0.722, train_accuracy  70.09%, valid_accuracy  68.60%\n",
      "epoch 27, batch 0, train_loss 0.694\n",
      "epoch 27, batch 10, train_loss 0.700\n",
      "epoch 27, batch 20, train_loss 0.695\n",
      "epoch 27, batch 30, train_loss 0.693\n",
      "epoch 27, batch 40, train_loss 0.692\n",
      "epoch 27, batch 50, train_loss 0.697\n",
      "epoch 27, batch 60, train_loss 0.702\n",
      "epoch 27, batch 70, train_loss 0.694\n",
      "epoch 27, batch 80, train_loss 0.695\n",
      "epoch 27, batch 90, train_loss 0.697\n",
      "epoch 27, batch 100, train_loss 0.687\n",
      "epoch 27, batch 110, train_loss 0.695\n",
      "epoch 27, batch 120, train_loss 0.693\n",
      "epoch 27, batch 130, train_loss 0.700\n",
      "epoch 27, batch 140, train_loss 0.700\n",
      "epoch 27, batch 150, train_loss 0.690\n",
      "epoch 27, batch 160, train_loss 0.680\n",
      "epoch 27, batch 170, train_loss 0.703\n",
      "epoch 27, batch 180, train_loss 0.693\n",
      "epoch 27, batch 190, train_loss 0.692\n",
      "epoch 27, batch 200, train_loss 0.695\n",
      "epoch 27, batch 210, train_loss 0.704\n",
      "epoch 27, batch 220, train_loss 0.684\n",
      "epoch 27, batch 230, train_loss 0.702\n",
      "epoch 27, batch 240, train_loss 0.698\n",
      "epoch 27, batch 250, train_loss 0.698\n",
      "epoch 27, batch 260, train_loss 0.698\n",
      "epoch 27, batch 270, train_loss 0.702\n",
      "epoch 27, batch 280, train_loss 0.697\n",
      "epoch 27, batch 290, train_loss 0.688\n",
      "epoch 27, batch 300, train_loss 0.700\n",
      "epoch 27, batch 310, train_loss 0.708\n",
      "epoch 27, batch 320, train_loss 0.707\n",
      "epoch 27, batch 330, train_loss 0.693\n",
      "epoch 27, batch 340, train_loss 0.700\n",
      "epoch 27, batch 350, train_loss 0.693\n",
      "epoch 27, batch 360, train_loss 0.700\n",
      "epoch 27, batch 370, train_loss 0.694\n",
      "epoch 27, batch 380, train_loss 0.701\n",
      "epoch 27, batch 390, train_loss 0.701\n",
      "epoch 27, batch 400, train_loss 0.692\n",
      "epoch 27, batch 410, train_loss 0.690\n",
      "epoch 27, batch 420, train_loss 0.690\n",
      "epoch 27, batch 430, train_loss 0.709\n",
      "epoch 27, batch 440, train_loss 0.695\n",
      "epoch 27, batch 450, train_loss 0.681\n",
      "epoch 27, batch 460, train_loss 0.695\n",
      "epoch 27, batch 470, train_loss 0.692\n",
      "epoch 27, batch 480, train_loss 0.696\n",
      "epoch 27, batch 490, train_loss 0.685\n",
      "epoch 27, batch 500, train_loss 0.693\n",
      "epoch 27, batch 510, train_loss 0.704\n",
      "epoch 27, batch 520, train_loss 0.696\n",
      "epoch 27, batch 530, train_loss 0.691\n",
      "epoch 27, batch 540, train_loss 0.697\n",
      "epoch 27, batch 550, train_loss 0.694\n",
      "epoch 27, batch 560, train_loss 0.695\n",
      "epoch 27, batch 570, train_loss 0.705\n",
      "epoch 27, batch 580, train_loss 0.695\n",
      "epoch 27, batch 590, train_loss 0.695\n",
      "epoch 27, batch 600, train_loss 0.693\n",
      "epoch 27, batch 610, train_loss 0.697\n",
      "epoch 27, batch 620, train_loss 0.694\n",
      "epoch 27, batch 630, train_loss 0.703\n",
      "epoch 27, batch 640, train_loss 0.694\n",
      "epoch 27, batch 650, train_loss 0.696\n",
      "epoch 27, batch 660, train_loss 0.701\n",
      "epoch 27, batch 670, train_loss 0.698\n",
      "epoch 27, batch 680, train_loss 0.705\n",
      "epoch 27, batch 690, train_loss 0.692\n",
      "epoch 27, batch 700, train_loss 0.689\n",
      "epoch 27, batch 710, train_loss 0.697\n",
      "epoch 27, batch 720, train_loss 0.692\n",
      "epoch 27, batch 730, train_loss 0.697\n",
      "epoch 27, batch 740, train_loss 0.687\n",
      "epoch 27, batch 750, train_loss 0.687\n",
      "epoch 27, batch 760, train_loss 0.693\n",
      "epoch 27, batch 770, train_loss 0.701\n",
      "epoch 27, batch 780, train_loss 0.695\n",
      "epoch 27, batch 790, train_loss 0.701\n",
      "epoch 27, batch 800, train_loss 0.695\n",
      "epoch 27, batch 810, train_loss 0.696\n",
      "epoch 27, batch 820, train_loss 0.696\n",
      "epoch 27, batch 830, train_loss 0.698\n",
      "epoch 27, batch 840, train_loss 0.699\n",
      "epoch 27, batch 850, train_loss 0.698\n",
      "epoch 27, batch 860, train_loss 0.701\n",
      "epoch 27, batch 870, train_loss 0.696\n",
      "epoch 27, batch 880, train_loss 0.697\n",
      "epoch 27, batch 890, train_loss 0.695\n",
      "epoch 27, batch 900, train_loss 0.697\n",
      "epoch 27, batch 910, train_loss 0.701\n",
      "epoch 27, batch 920, train_loss 0.695\n",
      "epoch 27, batch 930, train_loss 0.693\n",
      "epoch 27, batch 940, train_loss 0.697\n",
      "epoch 27, batch 950, train_loss 0.700\n",
      "epoch 27, batch 960, train_loss 0.700\n",
      "epoch 27, batch 970, train_loss 0.701\n",
      "epoch 27, batch 980, train_loss 0.698\n",
      "epoch 27, batch 990, train_loss 0.700\n",
      "epoch 27, batch 1000, train_loss 0.699\n",
      "epoch 27, batch 1010, train_loss 0.692\n",
      "epoch 27, batch 1020, train_loss 0.699\n",
      "epoch 27, batch 1030, train_loss 0.693\n",
      "epoch 27, batch 1040, train_loss 0.698\n",
      "epoch 27, batch 1050, train_loss 0.688\n",
      "epoch 27, batch 1060, train_loss 0.697\n",
      "epoch 27, batch 1070, train_loss 0.691\n",
      "epoch 27, batch 1080, train_loss 0.693\n",
      "epoch 27, batch 1090, train_loss 0.697\n",
      "epoch 27, batch 1100, train_loss 0.699\n",
      "epoch 27, batch 1110, train_loss 0.685\n",
      "epoch 27, batch 1120, train_loss 0.695\n",
      "epoch 27, batch 1130, train_loss 0.698\n",
      "epoch 27, batch 1140, train_loss 0.692\n",
      "epoch 27, batch 1150, train_loss 0.694\n",
      "epoch 27, batch 1160, train_loss 0.700\n",
      "epoch 27, batch 1170, train_loss 0.696\n",
      "epoch 27, batch 1180, train_loss 0.698\n",
      "epoch 27, batch 1190, train_loss 0.690\n",
      "epoch    27, train_loss 0.695, valid_loss 0.722, train_accuracy  70.14%, valid_accuracy  68.61%\n",
      "epoch 28, batch 0, train_loss 0.697\n",
      "epoch 28, batch 10, train_loss 0.693\n",
      "epoch 28, batch 20, train_loss 0.692\n",
      "epoch 28, batch 30, train_loss 0.698\n",
      "epoch 28, batch 40, train_loss 0.706\n",
      "epoch 28, batch 50, train_loss 0.698\n",
      "epoch 28, batch 60, train_loss 0.700\n",
      "epoch 28, batch 70, train_loss 0.701\n",
      "epoch 28, batch 80, train_loss 0.702\n",
      "epoch 28, batch 90, train_loss 0.694\n",
      "epoch 28, batch 100, train_loss 0.702\n",
      "epoch 28, batch 110, train_loss 0.697\n",
      "epoch 28, batch 120, train_loss 0.689\n",
      "epoch 28, batch 130, train_loss 0.694\n",
      "epoch 28, batch 140, train_loss 0.682\n",
      "epoch 28, batch 150, train_loss 0.687\n",
      "epoch 28, batch 160, train_loss 0.700\n",
      "epoch 28, batch 170, train_loss 0.697\n",
      "epoch 28, batch 180, train_loss 0.695\n",
      "epoch 28, batch 190, train_loss 0.694\n",
      "epoch 28, batch 200, train_loss 0.701\n",
      "epoch 28, batch 210, train_loss 0.704\n",
      "epoch 28, batch 220, train_loss 0.696\n",
      "epoch 28, batch 230, train_loss 0.701\n",
      "epoch 28, batch 240, train_loss 0.701\n",
      "epoch 28, batch 250, train_loss 0.692\n",
      "epoch 28, batch 260, train_loss 0.701\n",
      "epoch 28, batch 270, train_loss 0.699\n",
      "epoch 28, batch 280, train_loss 0.688\n",
      "epoch 28, batch 290, train_loss 0.689\n",
      "epoch 28, batch 300, train_loss 0.696\n",
      "epoch 28, batch 310, train_loss 0.691\n",
      "epoch 28, batch 320, train_loss 0.695\n",
      "epoch 28, batch 330, train_loss 0.695\n",
      "epoch 28, batch 340, train_loss 0.700\n",
      "epoch 28, batch 350, train_loss 0.694\n",
      "epoch 28, batch 360, train_loss 0.695\n",
      "epoch 28, batch 370, train_loss 0.696\n",
      "epoch 28, batch 380, train_loss 0.700\n",
      "epoch 28, batch 390, train_loss 0.690\n",
      "epoch 28, batch 400, train_loss 0.700\n",
      "epoch 28, batch 410, train_loss 0.701\n",
      "epoch 28, batch 420, train_loss 0.690\n",
      "epoch 28, batch 430, train_loss 0.695\n",
      "epoch 28, batch 440, train_loss 0.694\n",
      "epoch 28, batch 450, train_loss 0.699\n",
      "epoch 28, batch 460, train_loss 0.701\n",
      "epoch 28, batch 470, train_loss 0.704\n",
      "epoch 28, batch 480, train_loss 0.695\n",
      "epoch 28, batch 490, train_loss 0.703\n",
      "epoch 28, batch 500, train_loss 0.697\n",
      "epoch 28, batch 510, train_loss 0.697\n",
      "epoch 28, batch 520, train_loss 0.696\n",
      "epoch 28, batch 530, train_loss 0.704\n",
      "epoch 28, batch 540, train_loss 0.699\n",
      "epoch 28, batch 550, train_loss 0.690\n",
      "epoch 28, batch 560, train_loss 0.701\n",
      "epoch 28, batch 570, train_loss 0.700\n",
      "epoch 28, batch 580, train_loss 0.683\n",
      "epoch 28, batch 590, train_loss 0.703\n",
      "epoch 28, batch 600, train_loss 0.696\n",
      "epoch 28, batch 610, train_loss 0.688\n",
      "epoch 28, batch 620, train_loss 0.698\n",
      "epoch 28, batch 630, train_loss 0.695\n",
      "epoch 28, batch 640, train_loss 0.701\n",
      "epoch 28, batch 650, train_loss 0.691\n",
      "epoch 28, batch 660, train_loss 0.686\n",
      "epoch 28, batch 670, train_loss 0.694\n",
      "epoch 28, batch 680, train_loss 0.694\n",
      "epoch 28, batch 690, train_loss 0.686\n",
      "epoch 28, batch 700, train_loss 0.690\n",
      "epoch 28, batch 710, train_loss 0.698\n",
      "epoch 28, batch 720, train_loss 0.694\n",
      "epoch 28, batch 730, train_loss 0.690\n",
      "epoch 28, batch 740, train_loss 0.695\n",
      "epoch 28, batch 750, train_loss 0.698\n",
      "epoch 28, batch 760, train_loss 0.688\n",
      "epoch 28, batch 770, train_loss 0.697\n",
      "epoch 28, batch 780, train_loss 0.700\n",
      "epoch 28, batch 790, train_loss 0.705\n",
      "epoch 28, batch 800, train_loss 0.698\n",
      "epoch 28, batch 810, train_loss 0.701\n",
      "epoch 28, batch 820, train_loss 0.702\n",
      "epoch 28, batch 830, train_loss 0.698\n",
      "epoch 28, batch 840, train_loss 0.700\n",
      "epoch 28, batch 850, train_loss 0.694\n",
      "epoch 28, batch 860, train_loss 0.692\n",
      "epoch 28, batch 870, train_loss 0.689\n",
      "epoch 28, batch 880, train_loss 0.693\n",
      "epoch 28, batch 890, train_loss 0.700\n",
      "epoch 28, batch 900, train_loss 0.688\n",
      "epoch 28, batch 910, train_loss 0.698\n",
      "epoch 28, batch 920, train_loss 0.705\n",
      "epoch 28, batch 930, train_loss 0.695\n",
      "epoch 28, batch 940, train_loss 0.700\n",
      "epoch 28, batch 950, train_loss 0.707\n",
      "epoch 28, batch 960, train_loss 0.696\n",
      "epoch 28, batch 970, train_loss 0.700\n",
      "epoch 28, batch 980, train_loss 0.709\n",
      "epoch 28, batch 990, train_loss 0.696\n",
      "epoch 28, batch 1000, train_loss 0.695\n",
      "epoch 28, batch 1010, train_loss 0.691\n",
      "epoch 28, batch 1020, train_loss 0.688\n",
      "epoch 28, batch 1030, train_loss 0.701\n",
      "epoch 28, batch 1040, train_loss 0.693\n",
      "epoch 28, batch 1050, train_loss 0.693\n",
      "epoch 28, batch 1060, train_loss 0.690\n",
      "epoch 28, batch 1070, train_loss 0.698\n",
      "epoch 28, batch 1080, train_loss 0.701\n",
      "epoch 28, batch 1090, train_loss 0.706\n",
      "epoch 28, batch 1100, train_loss 0.698\n",
      "epoch 28, batch 1110, train_loss 0.691\n",
      "epoch 28, batch 1120, train_loss 0.685\n",
      "epoch 28, batch 1130, train_loss 0.693\n",
      "epoch 28, batch 1140, train_loss 0.688\n",
      "epoch 28, batch 1150, train_loss 0.695\n",
      "epoch 28, batch 1160, train_loss 0.697\n",
      "epoch 28, batch 1170, train_loss 0.697\n",
      "epoch 28, batch 1180, train_loss 0.696\n",
      "epoch 28, batch 1190, train_loss 0.698\n",
      "epoch    28, train_loss 0.694, valid_loss 0.722, train_accuracy  70.16%, valid_accuracy  68.63%\n",
      "epoch 29, batch 0, train_loss 0.700\n",
      "epoch 29, batch 10, train_loss 0.692\n",
      "epoch 29, batch 20, train_loss 0.697\n",
      "epoch 29, batch 30, train_loss 0.694\n",
      "epoch 29, batch 40, train_loss 0.701\n",
      "epoch 29, batch 50, train_loss 0.698\n",
      "epoch 29, batch 60, train_loss 0.689\n",
      "epoch 29, batch 70, train_loss 0.702\n",
      "epoch 29, batch 80, train_loss 0.696\n",
      "epoch 29, batch 90, train_loss 0.688\n",
      "epoch 29, batch 100, train_loss 0.708\n",
      "epoch 29, batch 110, train_loss 0.700\n",
      "epoch 29, batch 120, train_loss 0.686\n",
      "epoch 29, batch 130, train_loss 0.697\n",
      "epoch 29, batch 140, train_loss 0.695\n",
      "epoch 29, batch 150, train_loss 0.689\n",
      "epoch 29, batch 160, train_loss 0.700\n",
      "epoch 29, batch 170, train_loss 0.699\n",
      "epoch 29, batch 180, train_loss 0.700\n",
      "epoch 29, batch 190, train_loss 0.697\n",
      "epoch 29, batch 200, train_loss 0.694\n",
      "epoch 29, batch 210, train_loss 0.701\n",
      "epoch 29, batch 220, train_loss 0.691\n",
      "epoch 29, batch 230, train_loss 0.699\n",
      "epoch 29, batch 240, train_loss 0.698\n",
      "epoch 29, batch 250, train_loss 0.701\n",
      "epoch 29, batch 260, train_loss 0.699\n",
      "epoch 29, batch 270, train_loss 0.699\n",
      "epoch 29, batch 280, train_loss 0.685\n",
      "epoch 29, batch 290, train_loss 0.697\n",
      "epoch 29, batch 300, train_loss 0.695\n",
      "epoch 29, batch 310, train_loss 0.698\n",
      "epoch 29, batch 320, train_loss 0.687\n",
      "epoch 29, batch 330, train_loss 0.693\n",
      "epoch 29, batch 340, train_loss 0.694\n",
      "epoch 29, batch 350, train_loss 0.692\n",
      "epoch 29, batch 360, train_loss 0.701\n",
      "epoch 29, batch 370, train_loss 0.698\n",
      "epoch 29, batch 380, train_loss 0.703\n",
      "epoch 29, batch 390, train_loss 0.699\n",
      "epoch 29, batch 400, train_loss 0.691\n",
      "epoch 29, batch 410, train_loss 0.701\n",
      "epoch 29, batch 420, train_loss 0.695\n",
      "epoch 29, batch 430, train_loss 0.701\n",
      "epoch 29, batch 440, train_loss 0.697\n",
      "epoch 29, batch 450, train_loss 0.692\n",
      "epoch 29, batch 460, train_loss 0.693\n",
      "epoch 29, batch 470, train_loss 0.697\n",
      "epoch 29, batch 480, train_loss 0.698\n",
      "epoch 29, batch 490, train_loss 0.688\n",
      "epoch 29, batch 500, train_loss 0.700\n",
      "epoch 29, batch 510, train_loss 0.700\n",
      "epoch 29, batch 520, train_loss 0.693\n",
      "epoch 29, batch 530, train_loss 0.700\n",
      "epoch 29, batch 540, train_loss 0.705\n",
      "epoch 29, batch 550, train_loss 0.687\n",
      "epoch 29, batch 560, train_loss 0.691\n",
      "epoch 29, batch 570, train_loss 0.690\n",
      "epoch 29, batch 580, train_loss 0.704\n",
      "epoch 29, batch 590, train_loss 0.704\n",
      "epoch 29, batch 600, train_loss 0.674\n",
      "epoch 29, batch 610, train_loss 0.692\n",
      "epoch 29, batch 620, train_loss 0.689\n",
      "epoch 29, batch 630, train_loss 0.699\n",
      "epoch 29, batch 640, train_loss 0.701\n",
      "epoch 29, batch 650, train_loss 0.702\n",
      "epoch 29, batch 660, train_loss 0.707\n",
      "epoch 29, batch 670, train_loss 0.698\n",
      "epoch 29, batch 680, train_loss 0.684\n",
      "epoch 29, batch 690, train_loss 0.690\n",
      "epoch 29, batch 700, train_loss 0.695\n",
      "epoch 29, batch 710, train_loss 0.698\n",
      "epoch 29, batch 720, train_loss 0.688\n",
      "epoch 29, batch 730, train_loss 0.681\n",
      "epoch 29, batch 740, train_loss 0.697\n",
      "epoch 29, batch 750, train_loss 0.692\n",
      "epoch 29, batch 760, train_loss 0.693\n",
      "epoch 29, batch 770, train_loss 0.696\n",
      "epoch 29, batch 780, train_loss 0.692\n",
      "epoch 29, batch 790, train_loss 0.704\n",
      "epoch 29, batch 800, train_loss 0.693\n",
      "epoch 29, batch 810, train_loss 0.699\n",
      "epoch 29, batch 820, train_loss 0.713\n",
      "epoch 29, batch 830, train_loss 0.699\n",
      "epoch 29, batch 840, train_loss 0.693\n",
      "epoch 29, batch 850, train_loss 0.682\n",
      "epoch 29, batch 860, train_loss 0.695\n",
      "epoch 29, batch 870, train_loss 0.704\n",
      "epoch 29, batch 880, train_loss 0.691\n",
      "epoch 29, batch 890, train_loss 0.692\n",
      "epoch 29, batch 900, train_loss 0.693\n",
      "epoch 29, batch 910, train_loss 0.701\n",
      "epoch 29, batch 920, train_loss 0.693\n",
      "epoch 29, batch 930, train_loss 0.692\n",
      "epoch 29, batch 940, train_loss 0.698\n",
      "epoch 29, batch 950, train_loss 0.694\n",
      "epoch 29, batch 960, train_loss 0.697\n",
      "epoch 29, batch 970, train_loss 0.695\n",
      "epoch 29, batch 980, train_loss 0.700\n",
      "epoch 29, batch 990, train_loss 0.698\n",
      "epoch 29, batch 1000, train_loss 0.694\n",
      "epoch 29, batch 1010, train_loss 0.694\n",
      "epoch 29, batch 1020, train_loss 0.698\n",
      "epoch 29, batch 1030, train_loss 0.689\n",
      "epoch 29, batch 1040, train_loss 0.707\n",
      "epoch 29, batch 1050, train_loss 0.699\n",
      "epoch 29, batch 1060, train_loss 0.697\n",
      "epoch 29, batch 1070, train_loss 0.698\n",
      "epoch 29, batch 1080, train_loss 0.689\n",
      "epoch 29, batch 1090, train_loss 0.691\n",
      "epoch 29, batch 1100, train_loss 0.694\n",
      "epoch 29, batch 1110, train_loss 0.697\n",
      "epoch 29, batch 1120, train_loss 0.695\n",
      "epoch 29, batch 1130, train_loss 0.701\n",
      "epoch 29, batch 1140, train_loss 0.695\n",
      "epoch 29, batch 1150, train_loss 0.691\n",
      "epoch 29, batch 1160, train_loss 0.701\n",
      "epoch 29, batch 1170, train_loss 0.684\n",
      "epoch 29, batch 1180, train_loss 0.705\n",
      "epoch 29, batch 1190, train_loss 0.704\n",
      "epoch    29, train_loss 0.694, valid_loss 0.722, train_accuracy  70.16%, valid_accuracy  68.57%\n",
      "epoch 30, batch 0, train_loss 0.704\n",
      "epoch 30, batch 10, train_loss 0.690\n",
      "epoch 30, batch 20, train_loss 0.686\n",
      "epoch 30, batch 30, train_loss 0.697\n",
      "epoch 30, batch 40, train_loss 0.700\n",
      "epoch 30, batch 50, train_loss 0.694\n",
      "epoch 30, batch 60, train_loss 0.694\n",
      "epoch 30, batch 70, train_loss 0.697\n",
      "epoch 30, batch 80, train_loss 0.696\n",
      "epoch 30, batch 90, train_loss 0.690\n",
      "epoch 30, batch 100, train_loss 0.692\n",
      "epoch 30, batch 110, train_loss 0.697\n",
      "epoch 30, batch 120, train_loss 0.685\n",
      "epoch 30, batch 130, train_loss 0.706\n",
      "epoch 30, batch 140, train_loss 0.692\n",
      "epoch 30, batch 150, train_loss 0.702\n",
      "epoch 30, batch 160, train_loss 0.695\n",
      "epoch 30, batch 170, train_loss 0.695\n",
      "epoch 30, batch 180, train_loss 0.699\n",
      "epoch 30, batch 190, train_loss 0.692\n",
      "epoch 30, batch 200, train_loss 0.695\n",
      "epoch 30, batch 210, train_loss 0.694\n",
      "epoch 30, batch 220, train_loss 0.682\n",
      "epoch 30, batch 230, train_loss 0.702\n",
      "epoch 30, batch 240, train_loss 0.693\n",
      "epoch 30, batch 250, train_loss 0.688\n",
      "epoch 30, batch 260, train_loss 0.687\n",
      "epoch 30, batch 270, train_loss 0.695\n",
      "epoch 30, batch 280, train_loss 0.697\n",
      "epoch 30, batch 290, train_loss 0.689\n",
      "epoch 30, batch 300, train_loss 0.697\n",
      "epoch 30, batch 310, train_loss 0.690\n",
      "epoch 30, batch 320, train_loss 0.691\n",
      "epoch 30, batch 330, train_loss 0.693\n",
      "epoch 30, batch 340, train_loss 0.706\n",
      "epoch 30, batch 350, train_loss 0.700\n",
      "epoch 30, batch 360, train_loss 0.696\n",
      "epoch 30, batch 370, train_loss 0.698\n",
      "epoch 30, batch 380, train_loss 0.694\n",
      "epoch 30, batch 390, train_loss 0.695\n",
      "epoch 30, batch 400, train_loss 0.694\n",
      "epoch 30, batch 410, train_loss 0.700\n",
      "epoch 30, batch 420, train_loss 0.697\n",
      "epoch 30, batch 430, train_loss 0.692\n",
      "epoch 30, batch 440, train_loss 0.695\n",
      "epoch 30, batch 450, train_loss 0.694\n",
      "epoch 30, batch 460, train_loss 0.694\n",
      "epoch 30, batch 470, train_loss 0.696\n",
      "epoch 30, batch 480, train_loss 0.693\n",
      "epoch 30, batch 490, train_loss 0.696\n",
      "epoch 30, batch 500, train_loss 0.695\n",
      "epoch 30, batch 510, train_loss 0.706\n",
      "epoch 30, batch 520, train_loss 0.696\n",
      "epoch 30, batch 530, train_loss 0.688\n",
      "epoch 30, batch 540, train_loss 0.695\n",
      "epoch 30, batch 550, train_loss 0.703\n",
      "epoch 30, batch 560, train_loss 0.697\n",
      "epoch 30, batch 570, train_loss 0.699\n",
      "epoch 30, batch 580, train_loss 0.692\n",
      "epoch 30, batch 590, train_loss 0.693\n",
      "epoch 30, batch 600, train_loss 0.687\n",
      "epoch 30, batch 610, train_loss 0.698\n",
      "epoch 30, batch 620, train_loss 0.704\n",
      "epoch 30, batch 630, train_loss 0.695\n",
      "epoch 30, batch 640, train_loss 0.695\n",
      "epoch 30, batch 650, train_loss 0.689\n",
      "epoch 30, batch 660, train_loss 0.682\n",
      "epoch 30, batch 670, train_loss 0.692\n",
      "epoch 30, batch 680, train_loss 0.700\n",
      "epoch 30, batch 690, train_loss 0.698\n",
      "epoch 30, batch 700, train_loss 0.699\n",
      "epoch 30, batch 710, train_loss 0.696\n",
      "epoch 30, batch 720, train_loss 0.695\n",
      "epoch 30, batch 730, train_loss 0.689\n",
      "epoch 30, batch 740, train_loss 0.698\n",
      "epoch 30, batch 750, train_loss 0.696\n",
      "epoch 30, batch 760, train_loss 0.699\n",
      "epoch 30, batch 770, train_loss 0.691\n",
      "epoch 30, batch 780, train_loss 0.695\n",
      "epoch 30, batch 790, train_loss 0.689\n",
      "epoch 30, batch 800, train_loss 0.695\n",
      "epoch 30, batch 810, train_loss 0.695\n",
      "epoch 30, batch 820, train_loss 0.699\n",
      "epoch 30, batch 830, train_loss 0.697\n",
      "epoch 30, batch 840, train_loss 0.701\n",
      "epoch 30, batch 850, train_loss 0.702\n",
      "epoch 30, batch 860, train_loss 0.691\n",
      "epoch 30, batch 870, train_loss 0.688\n",
      "epoch 30, batch 880, train_loss 0.690\n",
      "epoch 30, batch 890, train_loss 0.702\n",
      "epoch 30, batch 900, train_loss 0.692\n",
      "epoch 30, batch 910, train_loss 0.708\n",
      "epoch 30, batch 920, train_loss 0.699\n",
      "epoch 30, batch 930, train_loss 0.695\n",
      "epoch 30, batch 940, train_loss 0.700\n",
      "epoch 30, batch 950, train_loss 0.706\n",
      "epoch 30, batch 960, train_loss 0.694\n",
      "epoch 30, batch 970, train_loss 0.700\n",
      "epoch 30, batch 980, train_loss 0.699\n",
      "epoch 30, batch 990, train_loss 0.682\n",
      "epoch 30, batch 1000, train_loss 0.699\n",
      "epoch 30, batch 1010, train_loss 0.689\n",
      "epoch 30, batch 1020, train_loss 0.690\n",
      "epoch 30, batch 1030, train_loss 0.682\n",
      "epoch 30, batch 1040, train_loss 0.691\n",
      "epoch 30, batch 1050, train_loss 0.685\n",
      "epoch 30, batch 1060, train_loss 0.700\n",
      "epoch 30, batch 1070, train_loss 0.699\n",
      "epoch 30, batch 1080, train_loss 0.696\n",
      "epoch 30, batch 1090, train_loss 0.687\n",
      "epoch 30, batch 1100, train_loss 0.685\n",
      "epoch 30, batch 1110, train_loss 0.690\n",
      "epoch 30, batch 1120, train_loss 0.700\n",
      "epoch 30, batch 1130, train_loss 0.691\n",
      "epoch 30, batch 1140, train_loss 0.681\n",
      "epoch 30, batch 1150, train_loss 0.685\n",
      "epoch 30, batch 1160, train_loss 0.695\n",
      "epoch 30, batch 1170, train_loss 0.689\n",
      "epoch 30, batch 1180, train_loss 0.698\n",
      "epoch 30, batch 1190, train_loss 0.697\n",
      "epoch    30, train_loss 0.694, valid_loss 0.722, train_accuracy  70.16%, valid_accuracy  68.65%\n",
      "epoch 31, batch 0, train_loss 0.690\n",
      "epoch 31, batch 10, train_loss 0.691\n",
      "epoch 31, batch 20, train_loss 0.702\n",
      "epoch 31, batch 30, train_loss 0.692\n",
      "epoch 31, batch 40, train_loss 0.695\n",
      "epoch 31, batch 50, train_loss 0.701\n",
      "epoch 31, batch 60, train_loss 0.689\n",
      "epoch 31, batch 70, train_loss 0.700\n",
      "epoch 31, batch 80, train_loss 0.686\n",
      "epoch 31, batch 90, train_loss 0.705\n",
      "epoch 31, batch 100, train_loss 0.692\n",
      "epoch 31, batch 110, train_loss 0.700\n",
      "epoch 31, batch 120, train_loss 0.700\n",
      "epoch 31, batch 130, train_loss 0.694\n",
      "epoch 31, batch 140, train_loss 0.698\n",
      "epoch 31, batch 150, train_loss 0.700\n",
      "epoch 31, batch 160, train_loss 0.696\n",
      "epoch 31, batch 170, train_loss 0.690\n",
      "epoch 31, batch 180, train_loss 0.692\n",
      "epoch 31, batch 190, train_loss 0.696\n",
      "epoch 31, batch 200, train_loss 0.698\n",
      "epoch 31, batch 210, train_loss 0.687\n",
      "epoch 31, batch 220, train_loss 0.693\n",
      "epoch 31, batch 230, train_loss 0.699\n",
      "epoch 31, batch 240, train_loss 0.699\n",
      "epoch 31, batch 250, train_loss 0.693\n",
      "epoch 31, batch 260, train_loss 0.697\n",
      "epoch 31, batch 270, train_loss 0.703\n",
      "epoch 31, batch 280, train_loss 0.693\n",
      "epoch 31, batch 290, train_loss 0.700\n",
      "epoch 31, batch 300, train_loss 0.694\n",
      "epoch 31, batch 310, train_loss 0.700\n",
      "epoch 31, batch 320, train_loss 0.699\n",
      "epoch 31, batch 330, train_loss 0.686\n",
      "epoch 31, batch 340, train_loss 0.694\n",
      "epoch 31, batch 350, train_loss 0.696\n",
      "epoch 31, batch 360, train_loss 0.700\n",
      "epoch 31, batch 370, train_loss 0.698\n",
      "epoch 31, batch 380, train_loss 0.695\n",
      "epoch 31, batch 390, train_loss 0.694\n",
      "epoch 31, batch 400, train_loss 0.703\n",
      "epoch 31, batch 410, train_loss 0.689\n",
      "epoch 31, batch 420, train_loss 0.694\n",
      "epoch 31, batch 430, train_loss 0.708\n",
      "epoch 31, batch 440, train_loss 0.695\n",
      "epoch 31, batch 450, train_loss 0.704\n",
      "epoch 31, batch 460, train_loss 0.696\n",
      "epoch 31, batch 470, train_loss 0.699\n",
      "epoch 31, batch 480, train_loss 0.698\n",
      "epoch 31, batch 490, train_loss 0.698\n",
      "epoch 31, batch 500, train_loss 0.688\n",
      "epoch 31, batch 510, train_loss 0.689\n",
      "epoch 31, batch 520, train_loss 0.696\n",
      "epoch 31, batch 530, train_loss 0.685\n",
      "epoch 31, batch 540, train_loss 0.692\n",
      "epoch 31, batch 550, train_loss 0.696\n",
      "epoch 31, batch 560, train_loss 0.697\n",
      "epoch 31, batch 570, train_loss 0.703\n",
      "epoch 31, batch 580, train_loss 0.684\n",
      "epoch 31, batch 590, train_loss 0.703\n",
      "epoch 31, batch 600, train_loss 0.689\n",
      "epoch 31, batch 610, train_loss 0.696\n",
      "epoch 31, batch 620, train_loss 0.693\n",
      "epoch 31, batch 630, train_loss 0.695\n",
      "epoch 31, batch 640, train_loss 0.695\n",
      "epoch 31, batch 650, train_loss 0.700\n",
      "epoch 31, batch 660, train_loss 0.700\n",
      "epoch 31, batch 670, train_loss 0.693\n",
      "epoch 31, batch 680, train_loss 0.708\n",
      "epoch 31, batch 690, train_loss 0.702\n",
      "epoch 31, batch 700, train_loss 0.697\n",
      "epoch 31, batch 710, train_loss 0.692\n",
      "epoch 31, batch 720, train_loss 0.693\n",
      "epoch 31, batch 730, train_loss 0.690\n",
      "epoch 31, batch 740, train_loss 0.695\n",
      "epoch 31, batch 750, train_loss 0.705\n",
      "epoch 31, batch 760, train_loss 0.698\n",
      "epoch 31, batch 770, train_loss 0.702\n",
      "epoch 31, batch 780, train_loss 0.711\n",
      "epoch 31, batch 790, train_loss 0.699\n",
      "epoch 31, batch 800, train_loss 0.691\n",
      "epoch 31, batch 810, train_loss 0.699\n",
      "epoch 31, batch 820, train_loss 0.698\n",
      "epoch 31, batch 830, train_loss 0.702\n",
      "epoch 31, batch 840, train_loss 0.701\n",
      "epoch 31, batch 850, train_loss 0.703\n",
      "epoch 31, batch 860, train_loss 0.692\n",
      "epoch 31, batch 870, train_loss 0.692\n",
      "epoch 31, batch 880, train_loss 0.697\n",
      "epoch 31, batch 890, train_loss 0.694\n",
      "epoch 31, batch 900, train_loss 0.694\n",
      "epoch 31, batch 910, train_loss 0.693\n",
      "epoch 31, batch 920, train_loss 0.692\n",
      "epoch 31, batch 930, train_loss 0.679\n",
      "epoch 31, batch 940, train_loss 0.694\n",
      "epoch 31, batch 950, train_loss 0.693\n",
      "epoch 31, batch 960, train_loss 0.689\n",
      "epoch 31, batch 970, train_loss 0.695\n",
      "epoch 31, batch 980, train_loss 0.699\n",
      "epoch 31, batch 990, train_loss 0.696\n",
      "epoch 31, batch 1000, train_loss 0.693\n",
      "epoch 31, batch 1010, train_loss 0.694\n",
      "epoch 31, batch 1020, train_loss 0.692\n",
      "epoch 31, batch 1030, train_loss 0.696\n",
      "epoch 31, batch 1040, train_loss 0.696\n",
      "epoch 31, batch 1050, train_loss 0.695\n",
      "epoch 31, batch 1060, train_loss 0.699\n",
      "epoch 31, batch 1070, train_loss 0.688\n",
      "epoch 31, batch 1080, train_loss 0.696\n",
      "epoch 31, batch 1090, train_loss 0.700\n",
      "epoch 31, batch 1100, train_loss 0.706\n",
      "epoch 31, batch 1110, train_loss 0.697\n",
      "epoch 31, batch 1120, train_loss 0.686\n",
      "epoch 31, batch 1130, train_loss 0.695\n",
      "epoch 31, batch 1140, train_loss 0.693\n",
      "epoch 31, batch 1150, train_loss 0.698\n",
      "epoch 31, batch 1160, train_loss 0.693\n",
      "epoch 31, batch 1170, train_loss 0.689\n",
      "epoch 31, batch 1180, train_loss 0.697\n",
      "epoch 31, batch 1190, train_loss 0.697\n",
      "epoch    31, train_loss 0.696, valid_loss 0.723, train_accuracy  70.09%, valid_accuracy  68.57%\n",
      "epoch 32, batch 0, train_loss 0.693\n",
      "epoch 32, batch 10, train_loss 0.698\n",
      "epoch 32, batch 20, train_loss 0.699\n",
      "epoch 32, batch 30, train_loss 0.703\n",
      "epoch 32, batch 40, train_loss 0.700\n",
      "epoch 32, batch 50, train_loss 0.704\n",
      "epoch 32, batch 60, train_loss 0.698\n",
      "epoch 32, batch 70, train_loss 0.693\n",
      "epoch 32, batch 80, train_loss 0.695\n",
      "epoch 32, batch 90, train_loss 0.699\n",
      "epoch 32, batch 100, train_loss 0.693\n",
      "epoch 32, batch 110, train_loss 0.692\n",
      "epoch 32, batch 120, train_loss 0.691\n",
      "epoch 32, batch 130, train_loss 0.695\n",
      "epoch 32, batch 140, train_loss 0.692\n",
      "epoch 32, batch 150, train_loss 0.689\n",
      "epoch 32, batch 160, train_loss 0.696\n",
      "epoch 32, batch 170, train_loss 0.690\n",
      "epoch 32, batch 180, train_loss 0.698\n",
      "epoch 32, batch 190, train_loss 0.689\n",
      "epoch 32, batch 200, train_loss 0.694\n",
      "epoch 32, batch 210, train_loss 0.696\n",
      "epoch 32, batch 220, train_loss 0.688\n",
      "epoch 32, batch 230, train_loss 0.703\n",
      "epoch 32, batch 240, train_loss 0.695\n",
      "epoch 32, batch 250, train_loss 0.691\n",
      "epoch 32, batch 260, train_loss 0.699\n",
      "epoch 32, batch 270, train_loss 0.693\n",
      "epoch 32, batch 280, train_loss 0.685\n",
      "epoch 32, batch 290, train_loss 0.692\n",
      "epoch 32, batch 300, train_loss 0.702\n",
      "epoch 32, batch 310, train_loss 0.690\n",
      "epoch 32, batch 320, train_loss 0.704\n",
      "epoch 32, batch 330, train_loss 0.689\n",
      "epoch 32, batch 340, train_loss 0.700\n",
      "epoch 32, batch 350, train_loss 0.710\n",
      "epoch 32, batch 360, train_loss 0.701\n",
      "epoch 32, batch 370, train_loss 0.705\n",
      "epoch 32, batch 380, train_loss 0.693\n",
      "epoch 32, batch 390, train_loss 0.695\n",
      "epoch 32, batch 400, train_loss 0.695\n",
      "epoch 32, batch 410, train_loss 0.699\n",
      "epoch 32, batch 420, train_loss 0.694\n",
      "epoch 32, batch 430, train_loss 0.696\n",
      "epoch 32, batch 440, train_loss 0.699\n",
      "epoch 32, batch 450, train_loss 0.702\n",
      "epoch 32, batch 460, train_loss 0.689\n",
      "epoch 32, batch 470, train_loss 0.693\n",
      "epoch 32, batch 480, train_loss 0.697\n",
      "epoch 32, batch 490, train_loss 0.690\n",
      "epoch 32, batch 500, train_loss 0.705\n",
      "epoch 32, batch 510, train_loss 0.702\n",
      "epoch 32, batch 520, train_loss 0.700\n",
      "epoch 32, batch 530, train_loss 0.687\n",
      "epoch 32, batch 540, train_loss 0.696\n",
      "epoch 32, batch 550, train_loss 0.700\n",
      "epoch 32, batch 560, train_loss 0.693\n",
      "epoch 32, batch 570, train_loss 0.699\n",
      "epoch 32, batch 580, train_loss 0.695\n",
      "epoch 32, batch 590, train_loss 0.696\n",
      "epoch 32, batch 600, train_loss 0.703\n",
      "epoch 32, batch 610, train_loss 0.698\n",
      "epoch 32, batch 620, train_loss 0.693\n",
      "epoch 32, batch 630, train_loss 0.694\n",
      "epoch 32, batch 640, train_loss 0.695\n",
      "epoch 32, batch 650, train_loss 0.695\n",
      "epoch 32, batch 660, train_loss 0.695\n",
      "epoch 32, batch 670, train_loss 0.695\n",
      "epoch 32, batch 680, train_loss 0.691\n",
      "epoch 32, batch 690, train_loss 0.696\n",
      "epoch 32, batch 700, train_loss 0.692\n",
      "epoch 32, batch 710, train_loss 0.687\n",
      "epoch 32, batch 720, train_loss 0.689\n",
      "epoch 32, batch 730, train_loss 0.696\n",
      "epoch 32, batch 740, train_loss 0.706\n",
      "epoch 32, batch 750, train_loss 0.693\n",
      "epoch 32, batch 760, train_loss 0.695\n",
      "epoch 32, batch 770, train_loss 0.696\n",
      "epoch 32, batch 780, train_loss 0.692\n",
      "epoch 32, batch 790, train_loss 0.701\n",
      "epoch 32, batch 800, train_loss 0.699\n",
      "epoch 32, batch 810, train_loss 0.690\n",
      "epoch 32, batch 820, train_loss 0.701\n",
      "epoch 32, batch 830, train_loss 0.698\n",
      "epoch 32, batch 840, train_loss 0.695\n",
      "epoch 32, batch 850, train_loss 0.694\n",
      "epoch 32, batch 860, train_loss 0.690\n",
      "epoch 32, batch 870, train_loss 0.699\n",
      "epoch 32, batch 880, train_loss 0.696\n",
      "epoch 32, batch 890, train_loss 0.691\n",
      "epoch 32, batch 900, train_loss 0.693\n",
      "epoch 32, batch 910, train_loss 0.690\n",
      "epoch 32, batch 920, train_loss 0.694\n",
      "epoch 32, batch 930, train_loss 0.692\n",
      "epoch 32, batch 940, train_loss 0.693\n",
      "epoch 32, batch 950, train_loss 0.694\n",
      "epoch 32, batch 960, train_loss 0.696\n",
      "epoch 32, batch 970, train_loss 0.700\n",
      "epoch 32, batch 980, train_loss 0.693\n",
      "epoch 32, batch 990, train_loss 0.701\n",
      "epoch 32, batch 1000, train_loss 0.702\n",
      "epoch 32, batch 1010, train_loss 0.691\n",
      "epoch 32, batch 1020, train_loss 0.693\n",
      "epoch 32, batch 1030, train_loss 0.689\n",
      "epoch 32, batch 1040, train_loss 0.690\n",
      "epoch 32, batch 1050, train_loss 0.695\n",
      "epoch 32, batch 1060, train_loss 0.695\n",
      "epoch 32, batch 1070, train_loss 0.698\n",
      "epoch 32, batch 1080, train_loss 0.702\n",
      "epoch 32, batch 1090, train_loss 0.689\n",
      "epoch 32, batch 1100, train_loss 0.696\n",
      "epoch 32, batch 1110, train_loss 0.704\n",
      "epoch 32, batch 1120, train_loss 0.704\n",
      "epoch 32, batch 1130, train_loss 0.691\n",
      "epoch 32, batch 1140, train_loss 0.697\n",
      "epoch 32, batch 1150, train_loss 0.694\n",
      "epoch 32, batch 1160, train_loss 0.693\n",
      "epoch 32, batch 1170, train_loss 0.702\n",
      "epoch 32, batch 1180, train_loss 0.686\n",
      "epoch 32, batch 1190, train_loss 0.699\n",
      "epoch    32, train_loss 0.694, valid_loss 0.722, train_accuracy  70.17%, valid_accuracy  68.58%\n",
      "epoch 33, batch 0, train_loss 0.691\n",
      "epoch 33, batch 10, train_loss 0.697\n",
      "epoch 33, batch 20, train_loss 0.695\n",
      "epoch 33, batch 30, train_loss 0.699\n",
      "epoch 33, batch 40, train_loss 0.684\n",
      "epoch 33, batch 50, train_loss 0.695\n",
      "epoch 33, batch 60, train_loss 0.695\n",
      "epoch 33, batch 70, train_loss 0.692\n",
      "epoch 33, batch 80, train_loss 0.695\n",
      "epoch 33, batch 90, train_loss 0.697\n",
      "epoch 33, batch 100, train_loss 0.694\n",
      "epoch 33, batch 110, train_loss 0.690\n",
      "epoch 33, batch 120, train_loss 0.694\n",
      "epoch 33, batch 130, train_loss 0.690\n",
      "epoch 33, batch 140, train_loss 0.689\n",
      "epoch 33, batch 150, train_loss 0.695\n",
      "epoch 33, batch 160, train_loss 0.692\n",
      "epoch 33, batch 170, train_loss 0.705\n",
      "epoch 33, batch 180, train_loss 0.694\n",
      "epoch 33, batch 190, train_loss 0.695\n",
      "epoch 33, batch 200, train_loss 0.694\n",
      "epoch 33, batch 210, train_loss 0.697\n",
      "epoch 33, batch 220, train_loss 0.696\n",
      "epoch 33, batch 230, train_loss 0.683\n",
      "epoch 33, batch 240, train_loss 0.689\n",
      "epoch 33, batch 250, train_loss 0.704\n",
      "epoch 33, batch 260, train_loss 0.690\n",
      "epoch 33, batch 270, train_loss 0.691\n",
      "epoch 33, batch 280, train_loss 0.701\n",
      "epoch 33, batch 290, train_loss 0.700\n",
      "epoch 33, batch 300, train_loss 0.691\n",
      "epoch 33, batch 310, train_loss 0.691\n",
      "epoch 33, batch 320, train_loss 0.685\n",
      "epoch 33, batch 330, train_loss 0.695\n",
      "epoch 33, batch 340, train_loss 0.688\n",
      "epoch 33, batch 350, train_loss 0.699\n",
      "epoch 33, batch 360, train_loss 0.696\n",
      "epoch 33, batch 370, train_loss 0.703\n",
      "epoch 33, batch 380, train_loss 0.692\n",
      "epoch 33, batch 390, train_loss 0.691\n",
      "epoch 33, batch 400, train_loss 0.694\n",
      "epoch 33, batch 410, train_loss 0.703\n",
      "epoch 33, batch 420, train_loss 0.699\n",
      "epoch 33, batch 430, train_loss 0.697\n",
      "epoch 33, batch 440, train_loss 0.685\n",
      "epoch 33, batch 450, train_loss 0.693\n",
      "epoch 33, batch 460, train_loss 0.694\n",
      "epoch 33, batch 470, train_loss 0.701\n",
      "epoch 33, batch 480, train_loss 0.685\n",
      "epoch 33, batch 490, train_loss 0.689\n",
      "epoch 33, batch 500, train_loss 0.692\n",
      "epoch 33, batch 510, train_loss 0.685\n",
      "epoch 33, batch 520, train_loss 0.697\n",
      "epoch 33, batch 530, train_loss 0.693\n",
      "epoch 33, batch 540, train_loss 0.692\n",
      "epoch 33, batch 550, train_loss 0.698\n",
      "epoch 33, batch 560, train_loss 0.698\n",
      "epoch 33, batch 570, train_loss 0.694\n",
      "epoch 33, batch 580, train_loss 0.694\n",
      "epoch 33, batch 590, train_loss 0.701\n",
      "epoch 33, batch 600, train_loss 0.697\n",
      "epoch 33, batch 610, train_loss 0.695\n",
      "epoch 33, batch 620, train_loss 0.700\n",
      "epoch 33, batch 630, train_loss 0.693\n",
      "epoch 33, batch 640, train_loss 0.702\n",
      "epoch 33, batch 650, train_loss 0.695\n",
      "epoch 33, batch 660, train_loss 0.699\n",
      "epoch 33, batch 670, train_loss 0.693\n",
      "epoch 33, batch 680, train_loss 0.692\n",
      "epoch 33, batch 690, train_loss 0.692\n",
      "epoch 33, batch 700, train_loss 0.695\n",
      "epoch 33, batch 710, train_loss 0.701\n",
      "epoch 33, batch 720, train_loss 0.694\n",
      "epoch 33, batch 730, train_loss 0.696\n",
      "epoch 33, batch 740, train_loss 0.698\n",
      "epoch 33, batch 750, train_loss 0.686\n",
      "epoch 33, batch 760, train_loss 0.694\n",
      "epoch 33, batch 770, train_loss 0.692\n",
      "epoch 33, batch 780, train_loss 0.699\n",
      "epoch 33, batch 790, train_loss 0.695\n",
      "epoch 33, batch 800, train_loss 0.704\n",
      "epoch 33, batch 810, train_loss 0.689\n",
      "epoch 33, batch 820, train_loss 0.694\n",
      "epoch 33, batch 830, train_loss 0.696\n",
      "epoch 33, batch 840, train_loss 0.691\n",
      "epoch 33, batch 850, train_loss 0.699\n",
      "epoch 33, batch 860, train_loss 0.693\n",
      "epoch 33, batch 870, train_loss 0.704\n",
      "epoch 33, batch 880, train_loss 0.692\n",
      "epoch 33, batch 890, train_loss 0.692\n",
      "epoch 33, batch 900, train_loss 0.690\n",
      "epoch 33, batch 910, train_loss 0.694\n",
      "epoch 33, batch 920, train_loss 0.693\n",
      "epoch 33, batch 930, train_loss 0.696\n",
      "epoch 33, batch 940, train_loss 0.702\n",
      "epoch 33, batch 950, train_loss 0.695\n",
      "epoch 33, batch 960, train_loss 0.692\n",
      "epoch 33, batch 970, train_loss 0.685\n",
      "epoch 33, batch 980, train_loss 0.695\n",
      "epoch 33, batch 990, train_loss 0.695\n",
      "epoch 33, batch 1000, train_loss 0.691\n",
      "epoch 33, batch 1010, train_loss 0.701\n",
      "epoch 33, batch 1020, train_loss 0.689\n",
      "epoch 33, batch 1030, train_loss 0.703\n",
      "epoch 33, batch 1040, train_loss 0.686\n",
      "epoch 33, batch 1050, train_loss 0.709\n",
      "epoch 33, batch 1060, train_loss 0.693\n",
      "epoch 33, batch 1070, train_loss 0.696\n",
      "epoch 33, batch 1080, train_loss 0.697\n",
      "epoch 33, batch 1090, train_loss 0.698\n",
      "epoch 33, batch 1100, train_loss 0.687\n",
      "epoch 33, batch 1110, train_loss 0.686\n",
      "epoch 33, batch 1120, train_loss 0.698\n",
      "epoch 33, batch 1130, train_loss 0.698\n",
      "epoch 33, batch 1140, train_loss 0.699\n",
      "epoch 33, batch 1150, train_loss 0.698\n",
      "epoch 33, batch 1160, train_loss 0.684\n",
      "epoch 33, batch 1170, train_loss 0.696\n",
      "epoch 33, batch 1180, train_loss 0.691\n",
      "epoch 33, batch 1190, train_loss 0.704\n",
      "epoch    33, train_loss 0.693, valid_loss 0.722, train_accuracy  70.19%, valid_accuracy  68.57%\n",
      "epoch 34, batch 0, train_loss 0.696\n",
      "epoch 34, batch 10, train_loss 0.701\n",
      "epoch 34, batch 20, train_loss 0.687\n",
      "epoch 34, batch 30, train_loss 0.699\n",
      "epoch 34, batch 40, train_loss 0.701\n",
      "epoch 34, batch 50, train_loss 0.689\n",
      "epoch 34, batch 60, train_loss 0.691\n",
      "epoch 34, batch 70, train_loss 0.693\n",
      "epoch 34, batch 80, train_loss 0.693\n",
      "epoch 34, batch 90, train_loss 0.699\n",
      "epoch 34, batch 100, train_loss 0.694\n",
      "epoch 34, batch 110, train_loss 0.696\n",
      "epoch 34, batch 120, train_loss 0.693\n",
      "epoch 34, batch 130, train_loss 0.686\n",
      "epoch 34, batch 140, train_loss 0.696\n",
      "epoch 34, batch 150, train_loss 0.694\n",
      "epoch 34, batch 160, train_loss 0.697\n",
      "epoch 34, batch 170, train_loss 0.692\n",
      "epoch 34, batch 180, train_loss 0.696\n",
      "epoch 34, batch 190, train_loss 0.688\n",
      "epoch 34, batch 200, train_loss 0.707\n",
      "epoch 34, batch 210, train_loss 0.689\n",
      "epoch 34, batch 220, train_loss 0.688\n",
      "epoch 34, batch 230, train_loss 0.698\n",
      "epoch 34, batch 240, train_loss 0.692\n",
      "epoch 34, batch 250, train_loss 0.697\n",
      "epoch 34, batch 260, train_loss 0.687\n",
      "epoch 34, batch 270, train_loss 0.696\n",
      "epoch 34, batch 280, train_loss 0.694\n",
      "epoch 34, batch 290, train_loss 0.696\n",
      "epoch 34, batch 300, train_loss 0.691\n",
      "epoch 34, batch 310, train_loss 0.689\n",
      "epoch 34, batch 320, train_loss 0.701\n",
      "epoch 34, batch 330, train_loss 0.695\n",
      "epoch 34, batch 340, train_loss 0.703\n",
      "epoch 34, batch 350, train_loss 0.702\n",
      "epoch 34, batch 360, train_loss 0.699\n",
      "epoch 34, batch 370, train_loss 0.697\n",
      "epoch 34, batch 380, train_loss 0.695\n",
      "epoch 34, batch 390, train_loss 0.694\n",
      "epoch 34, batch 400, train_loss 0.692\n",
      "epoch 34, batch 410, train_loss 0.697\n",
      "epoch 34, batch 420, train_loss 0.700\n",
      "epoch 34, batch 430, train_loss 0.689\n",
      "epoch 34, batch 440, train_loss 0.697\n",
      "epoch 34, batch 450, train_loss 0.687\n",
      "epoch 34, batch 460, train_loss 0.696\n",
      "epoch 34, batch 470, train_loss 0.699\n",
      "epoch 34, batch 480, train_loss 0.693\n",
      "epoch 34, batch 490, train_loss 0.687\n",
      "epoch 34, batch 500, train_loss 0.690\n",
      "epoch 34, batch 510, train_loss 0.696\n",
      "epoch 34, batch 520, train_loss 0.687\n",
      "epoch 34, batch 530, train_loss 0.689\n",
      "epoch 34, batch 540, train_loss 0.687\n",
      "epoch 34, batch 550, train_loss 0.700\n",
      "epoch 34, batch 560, train_loss 0.697\n",
      "epoch 34, batch 570, train_loss 0.693\n",
      "epoch 34, batch 580, train_loss 0.697\n",
      "epoch 34, batch 590, train_loss 0.686\n",
      "epoch 34, batch 600, train_loss 0.694\n",
      "epoch 34, batch 610, train_loss 0.693\n",
      "epoch 34, batch 620, train_loss 0.708\n",
      "epoch 34, batch 630, train_loss 0.699\n",
      "epoch 34, batch 640, train_loss 0.696\n",
      "epoch 34, batch 650, train_loss 0.702\n",
      "epoch 34, batch 660, train_loss 0.699\n",
      "epoch 34, batch 670, train_loss 0.689\n",
      "epoch 34, batch 680, train_loss 0.708\n",
      "epoch 34, batch 690, train_loss 0.698\n",
      "epoch 34, batch 700, train_loss 0.691\n",
      "epoch 34, batch 710, train_loss 0.698\n",
      "epoch 34, batch 720, train_loss 0.700\n",
      "epoch 34, batch 730, train_loss 0.689\n",
      "epoch 34, batch 740, train_loss 0.696\n",
      "epoch 34, batch 750, train_loss 0.683\n",
      "epoch 34, batch 760, train_loss 0.688\n",
      "epoch 34, batch 770, train_loss 0.696\n",
      "epoch 34, batch 780, train_loss 0.703\n",
      "epoch 34, batch 790, train_loss 0.702\n",
      "epoch 34, batch 800, train_loss 0.689\n",
      "epoch 34, batch 810, train_loss 0.690\n",
      "epoch 34, batch 820, train_loss 0.701\n",
      "epoch 34, batch 830, train_loss 0.691\n",
      "epoch 34, batch 840, train_loss 0.695\n",
      "epoch 34, batch 850, train_loss 0.701\n",
      "epoch 34, batch 860, train_loss 0.680\n",
      "epoch 34, batch 870, train_loss 0.696\n",
      "epoch 34, batch 880, train_loss 0.698\n",
      "epoch 34, batch 890, train_loss 0.702\n",
      "epoch 34, batch 900, train_loss 0.697\n",
      "epoch 34, batch 910, train_loss 0.701\n",
      "epoch 34, batch 920, train_loss 0.702\n",
      "epoch 34, batch 930, train_loss 0.683\n",
      "epoch 34, batch 940, train_loss 0.691\n",
      "epoch 34, batch 950, train_loss 0.689\n",
      "epoch 34, batch 960, train_loss 0.692\n",
      "epoch 34, batch 970, train_loss 0.694\n",
      "epoch 34, batch 980, train_loss 0.693\n",
      "epoch 34, batch 990, train_loss 0.693\n",
      "epoch 34, batch 1000, train_loss 0.694\n",
      "epoch 34, batch 1010, train_loss 0.693\n",
      "epoch 34, batch 1020, train_loss 0.699\n",
      "epoch 34, batch 1030, train_loss 0.698\n",
      "epoch 34, batch 1040, train_loss 0.688\n",
      "epoch 34, batch 1050, train_loss 0.692\n",
      "epoch 34, batch 1060, train_loss 0.708\n",
      "epoch 34, batch 1070, train_loss 0.702\n",
      "epoch 34, batch 1080, train_loss 0.688\n",
      "epoch 34, batch 1090, train_loss 0.691\n",
      "epoch 34, batch 1100, train_loss 0.693\n",
      "epoch 34, batch 1110, train_loss 0.701\n",
      "epoch 34, batch 1120, train_loss 0.691\n",
      "epoch 34, batch 1130, train_loss 0.686\n",
      "epoch 34, batch 1140, train_loss 0.700\n",
      "epoch 34, batch 1150, train_loss 0.700\n",
      "epoch 34, batch 1160, train_loss 0.700\n",
      "epoch 34, batch 1170, train_loss 0.711\n",
      "epoch 34, batch 1180, train_loss 0.697\n",
      "epoch 34, batch 1190, train_loss 0.697\n",
      "epoch    34, train_loss 0.694, valid_loss 0.723, train_accuracy  70.13%, valid_accuracy  68.54%\n",
      "epoch 35, batch 0, train_loss 0.707\n",
      "epoch 35, batch 10, train_loss 0.684\n",
      "epoch 35, batch 20, train_loss 0.701\n",
      "epoch 35, batch 30, train_loss 0.691\n",
      "epoch 35, batch 40, train_loss 0.700\n",
      "epoch 35, batch 50, train_loss 0.688\n",
      "epoch 35, batch 60, train_loss 0.697\n",
      "epoch 35, batch 70, train_loss 0.691\n",
      "epoch 35, batch 80, train_loss 0.691\n",
      "epoch 35, batch 90, train_loss 0.702\n",
      "epoch 35, batch 100, train_loss 0.691\n",
      "epoch 35, batch 110, train_loss 0.697\n",
      "epoch 35, batch 120, train_loss 0.707\n",
      "epoch 35, batch 130, train_loss 0.689\n",
      "epoch 35, batch 140, train_loss 0.691\n",
      "epoch 35, batch 150, train_loss 0.686\n",
      "epoch 35, batch 160, train_loss 0.692\n",
      "epoch 35, batch 170, train_loss 0.696\n",
      "epoch 35, batch 180, train_loss 0.693\n",
      "epoch 35, batch 190, train_loss 0.695\n",
      "epoch 35, batch 200, train_loss 0.686\n",
      "epoch 35, batch 210, train_loss 0.692\n",
      "epoch 35, batch 220, train_loss 0.699\n",
      "epoch 35, batch 230, train_loss 0.689\n",
      "epoch 35, batch 240, train_loss 0.694\n",
      "epoch 35, batch 250, train_loss 0.709\n",
      "epoch 35, batch 260, train_loss 0.695\n",
      "epoch 35, batch 270, train_loss 0.693\n",
      "epoch 35, batch 280, train_loss 0.695\n",
      "epoch 35, batch 290, train_loss 0.689\n",
      "epoch 35, batch 300, train_loss 0.701\n",
      "epoch 35, batch 310, train_loss 0.691\n",
      "epoch 35, batch 320, train_loss 0.689\n",
      "epoch 35, batch 330, train_loss 0.688\n",
      "epoch 35, batch 340, train_loss 0.703\n",
      "epoch 35, batch 350, train_loss 0.706\n",
      "epoch 35, batch 360, train_loss 0.697\n",
      "epoch 35, batch 370, train_loss 0.694\n",
      "epoch 35, batch 380, train_loss 0.685\n",
      "epoch 35, batch 390, train_loss 0.699\n",
      "epoch 35, batch 400, train_loss 0.692\n",
      "epoch 35, batch 410, train_loss 0.696\n",
      "epoch 35, batch 420, train_loss 0.691\n",
      "epoch 35, batch 430, train_loss 0.691\n",
      "epoch 35, batch 440, train_loss 0.692\n",
      "epoch 35, batch 450, train_loss 0.687\n",
      "epoch 35, batch 460, train_loss 0.690\n",
      "epoch 35, batch 470, train_loss 0.693\n",
      "epoch 35, batch 480, train_loss 0.693\n",
      "epoch 35, batch 490, train_loss 0.687\n",
      "epoch 35, batch 500, train_loss 0.700\n",
      "epoch 35, batch 510, train_loss 0.691\n",
      "epoch 35, batch 520, train_loss 0.697\n",
      "epoch 35, batch 530, train_loss 0.692\n",
      "epoch 35, batch 540, train_loss 0.690\n",
      "epoch 35, batch 550, train_loss 0.702\n",
      "epoch 35, batch 560, train_loss 0.694\n",
      "epoch 35, batch 570, train_loss 0.689\n",
      "epoch 35, batch 580, train_loss 0.695\n",
      "epoch 35, batch 590, train_loss 0.705\n",
      "epoch 35, batch 600, train_loss 0.691\n",
      "epoch 35, batch 610, train_loss 0.692\n",
      "epoch 35, batch 620, train_loss 0.697\n",
      "epoch 35, batch 630, train_loss 0.687\n",
      "epoch 35, batch 640, train_loss 0.697\n",
      "epoch 35, batch 650, train_loss 0.696\n",
      "epoch 35, batch 660, train_loss 0.694\n",
      "epoch 35, batch 670, train_loss 0.689\n",
      "epoch 35, batch 680, train_loss 0.695\n",
      "epoch 35, batch 690, train_loss 0.690\n",
      "epoch 35, batch 700, train_loss 0.710\n",
      "epoch 35, batch 710, train_loss 0.696\n",
      "epoch 35, batch 720, train_loss 0.689\n",
      "epoch 35, batch 730, train_loss 0.686\n",
      "epoch 35, batch 740, train_loss 0.686\n",
      "epoch 35, batch 750, train_loss 0.694\n",
      "epoch 35, batch 760, train_loss 0.694\n",
      "epoch 35, batch 770, train_loss 0.690\n",
      "epoch 35, batch 780, train_loss 0.706\n",
      "epoch 35, batch 790, train_loss 0.699\n",
      "epoch 35, batch 800, train_loss 0.686\n",
      "epoch 35, batch 810, train_loss 0.703\n",
      "epoch 35, batch 820, train_loss 0.702\n",
      "epoch 35, batch 830, train_loss 0.691\n",
      "epoch 35, batch 840, train_loss 0.692\n",
      "epoch 35, batch 850, train_loss 0.704\n",
      "epoch 35, batch 860, train_loss 0.703\n",
      "epoch 35, batch 870, train_loss 0.696\n",
      "epoch 35, batch 880, train_loss 0.696\n",
      "epoch 35, batch 890, train_loss 0.705\n",
      "epoch 35, batch 900, train_loss 0.699\n",
      "epoch 35, batch 910, train_loss 0.701\n",
      "epoch 35, batch 920, train_loss 0.695\n",
      "epoch 35, batch 930, train_loss 0.685\n",
      "epoch 35, batch 940, train_loss 0.708\n",
      "epoch 35, batch 950, train_loss 0.702\n",
      "epoch 35, batch 960, train_loss 0.688\n",
      "epoch 35, batch 970, train_loss 0.693\n",
      "epoch 35, batch 980, train_loss 0.699\n",
      "epoch 35, batch 990, train_loss 0.691\n",
      "epoch 35, batch 1000, train_loss 0.687\n",
      "epoch 35, batch 1010, train_loss 0.700\n",
      "epoch 35, batch 1020, train_loss 0.700\n",
      "epoch 35, batch 1030, train_loss 0.691\n",
      "epoch 35, batch 1040, train_loss 0.693\n",
      "epoch 35, batch 1050, train_loss 0.698\n",
      "epoch 35, batch 1060, train_loss 0.683\n",
      "epoch 35, batch 1070, train_loss 0.701\n",
      "epoch 35, batch 1080, train_loss 0.680\n",
      "epoch 35, batch 1090, train_loss 0.699\n",
      "epoch 35, batch 1100, train_loss 0.690\n",
      "epoch 35, batch 1110, train_loss 0.705\n",
      "epoch 35, batch 1120, train_loss 0.696\n",
      "epoch 35, batch 1130, train_loss 0.688\n",
      "epoch 35, batch 1140, train_loss 0.695\n",
      "epoch 35, batch 1150, train_loss 0.698\n",
      "epoch 35, batch 1160, train_loss 0.696\n",
      "epoch 35, batch 1170, train_loss 0.695\n",
      "epoch 35, batch 1180, train_loss 0.700\n",
      "epoch 35, batch 1190, train_loss 0.708\n",
      "epoch    35, train_loss 0.694, valid_loss 0.723, train_accuracy  70.20%, valid_accuracy  68.60%\n",
      "epoch 36, batch 0, train_loss 0.686\n",
      "epoch 36, batch 10, train_loss 0.695\n",
      "epoch 36, batch 20, train_loss 0.692\n",
      "epoch 36, batch 30, train_loss 0.686\n",
      "epoch 36, batch 40, train_loss 0.688\n",
      "epoch 36, batch 50, train_loss 0.698\n",
      "epoch 36, batch 60, train_loss 0.691\n",
      "epoch 36, batch 70, train_loss 0.696\n",
      "epoch 36, batch 80, train_loss 0.690\n",
      "epoch 36, batch 90, train_loss 0.690\n",
      "epoch 36, batch 100, train_loss 0.698\n",
      "epoch 36, batch 110, train_loss 0.689\n",
      "epoch 36, batch 120, train_loss 0.705\n",
      "epoch 36, batch 130, train_loss 0.702\n",
      "epoch 36, batch 140, train_loss 0.700\n",
      "epoch 36, batch 150, train_loss 0.693\n",
      "epoch 36, batch 160, train_loss 0.690\n",
      "epoch 36, batch 170, train_loss 0.691\n",
      "epoch 36, batch 180, train_loss 0.694\n",
      "epoch 36, batch 190, train_loss 0.689\n",
      "epoch 36, batch 200, train_loss 0.697\n",
      "epoch 36, batch 210, train_loss 0.695\n",
      "epoch 36, batch 220, train_loss 0.692\n",
      "epoch 36, batch 230, train_loss 0.694\n",
      "epoch 36, batch 240, train_loss 0.693\n",
      "epoch 36, batch 250, train_loss 0.702\n",
      "epoch 36, batch 260, train_loss 0.689\n",
      "epoch 36, batch 270, train_loss 0.695\n",
      "epoch 36, batch 280, train_loss 0.691\n",
      "epoch 36, batch 290, train_loss 0.691\n",
      "epoch 36, batch 300, train_loss 0.704\n",
      "epoch 36, batch 310, train_loss 0.698\n",
      "epoch 36, batch 320, train_loss 0.692\n",
      "epoch 36, batch 330, train_loss 0.696\n",
      "epoch 36, batch 340, train_loss 0.693\n",
      "epoch 36, batch 350, train_loss 0.685\n",
      "epoch 36, batch 360, train_loss 0.698\n",
      "epoch 36, batch 370, train_loss 0.693\n",
      "epoch 36, batch 380, train_loss 0.699\n",
      "epoch 36, batch 390, train_loss 0.686\n",
      "epoch 36, batch 400, train_loss 0.704\n",
      "epoch 36, batch 410, train_loss 0.696\n",
      "epoch 36, batch 420, train_loss 0.696\n",
      "epoch 36, batch 430, train_loss 0.699\n",
      "epoch 36, batch 440, train_loss 0.700\n",
      "epoch 36, batch 450, train_loss 0.688\n",
      "epoch 36, batch 460, train_loss 0.692\n",
      "epoch 36, batch 470, train_loss 0.697\n",
      "epoch 36, batch 480, train_loss 0.694\n",
      "epoch 36, batch 490, train_loss 0.688\n",
      "epoch 36, batch 500, train_loss 0.693\n",
      "epoch 36, batch 510, train_loss 0.704\n",
      "epoch 36, batch 520, train_loss 0.698\n",
      "epoch 36, batch 530, train_loss 0.701\n",
      "epoch 36, batch 540, train_loss 0.682\n",
      "epoch 36, batch 550, train_loss 0.697\n",
      "epoch 36, batch 560, train_loss 0.695\n",
      "epoch 36, batch 570, train_loss 0.691\n",
      "epoch 36, batch 580, train_loss 0.686\n",
      "epoch 36, batch 590, train_loss 0.700\n",
      "epoch 36, batch 600, train_loss 0.696\n",
      "epoch 36, batch 610, train_loss 0.692\n",
      "epoch 36, batch 620, train_loss 0.688\n",
      "epoch 36, batch 630, train_loss 0.700\n",
      "epoch 36, batch 640, train_loss 0.694\n",
      "epoch 36, batch 650, train_loss 0.702\n",
      "epoch 36, batch 660, train_loss 0.706\n",
      "epoch 36, batch 670, train_loss 0.692\n",
      "epoch 36, batch 680, train_loss 0.698\n",
      "epoch 36, batch 690, train_loss 0.694\n",
      "epoch 36, batch 700, train_loss 0.690\n",
      "epoch 36, batch 710, train_loss 0.692\n",
      "epoch 36, batch 720, train_loss 0.694\n",
      "epoch 36, batch 730, train_loss 0.691\n",
      "epoch 36, batch 740, train_loss 0.696\n",
      "epoch 36, batch 750, train_loss 0.683\n",
      "epoch 36, batch 760, train_loss 0.702\n",
      "epoch 36, batch 770, train_loss 0.697\n",
      "epoch 36, batch 780, train_loss 0.688\n",
      "epoch 36, batch 790, train_loss 0.694\n",
      "epoch 36, batch 800, train_loss 0.695\n",
      "epoch 36, batch 810, train_loss 0.695\n",
      "epoch 36, batch 820, train_loss 0.696\n",
      "epoch 36, batch 830, train_loss 0.696\n",
      "epoch 36, batch 840, train_loss 0.681\n",
      "epoch 36, batch 850, train_loss 0.698\n",
      "epoch 36, batch 860, train_loss 0.702\n",
      "epoch 36, batch 870, train_loss 0.685\n",
      "epoch 36, batch 880, train_loss 0.689\n",
      "epoch 36, batch 890, train_loss 0.690\n",
      "epoch 36, batch 900, train_loss 0.700\n",
      "epoch 36, batch 910, train_loss 0.698\n",
      "epoch 36, batch 920, train_loss 0.695\n",
      "epoch 36, batch 930, train_loss 0.692\n",
      "epoch 36, batch 940, train_loss 0.692\n",
      "epoch 36, batch 950, train_loss 0.699\n",
      "epoch 36, batch 960, train_loss 0.696\n",
      "epoch 36, batch 970, train_loss 0.701\n",
      "epoch 36, batch 980, train_loss 0.702\n",
      "epoch 36, batch 990, train_loss 0.686\n",
      "epoch 36, batch 1000, train_loss 0.692\n",
      "epoch 36, batch 1010, train_loss 0.689\n",
      "epoch 36, batch 1020, train_loss 0.682\n",
      "epoch 36, batch 1030, train_loss 0.703\n",
      "epoch 36, batch 1040, train_loss 0.698\n",
      "epoch 36, batch 1050, train_loss 0.707\n",
      "epoch 36, batch 1060, train_loss 0.700\n",
      "epoch 36, batch 1070, train_loss 0.699\n",
      "epoch 36, batch 1080, train_loss 0.692\n",
      "epoch 36, batch 1090, train_loss 0.693\n",
      "epoch 36, batch 1100, train_loss 0.695\n",
      "epoch 36, batch 1110, train_loss 0.694\n",
      "epoch 36, batch 1120, train_loss 0.698\n",
      "epoch 36, batch 1130, train_loss 0.692\n",
      "epoch 36, batch 1140, train_loss 0.687\n",
      "epoch 36, batch 1150, train_loss 0.699\n",
      "epoch 36, batch 1160, train_loss 0.694\n",
      "epoch 36, batch 1170, train_loss 0.693\n",
      "epoch 36, batch 1180, train_loss 0.699\n",
      "epoch 36, batch 1190, train_loss 0.699\n",
      "epoch    36, train_loss 0.693, valid_loss 0.722, train_accuracy  70.21%, valid_accuracy  68.61%\n",
      "epoch 37, batch 0, train_loss 0.695\n",
      "epoch 37, batch 10, train_loss 0.697\n",
      "epoch 37, batch 20, train_loss 0.690\n",
      "epoch 37, batch 30, train_loss 0.695\n",
      "epoch 37, batch 40, train_loss 0.691\n",
      "epoch 37, batch 50, train_loss 0.695\n",
      "epoch 37, batch 60, train_loss 0.691\n",
      "epoch 37, batch 70, train_loss 0.689\n",
      "epoch 37, batch 80, train_loss 0.685\n",
      "epoch 37, batch 90, train_loss 0.697\n",
      "epoch 37, batch 100, train_loss 0.709\n",
      "epoch 37, batch 110, train_loss 0.682\n",
      "epoch 37, batch 120, train_loss 0.682\n",
      "epoch 37, batch 130, train_loss 0.687\n",
      "epoch 37, batch 140, train_loss 0.692\n",
      "epoch 37, batch 150, train_loss 0.703\n",
      "epoch 37, batch 160, train_loss 0.694\n",
      "epoch 37, batch 170, train_loss 0.697\n",
      "epoch 37, batch 180, train_loss 0.693\n",
      "epoch 37, batch 190, train_loss 0.697\n",
      "epoch 37, batch 200, train_loss 0.700\n",
      "epoch 37, batch 210, train_loss 0.694\n",
      "epoch 37, batch 220, train_loss 0.696\n",
      "epoch 37, batch 230, train_loss 0.697\n",
      "epoch 37, batch 240, train_loss 0.692\n",
      "epoch 37, batch 250, train_loss 0.701\n",
      "epoch 37, batch 260, train_loss 0.691\n",
      "epoch 37, batch 270, train_loss 0.693\n",
      "epoch 37, batch 280, train_loss 0.695\n",
      "epoch 37, batch 290, train_loss 0.705\n",
      "epoch 37, batch 300, train_loss 0.686\n",
      "epoch 37, batch 310, train_loss 0.683\n",
      "epoch 37, batch 320, train_loss 0.691\n",
      "epoch 37, batch 330, train_loss 0.692\n",
      "epoch 37, batch 340, train_loss 0.697\n",
      "epoch 37, batch 350, train_loss 0.681\n",
      "epoch 37, batch 360, train_loss 0.694\n",
      "epoch 37, batch 370, train_loss 0.695\n",
      "epoch 37, batch 380, train_loss 0.698\n",
      "epoch 37, batch 390, train_loss 0.697\n",
      "epoch 37, batch 400, train_loss 0.703\n",
      "epoch 37, batch 410, train_loss 0.701\n",
      "epoch 37, batch 420, train_loss 0.705\n",
      "epoch 37, batch 430, train_loss 0.685\n",
      "epoch 37, batch 440, train_loss 0.694\n",
      "epoch 37, batch 450, train_loss 0.693\n",
      "epoch 37, batch 460, train_loss 0.684\n",
      "epoch 37, batch 470, train_loss 0.698\n",
      "epoch 37, batch 480, train_loss 0.701\n",
      "epoch 37, batch 490, train_loss 0.701\n",
      "epoch 37, batch 500, train_loss 0.692\n",
      "epoch 37, batch 510, train_loss 0.703\n",
      "epoch 37, batch 520, train_loss 0.695\n",
      "epoch 37, batch 530, train_loss 0.691\n",
      "epoch 37, batch 540, train_loss 0.684\n",
      "epoch 37, batch 550, train_loss 0.702\n",
      "epoch 37, batch 560, train_loss 0.694\n",
      "epoch 37, batch 570, train_loss 0.686\n",
      "epoch 37, batch 580, train_loss 0.696\n",
      "epoch 37, batch 590, train_loss 0.697\n",
      "epoch 37, batch 600, train_loss 0.698\n",
      "epoch 37, batch 610, train_loss 0.699\n",
      "epoch 37, batch 620, train_loss 0.689\n",
      "epoch 37, batch 630, train_loss 0.704\n",
      "epoch 37, batch 640, train_loss 0.702\n",
      "epoch 37, batch 650, train_loss 0.688\n",
      "epoch 37, batch 660, train_loss 0.693\n",
      "epoch 37, batch 670, train_loss 0.689\n",
      "epoch 37, batch 680, train_loss 0.692\n",
      "epoch 37, batch 690, train_loss 0.694\n",
      "epoch 37, batch 700, train_loss 0.693\n",
      "epoch 37, batch 710, train_loss 0.697\n",
      "epoch 37, batch 720, train_loss 0.687\n",
      "epoch 37, batch 730, train_loss 0.690\n",
      "epoch 37, batch 740, train_loss 0.696\n",
      "epoch 37, batch 750, train_loss 0.702\n",
      "epoch 37, batch 760, train_loss 0.694\n",
      "epoch 37, batch 770, train_loss 0.686\n",
      "epoch 37, batch 780, train_loss 0.695\n",
      "epoch 37, batch 790, train_loss 0.697\n",
      "epoch 37, batch 800, train_loss 0.694\n",
      "epoch 37, batch 810, train_loss 0.698\n",
      "epoch 37, batch 820, train_loss 0.690\n",
      "epoch 37, batch 830, train_loss 0.690\n",
      "epoch 37, batch 840, train_loss 0.683\n",
      "epoch 37, batch 850, train_loss 0.687\n",
      "epoch 37, batch 860, train_loss 0.696\n",
      "epoch 37, batch 870, train_loss 0.686\n",
      "epoch 37, batch 880, train_loss 0.693\n",
      "epoch 37, batch 890, train_loss 0.684\n",
      "epoch 37, batch 900, train_loss 0.692\n",
      "epoch 37, batch 910, train_loss 0.698\n",
      "epoch 37, batch 920, train_loss 0.700\n",
      "epoch 37, batch 930, train_loss 0.706\n",
      "epoch 37, batch 940, train_loss 0.703\n",
      "epoch 37, batch 950, train_loss 0.704\n",
      "epoch 37, batch 960, train_loss 0.693\n",
      "epoch 37, batch 970, train_loss 0.697\n",
      "epoch 37, batch 980, train_loss 0.695\n",
      "epoch 37, batch 990, train_loss 0.698\n",
      "epoch 37, batch 1000, train_loss 0.688\n",
      "epoch 37, batch 1010, train_loss 0.705\n",
      "epoch 37, batch 1020, train_loss 0.682\n",
      "epoch 37, batch 1030, train_loss 0.698\n",
      "epoch 37, batch 1040, train_loss 0.694\n",
      "epoch 37, batch 1050, train_loss 0.680\n",
      "epoch 37, batch 1060, train_loss 0.695\n",
      "epoch 37, batch 1070, train_loss 0.689\n",
      "epoch 37, batch 1080, train_loss 0.702\n",
      "epoch 37, batch 1090, train_loss 0.694\n",
      "epoch 37, batch 1100, train_loss 0.695\n",
      "epoch 37, batch 1110, train_loss 0.692\n",
      "epoch 37, batch 1120, train_loss 0.691\n",
      "epoch 37, batch 1130, train_loss 0.704\n",
      "epoch 37, batch 1140, train_loss 0.694\n",
      "epoch 37, batch 1150, train_loss 0.698\n",
      "epoch 37, batch 1160, train_loss 0.687\n",
      "epoch 37, batch 1170, train_loss 0.691\n",
      "epoch 37, batch 1180, train_loss 0.700\n",
      "epoch 37, batch 1190, train_loss 0.693\n",
      "epoch    37, train_loss 0.693, valid_loss 0.722, train_accuracy  70.23%, valid_accuracy  68.58%\n",
      "epoch 38, batch 0, train_loss 0.694\n",
      "epoch 38, batch 10, train_loss 0.693\n",
      "epoch 38, batch 20, train_loss 0.695\n",
      "epoch 38, batch 30, train_loss 0.689\n",
      "epoch 38, batch 40, train_loss 0.684\n",
      "epoch 38, batch 50, train_loss 0.702\n",
      "epoch 38, batch 60, train_loss 0.699\n",
      "epoch 38, batch 70, train_loss 0.686\n",
      "epoch 38, batch 80, train_loss 0.693\n",
      "epoch 38, batch 90, train_loss 0.692\n",
      "epoch 38, batch 100, train_loss 0.688\n",
      "epoch 38, batch 110, train_loss 0.707\n",
      "epoch 38, batch 120, train_loss 0.689\n",
      "epoch 38, batch 130, train_loss 0.701\n",
      "epoch 38, batch 140, train_loss 0.686\n",
      "epoch 38, batch 150, train_loss 0.697\n",
      "epoch 38, batch 160, train_loss 0.691\n",
      "epoch 38, batch 170, train_loss 0.687\n",
      "epoch 38, batch 180, train_loss 0.696\n",
      "epoch 38, batch 190, train_loss 0.705\n",
      "epoch 38, batch 200, train_loss 0.704\n",
      "epoch 38, batch 210, train_loss 0.697\n",
      "epoch 38, batch 220, train_loss 0.695\n",
      "epoch 38, batch 230, train_loss 0.704\n",
      "epoch 38, batch 240, train_loss 0.698\n",
      "epoch 38, batch 250, train_loss 0.707\n",
      "epoch 38, batch 260, train_loss 0.691\n",
      "epoch 38, batch 270, train_loss 0.684\n",
      "epoch 38, batch 280, train_loss 0.693\n",
      "epoch 38, batch 290, train_loss 0.698\n",
      "epoch 38, batch 300, train_loss 0.691\n",
      "epoch 38, batch 310, train_loss 0.700\n",
      "epoch 38, batch 320, train_loss 0.687\n",
      "epoch 38, batch 330, train_loss 0.689\n",
      "epoch 38, batch 340, train_loss 0.699\n",
      "epoch 38, batch 350, train_loss 0.695\n",
      "epoch 38, batch 360, train_loss 0.691\n",
      "epoch 38, batch 370, train_loss 0.693\n",
      "epoch 38, batch 380, train_loss 0.700\n",
      "epoch 38, batch 390, train_loss 0.687\n",
      "epoch 38, batch 400, train_loss 0.692\n",
      "epoch 38, batch 410, train_loss 0.688\n",
      "epoch 38, batch 420, train_loss 0.693\n",
      "epoch 38, batch 430, train_loss 0.694\n",
      "epoch 38, batch 440, train_loss 0.693\n",
      "epoch 38, batch 450, train_loss 0.701\n",
      "epoch 38, batch 460, train_loss 0.689\n",
      "epoch 38, batch 470, train_loss 0.687\n",
      "epoch 38, batch 480, train_loss 0.706\n",
      "epoch 38, batch 490, train_loss 0.689\n",
      "epoch 38, batch 500, train_loss 0.699\n",
      "epoch 38, batch 510, train_loss 0.695\n",
      "epoch 38, batch 520, train_loss 0.688\n",
      "epoch 38, batch 530, train_loss 0.692\n",
      "epoch 38, batch 540, train_loss 0.707\n",
      "epoch 38, batch 550, train_loss 0.698\n",
      "epoch 38, batch 560, train_loss 0.686\n",
      "epoch 38, batch 570, train_loss 0.689\n",
      "epoch 38, batch 580, train_loss 0.699\n",
      "epoch 38, batch 590, train_loss 0.697\n",
      "epoch 38, batch 600, train_loss 0.706\n",
      "epoch 38, batch 610, train_loss 0.696\n",
      "epoch 38, batch 620, train_loss 0.695\n",
      "epoch 38, batch 630, train_loss 0.692\n",
      "epoch 38, batch 640, train_loss 0.697\n",
      "epoch 38, batch 650, train_loss 0.696\n",
      "epoch 38, batch 660, train_loss 0.693\n",
      "epoch 38, batch 670, train_loss 0.686\n",
      "epoch 38, batch 680, train_loss 0.694\n",
      "epoch 38, batch 690, train_loss 0.705\n",
      "epoch 38, batch 700, train_loss 0.692\n",
      "epoch 38, batch 710, train_loss 0.694\n",
      "epoch 38, batch 720, train_loss 0.691\n",
      "epoch 38, batch 730, train_loss 0.693\n",
      "epoch 38, batch 740, train_loss 0.697\n",
      "epoch 38, batch 750, train_loss 0.692\n",
      "epoch 38, batch 760, train_loss 0.696\n",
      "epoch 38, batch 770, train_loss 0.692\n",
      "epoch 38, batch 780, train_loss 0.691\n",
      "epoch 38, batch 790, train_loss 0.694\n",
      "epoch 38, batch 800, train_loss 0.699\n",
      "epoch 38, batch 810, train_loss 0.683\n",
      "epoch 38, batch 820, train_loss 0.693\n",
      "epoch 38, batch 830, train_loss 0.696\n",
      "epoch 38, batch 840, train_loss 0.692\n",
      "epoch 38, batch 850, train_loss 0.689\n",
      "epoch 38, batch 860, train_loss 0.712\n",
      "epoch 38, batch 870, train_loss 0.690\n",
      "epoch 38, batch 880, train_loss 0.699\n",
      "epoch 38, batch 890, train_loss 0.701\n",
      "epoch 38, batch 900, train_loss 0.697\n",
      "epoch 38, batch 910, train_loss 0.675\n",
      "epoch 38, batch 920, train_loss 0.692\n",
      "epoch 38, batch 930, train_loss 0.696\n",
      "epoch 38, batch 940, train_loss 0.703\n",
      "epoch 38, batch 950, train_loss 0.681\n",
      "epoch 38, batch 960, train_loss 0.700\n",
      "epoch 38, batch 970, train_loss 0.695\n",
      "epoch 38, batch 980, train_loss 0.703\n",
      "epoch 38, batch 990, train_loss 0.694\n",
      "epoch 38, batch 1000, train_loss 0.694\n",
      "epoch 38, batch 1010, train_loss 0.693\n",
      "epoch 38, batch 1020, train_loss 0.694\n",
      "epoch 38, batch 1030, train_loss 0.693\n",
      "epoch 38, batch 1040, train_loss 0.689\n",
      "epoch 38, batch 1050, train_loss 0.685\n",
      "epoch 38, batch 1060, train_loss 0.697\n",
      "epoch 38, batch 1070, train_loss 0.688\n",
      "epoch 38, batch 1080, train_loss 0.699\n",
      "epoch 38, batch 1090, train_loss 0.699\n",
      "epoch 38, batch 1100, train_loss 0.690\n",
      "epoch 38, batch 1110, train_loss 0.692\n",
      "epoch 38, batch 1120, train_loss 0.708\n",
      "epoch 38, batch 1130, train_loss 0.690\n",
      "epoch 38, batch 1140, train_loss 0.682\n",
      "epoch 38, batch 1150, train_loss 0.695\n",
      "epoch 38, batch 1160, train_loss 0.682\n",
      "epoch 38, batch 1170, train_loss 0.704\n",
      "epoch 38, batch 1180, train_loss 0.700\n",
      "epoch 38, batch 1190, train_loss 0.699\n",
      "epoch    38, train_loss 0.693, valid_loss 0.722, train_accuracy  70.22%, valid_accuracy  68.63%\n",
      "epoch 39, batch 0, train_loss 0.700\n",
      "epoch 39, batch 10, train_loss 0.689\n",
      "epoch 39, batch 20, train_loss 0.697\n",
      "epoch 39, batch 30, train_loss 0.682\n",
      "epoch 39, batch 40, train_loss 0.699\n",
      "epoch 39, batch 50, train_loss 0.699\n",
      "epoch 39, batch 60, train_loss 0.700\n",
      "epoch 39, batch 70, train_loss 0.698\n",
      "epoch 39, batch 80, train_loss 0.692\n",
      "epoch 39, batch 90, train_loss 0.702\n",
      "epoch 39, batch 100, train_loss 0.690\n",
      "epoch 39, batch 110, train_loss 0.695\n",
      "epoch 39, batch 120, train_loss 0.691\n",
      "epoch 39, batch 130, train_loss 0.703\n",
      "epoch 39, batch 140, train_loss 0.701\n",
      "epoch 39, batch 150, train_loss 0.696\n",
      "epoch 39, batch 160, train_loss 0.689\n",
      "epoch 39, batch 170, train_loss 0.703\n",
      "epoch 39, batch 180, train_loss 0.693\n",
      "epoch 39, batch 190, train_loss 0.690\n",
      "epoch 39, batch 200, train_loss 0.690\n",
      "epoch 39, batch 210, train_loss 0.700\n",
      "epoch 39, batch 220, train_loss 0.694\n",
      "epoch 39, batch 230, train_loss 0.689\n",
      "epoch 39, batch 240, train_loss 0.693\n",
      "epoch 39, batch 250, train_loss 0.690\n",
      "epoch 39, batch 260, train_loss 0.697\n",
      "epoch 39, batch 270, train_loss 0.700\n",
      "epoch 39, batch 280, train_loss 0.692\n",
      "epoch 39, batch 290, train_loss 0.693\n",
      "epoch 39, batch 300, train_loss 0.693\n",
      "epoch 39, batch 310, train_loss 0.691\n",
      "epoch 39, batch 320, train_loss 0.694\n",
      "epoch 39, batch 330, train_loss 0.705\n",
      "epoch 39, batch 340, train_loss 0.698\n",
      "epoch 39, batch 350, train_loss 0.688\n",
      "epoch 39, batch 360, train_loss 0.690\n",
      "epoch 39, batch 370, train_loss 0.692\n",
      "epoch 39, batch 380, train_loss 0.680\n",
      "epoch 39, batch 390, train_loss 0.691\n",
      "epoch 39, batch 400, train_loss 0.695\n",
      "epoch 39, batch 410, train_loss 0.697\n",
      "epoch 39, batch 420, train_loss 0.686\n",
      "epoch 39, batch 430, train_loss 0.695\n",
      "epoch 39, batch 440, train_loss 0.697\n",
      "epoch 39, batch 450, train_loss 0.691\n",
      "epoch 39, batch 460, train_loss 0.701\n",
      "epoch 39, batch 470, train_loss 0.694\n",
      "epoch 39, batch 480, train_loss 0.684\n",
      "epoch 39, batch 490, train_loss 0.692\n",
      "epoch 39, batch 500, train_loss 0.684\n",
      "epoch 39, batch 510, train_loss 0.696\n",
      "epoch 39, batch 520, train_loss 0.702\n",
      "epoch 39, batch 530, train_loss 0.692\n",
      "epoch 39, batch 540, train_loss 0.699\n",
      "epoch 39, batch 550, train_loss 0.699\n",
      "epoch 39, batch 560, train_loss 0.699\n",
      "epoch 39, batch 570, train_loss 0.687\n",
      "epoch 39, batch 580, train_loss 0.700\n",
      "epoch 39, batch 590, train_loss 0.700\n",
      "epoch 39, batch 600, train_loss 0.685\n",
      "epoch 39, batch 610, train_loss 0.696\n",
      "epoch 39, batch 620, train_loss 0.690\n",
      "epoch 39, batch 630, train_loss 0.703\n",
      "epoch 39, batch 640, train_loss 0.691\n",
      "epoch 39, batch 650, train_loss 0.690\n",
      "epoch 39, batch 660, train_loss 0.692\n",
      "epoch 39, batch 670, train_loss 0.697\n",
      "epoch 39, batch 680, train_loss 0.695\n",
      "epoch 39, batch 690, train_loss 0.686\n",
      "epoch 39, batch 700, train_loss 0.689\n",
      "epoch 39, batch 710, train_loss 0.701\n",
      "epoch 39, batch 720, train_loss 0.697\n",
      "epoch 39, batch 730, train_loss 0.695\n",
      "epoch 39, batch 740, train_loss 0.698\n",
      "epoch 39, batch 750, train_loss 0.692\n",
      "epoch 39, batch 760, train_loss 0.694\n",
      "epoch 39, batch 770, train_loss 0.699\n",
      "epoch 39, batch 780, train_loss 0.701\n",
      "epoch 39, batch 790, train_loss 0.704\n",
      "epoch 39, batch 800, train_loss 0.694\n",
      "epoch 39, batch 810, train_loss 0.691\n",
      "epoch 39, batch 820, train_loss 0.692\n",
      "epoch 39, batch 830, train_loss 0.688\n",
      "epoch 39, batch 840, train_loss 0.696\n",
      "epoch 39, batch 850, train_loss 0.690\n",
      "epoch 39, batch 860, train_loss 0.694\n",
      "epoch 39, batch 870, train_loss 0.700\n",
      "epoch 39, batch 880, train_loss 0.703\n",
      "epoch 39, batch 890, train_loss 0.696\n",
      "epoch 39, batch 900, train_loss 0.686\n",
      "epoch 39, batch 910, train_loss 0.695\n",
      "epoch 39, batch 920, train_loss 0.690\n",
      "epoch 39, batch 930, train_loss 0.685\n",
      "epoch 39, batch 940, train_loss 0.698\n",
      "epoch 39, batch 950, train_loss 0.700\n",
      "epoch 39, batch 960, train_loss 0.695\n",
      "epoch 39, batch 970, train_loss 0.693\n",
      "epoch 39, batch 980, train_loss 0.691\n",
      "epoch 39, batch 990, train_loss 0.694\n",
      "epoch 39, batch 1000, train_loss 0.696\n",
      "epoch 39, batch 1010, train_loss 0.699\n",
      "epoch 39, batch 1020, train_loss 0.695\n",
      "epoch 39, batch 1030, train_loss 0.686\n",
      "epoch 39, batch 1040, train_loss 0.694\n",
      "epoch 39, batch 1050, train_loss 0.691\n",
      "epoch 39, batch 1060, train_loss 0.691\n",
      "epoch 39, batch 1070, train_loss 0.686\n",
      "epoch 39, batch 1080, train_loss 0.703\n",
      "epoch 39, batch 1090, train_loss 0.695\n",
      "epoch 39, batch 1100, train_loss 0.683\n",
      "epoch 39, batch 1110, train_loss 0.692\n",
      "epoch 39, batch 1120, train_loss 0.690\n",
      "epoch 39, batch 1130, train_loss 0.692\n",
      "epoch 39, batch 1140, train_loss 0.692\n",
      "epoch 39, batch 1150, train_loss 0.694\n",
      "epoch 39, batch 1160, train_loss 0.683\n",
      "epoch 39, batch 1170, train_loss 0.694\n",
      "epoch 39, batch 1180, train_loss 0.702\n",
      "epoch 39, batch 1190, train_loss 0.697\n",
      "epoch    39, train_loss 0.693, valid_loss 0.723, train_accuracy  70.21%, valid_accuracy  68.58%\n",
      "epoch 40, batch 0, train_loss 0.698\n",
      "epoch 40, batch 10, train_loss 0.695\n",
      "epoch 40, batch 20, train_loss 0.694\n",
      "epoch 40, batch 30, train_loss 0.700\n",
      "epoch 40, batch 40, train_loss 0.694\n",
      "epoch 40, batch 50, train_loss 0.688\n",
      "epoch 40, batch 60, train_loss 0.695\n",
      "epoch 40, batch 70, train_loss 0.694\n",
      "epoch 40, batch 80, train_loss 0.693\n",
      "epoch 40, batch 90, train_loss 0.700\n",
      "epoch 40, batch 100, train_loss 0.691\n",
      "epoch 40, batch 110, train_loss 0.688\n",
      "epoch 40, batch 120, train_loss 0.688\n",
      "epoch 40, batch 130, train_loss 0.698\n",
      "epoch 40, batch 140, train_loss 0.704\n",
      "epoch 40, batch 150, train_loss 0.700\n",
      "epoch 40, batch 160, train_loss 0.693\n",
      "epoch 40, batch 170, train_loss 0.694\n",
      "epoch 40, batch 180, train_loss 0.689\n",
      "epoch 40, batch 190, train_loss 0.690\n",
      "epoch 40, batch 200, train_loss 0.691\n",
      "epoch 40, batch 210, train_loss 0.697\n",
      "epoch 40, batch 220, train_loss 0.703\n",
      "epoch 40, batch 230, train_loss 0.697\n",
      "epoch 40, batch 240, train_loss 0.696\n",
      "epoch 40, batch 250, train_loss 0.699\n",
      "epoch 40, batch 260, train_loss 0.700\n",
      "epoch 40, batch 270, train_loss 0.705\n",
      "epoch 40, batch 280, train_loss 0.686\n",
      "epoch 40, batch 290, train_loss 0.700\n",
      "epoch 40, batch 300, train_loss 0.693\n",
      "epoch 40, batch 310, train_loss 0.680\n",
      "epoch 40, batch 320, train_loss 0.697\n",
      "epoch 40, batch 330, train_loss 0.689\n",
      "epoch 40, batch 340, train_loss 0.692\n",
      "epoch 40, batch 350, train_loss 0.703\n",
      "epoch 40, batch 360, train_loss 0.690\n",
      "epoch 40, batch 370, train_loss 0.695\n",
      "epoch 40, batch 380, train_loss 0.705\n",
      "epoch 40, batch 390, train_loss 0.697\n",
      "epoch 40, batch 400, train_loss 0.693\n",
      "epoch 40, batch 410, train_loss 0.698\n",
      "epoch 40, batch 420, train_loss 0.699\n",
      "epoch 40, batch 430, train_loss 0.692\n",
      "epoch 40, batch 440, train_loss 0.696\n",
      "epoch 40, batch 450, train_loss 0.695\n",
      "epoch 40, batch 460, train_loss 0.694\n",
      "epoch 40, batch 470, train_loss 0.691\n",
      "epoch 40, batch 480, train_loss 0.693\n",
      "epoch 40, batch 490, train_loss 0.694\n",
      "epoch 40, batch 500, train_loss 0.690\n",
      "epoch 40, batch 510, train_loss 0.686\n",
      "epoch 40, batch 520, train_loss 0.700\n",
      "epoch 40, batch 530, train_loss 0.702\n",
      "epoch 40, batch 540, train_loss 0.704\n",
      "epoch 40, batch 550, train_loss 0.692\n",
      "epoch 40, batch 560, train_loss 0.689\n",
      "epoch 40, batch 570, train_loss 0.689\n",
      "epoch 40, batch 580, train_loss 0.695\n",
      "epoch 40, batch 590, train_loss 0.687\n",
      "epoch 40, batch 600, train_loss 0.699\n",
      "epoch 40, batch 610, train_loss 0.693\n",
      "epoch 40, batch 620, train_loss 0.685\n",
      "epoch 40, batch 630, train_loss 0.698\n",
      "epoch 40, batch 640, train_loss 0.701\n",
      "epoch 40, batch 650, train_loss 0.687\n",
      "epoch 40, batch 660, train_loss 0.696\n",
      "epoch 40, batch 670, train_loss 0.694\n",
      "epoch 40, batch 680, train_loss 0.687\n",
      "epoch 40, batch 690, train_loss 0.688\n",
      "epoch 40, batch 700, train_loss 0.696\n",
      "epoch 40, batch 710, train_loss 0.689\n",
      "epoch 40, batch 720, train_loss 0.702\n",
      "epoch 40, batch 730, train_loss 0.692\n",
      "epoch 40, batch 740, train_loss 0.688\n",
      "epoch 40, batch 750, train_loss 0.703\n",
      "epoch 40, batch 760, train_loss 0.696\n",
      "epoch 40, batch 770, train_loss 0.701\n",
      "epoch 40, batch 780, train_loss 0.699\n",
      "epoch 40, batch 790, train_loss 0.689\n",
      "epoch 40, batch 800, train_loss 0.695\n",
      "epoch 40, batch 810, train_loss 0.689\n",
      "epoch 40, batch 820, train_loss 0.692\n",
      "epoch 40, batch 830, train_loss 0.690\n",
      "epoch 40, batch 840, train_loss 0.698\n",
      "epoch 40, batch 850, train_loss 0.693\n",
      "epoch 40, batch 860, train_loss 0.694\n",
      "epoch 40, batch 870, train_loss 0.694\n",
      "epoch 40, batch 880, train_loss 0.695\n",
      "epoch 40, batch 890, train_loss 0.683\n",
      "epoch 40, batch 900, train_loss 0.693\n",
      "epoch 40, batch 910, train_loss 0.699\n",
      "epoch 40, batch 920, train_loss 0.696\n",
      "epoch 40, batch 930, train_loss 0.688\n",
      "epoch 40, batch 940, train_loss 0.694\n",
      "epoch 40, batch 950, train_loss 0.700\n",
      "epoch 40, batch 960, train_loss 0.686\n",
      "epoch 40, batch 970, train_loss 0.691\n",
      "epoch 40, batch 980, train_loss 0.701\n",
      "epoch 40, batch 990, train_loss 0.687\n",
      "epoch 40, batch 1000, train_loss 0.692\n",
      "epoch 40, batch 1010, train_loss 0.689\n",
      "epoch 40, batch 1020, train_loss 0.694\n",
      "epoch 40, batch 1030, train_loss 0.697\n",
      "epoch 40, batch 1040, train_loss 0.695\n",
      "epoch 40, batch 1050, train_loss 0.696\n",
      "epoch 40, batch 1060, train_loss 0.690\n",
      "epoch 40, batch 1070, train_loss 0.687\n",
      "epoch 40, batch 1080, train_loss 0.686\n",
      "epoch 40, batch 1090, train_loss 0.679\n",
      "epoch 40, batch 1100, train_loss 0.690\n",
      "epoch 40, batch 1110, train_loss 0.700\n",
      "epoch 40, batch 1120, train_loss 0.692\n",
      "epoch 40, batch 1130, train_loss 0.684\n",
      "epoch 40, batch 1140, train_loss 0.688\n",
      "epoch 40, batch 1150, train_loss 0.704\n",
      "epoch 40, batch 1160, train_loss 0.692\n",
      "epoch 40, batch 1170, train_loss 0.690\n",
      "epoch 40, batch 1180, train_loss 0.690\n",
      "epoch 40, batch 1190, train_loss 0.691\n",
      "epoch    40, train_loss 0.693, valid_loss 0.722, train_accuracy  70.23%, valid_accuracy  68.62%\n",
      "epoch 41, batch 0, train_loss 0.698\n",
      "epoch 41, batch 10, train_loss 0.681\n",
      "epoch 41, batch 20, train_loss 0.689\n",
      "epoch 41, batch 30, train_loss 0.694\n",
      "epoch 41, batch 40, train_loss 0.689\n",
      "epoch 41, batch 50, train_loss 0.693\n",
      "epoch 41, batch 60, train_loss 0.688\n",
      "epoch 41, batch 70, train_loss 0.694\n",
      "epoch 41, batch 80, train_loss 0.696\n",
      "epoch 41, batch 90, train_loss 0.695\n",
      "epoch 41, batch 100, train_loss 0.684\n",
      "epoch 41, batch 110, train_loss 0.698\n",
      "epoch 41, batch 120, train_loss 0.691\n",
      "epoch 41, batch 130, train_loss 0.691\n",
      "epoch 41, batch 140, train_loss 0.690\n",
      "epoch 41, batch 150, train_loss 0.693\n",
      "epoch 41, batch 160, train_loss 0.698\n",
      "epoch 41, batch 170, train_loss 0.696\n",
      "epoch 41, batch 180, train_loss 0.691\n",
      "epoch 41, batch 190, train_loss 0.699\n",
      "epoch 41, batch 200, train_loss 0.701\n",
      "epoch 41, batch 210, train_loss 0.689\n",
      "epoch 41, batch 220, train_loss 0.692\n",
      "epoch 41, batch 230, train_loss 0.687\n",
      "epoch 41, batch 240, train_loss 0.697\n",
      "epoch 41, batch 250, train_loss 0.701\n",
      "epoch 41, batch 260, train_loss 0.693\n",
      "epoch 41, batch 270, train_loss 0.697\n",
      "epoch 41, batch 280, train_loss 0.693\n",
      "epoch 41, batch 290, train_loss 0.700\n",
      "epoch 41, batch 300, train_loss 0.690\n",
      "epoch 41, batch 310, train_loss 0.693\n",
      "epoch 41, batch 320, train_loss 0.688\n",
      "epoch 41, batch 330, train_loss 0.689\n",
      "epoch 41, batch 340, train_loss 0.690\n",
      "epoch 41, batch 350, train_loss 0.709\n",
      "epoch 41, batch 360, train_loss 0.693\n",
      "epoch 41, batch 370, train_loss 0.695\n",
      "epoch 41, batch 380, train_loss 0.691\n",
      "epoch 41, batch 390, train_loss 0.699\n",
      "epoch 41, batch 400, train_loss 0.689\n",
      "epoch 41, batch 410, train_loss 0.695\n",
      "epoch 41, batch 420, train_loss 0.694\n",
      "epoch 41, batch 430, train_loss 0.694\n",
      "epoch 41, batch 440, train_loss 0.703\n",
      "epoch 41, batch 450, train_loss 0.691\n",
      "epoch 41, batch 460, train_loss 0.684\n",
      "epoch 41, batch 470, train_loss 0.693\n",
      "epoch 41, batch 480, train_loss 0.695\n",
      "epoch 41, batch 490, train_loss 0.688\n",
      "epoch 41, batch 500, train_loss 0.683\n",
      "epoch 41, batch 510, train_loss 0.694\n",
      "epoch 41, batch 520, train_loss 0.686\n",
      "epoch 41, batch 530, train_loss 0.695\n",
      "epoch 41, batch 540, train_loss 0.697\n",
      "epoch 41, batch 550, train_loss 0.698\n",
      "epoch 41, batch 560, train_loss 0.697\n",
      "epoch 41, batch 570, train_loss 0.687\n",
      "epoch 41, batch 580, train_loss 0.696\n",
      "epoch 41, batch 590, train_loss 0.704\n",
      "epoch 41, batch 600, train_loss 0.701\n",
      "epoch 41, batch 610, train_loss 0.695\n",
      "epoch 41, batch 620, train_loss 0.690\n",
      "epoch 41, batch 630, train_loss 0.691\n",
      "epoch 41, batch 640, train_loss 0.678\n",
      "epoch 41, batch 650, train_loss 0.692\n",
      "epoch 41, batch 660, train_loss 0.684\n",
      "epoch 41, batch 670, train_loss 0.688\n",
      "epoch 41, batch 680, train_loss 0.695\n",
      "epoch 41, batch 690, train_loss 0.690\n",
      "epoch 41, batch 700, train_loss 0.687\n",
      "epoch 41, batch 710, train_loss 0.702\n",
      "epoch 41, batch 720, train_loss 0.697\n",
      "epoch 41, batch 730, train_loss 0.702\n",
      "epoch 41, batch 740, train_loss 0.689\n",
      "epoch 41, batch 750, train_loss 0.688\n",
      "epoch 41, batch 760, train_loss 0.695\n",
      "epoch 41, batch 770, train_loss 0.695\n",
      "epoch 41, batch 780, train_loss 0.696\n",
      "epoch 41, batch 790, train_loss 0.684\n",
      "epoch 41, batch 800, train_loss 0.692\n",
      "epoch 41, batch 810, train_loss 0.691\n",
      "epoch 41, batch 820, train_loss 0.701\n",
      "epoch 41, batch 830, train_loss 0.689\n",
      "epoch 41, batch 840, train_loss 0.697\n",
      "epoch 41, batch 850, train_loss 0.700\n",
      "epoch 41, batch 860, train_loss 0.693\n",
      "epoch 41, batch 870, train_loss 0.698\n",
      "epoch 41, batch 880, train_loss 0.700\n",
      "epoch 41, batch 890, train_loss 0.691\n",
      "epoch 41, batch 900, train_loss 0.691\n",
      "epoch 41, batch 910, train_loss 0.695\n",
      "epoch 41, batch 920, train_loss 0.695\n",
      "epoch 41, batch 930, train_loss 0.695\n",
      "epoch 41, batch 940, train_loss 0.691\n",
      "epoch 41, batch 950, train_loss 0.696\n",
      "epoch 41, batch 960, train_loss 0.692\n",
      "epoch 41, batch 970, train_loss 0.687\n",
      "epoch 41, batch 980, train_loss 0.689\n",
      "epoch 41, batch 990, train_loss 0.688\n",
      "epoch 41, batch 1000, train_loss 0.687\n",
      "epoch 41, batch 1010, train_loss 0.693\n",
      "epoch 41, batch 1020, train_loss 0.703\n",
      "epoch 41, batch 1030, train_loss 0.691\n",
      "epoch 41, batch 1040, train_loss 0.707\n",
      "epoch 41, batch 1050, train_loss 0.691\n",
      "epoch 41, batch 1060, train_loss 0.693\n",
      "epoch 41, batch 1070, train_loss 0.690\n",
      "epoch 41, batch 1080, train_loss 0.687\n",
      "epoch 41, batch 1090, train_loss 0.685\n",
      "epoch 41, batch 1100, train_loss 0.691\n",
      "epoch 41, batch 1110, train_loss 0.699\n",
      "epoch 41, batch 1120, train_loss 0.700\n",
      "epoch 41, batch 1130, train_loss 0.700\n",
      "epoch 41, batch 1140, train_loss 0.694\n",
      "epoch 41, batch 1150, train_loss 0.699\n",
      "epoch 41, batch 1160, train_loss 0.687\n",
      "epoch 41, batch 1170, train_loss 0.706\n",
      "epoch 41, batch 1180, train_loss 0.683\n",
      "epoch 41, batch 1190, train_loss 0.693\n",
      "epoch    41, train_loss 0.693, valid_loss 0.723, train_accuracy  70.25%, valid_accuracy  68.58%\n",
      "epoch 42, batch 0, train_loss 0.696\n",
      "epoch 42, batch 10, train_loss 0.678\n",
      "epoch 42, batch 20, train_loss 0.696\n",
      "epoch 42, batch 30, train_loss 0.697\n",
      "epoch 42, batch 40, train_loss 0.697\n",
      "epoch 42, batch 50, train_loss 0.695\n",
      "epoch 42, batch 60, train_loss 0.689\n",
      "epoch 42, batch 70, train_loss 0.699\n",
      "epoch 42, batch 80, train_loss 0.695\n",
      "epoch 42, batch 90, train_loss 0.682\n",
      "epoch 42, batch 100, train_loss 0.701\n",
      "epoch 42, batch 110, train_loss 0.691\n",
      "epoch 42, batch 120, train_loss 0.687\n",
      "epoch 42, batch 130, train_loss 0.699\n",
      "epoch 42, batch 140, train_loss 0.690\n",
      "epoch 42, batch 150, train_loss 0.711\n",
      "epoch 42, batch 160, train_loss 0.688\n",
      "epoch 42, batch 170, train_loss 0.697\n",
      "epoch 42, batch 180, train_loss 0.689\n",
      "epoch 42, batch 190, train_loss 0.703\n",
      "epoch 42, batch 200, train_loss 0.691\n",
      "epoch 42, batch 210, train_loss 0.691\n",
      "epoch 42, batch 220, train_loss 0.691\n",
      "epoch 42, batch 230, train_loss 0.689\n",
      "epoch 42, batch 240, train_loss 0.699\n",
      "epoch 42, batch 250, train_loss 0.692\n",
      "epoch 42, batch 260, train_loss 0.690\n",
      "epoch 42, batch 270, train_loss 0.693\n",
      "epoch 42, batch 280, train_loss 0.698\n",
      "epoch 42, batch 290, train_loss 0.698\n",
      "epoch 42, batch 300, train_loss 0.692\n",
      "epoch 42, batch 310, train_loss 0.694\n",
      "epoch 42, batch 320, train_loss 0.685\n",
      "epoch 42, batch 330, train_loss 0.695\n",
      "epoch 42, batch 340, train_loss 0.699\n",
      "epoch 42, batch 350, train_loss 0.691\n",
      "epoch 42, batch 360, train_loss 0.701\n",
      "epoch 42, batch 370, train_loss 0.685\n",
      "epoch 42, batch 380, train_loss 0.704\n",
      "epoch 42, batch 390, train_loss 0.696\n",
      "epoch 42, batch 400, train_loss 0.692\n",
      "epoch 42, batch 410, train_loss 0.684\n",
      "epoch 42, batch 420, train_loss 0.689\n",
      "epoch 42, batch 430, train_loss 0.696\n",
      "epoch 42, batch 440, train_loss 0.690\n",
      "epoch 42, batch 450, train_loss 0.697\n",
      "epoch 42, batch 460, train_loss 0.690\n",
      "epoch 42, batch 470, train_loss 0.695\n",
      "epoch 42, batch 480, train_loss 0.695\n",
      "epoch 42, batch 490, train_loss 0.688\n",
      "epoch 42, batch 500, train_loss 0.696\n",
      "epoch 42, batch 510, train_loss 0.693\n",
      "epoch 42, batch 520, train_loss 0.690\n",
      "epoch 42, batch 530, train_loss 0.698\n",
      "epoch 42, batch 540, train_loss 0.701\n",
      "epoch 42, batch 550, train_loss 0.698\n",
      "epoch 42, batch 560, train_loss 0.700\n",
      "epoch 42, batch 570, train_loss 0.696\n",
      "epoch 42, batch 580, train_loss 0.699\n",
      "epoch 42, batch 590, train_loss 0.693\n",
      "epoch 42, batch 600, train_loss 0.688\n",
      "epoch 42, batch 610, train_loss 0.691\n",
      "epoch 42, batch 620, train_loss 0.697\n",
      "epoch 42, batch 630, train_loss 0.691\n",
      "epoch 42, batch 640, train_loss 0.706\n",
      "epoch 42, batch 650, train_loss 0.690\n",
      "epoch 42, batch 660, train_loss 0.691\n",
      "epoch 42, batch 670, train_loss 0.688\n",
      "epoch 42, batch 680, train_loss 0.692\n",
      "epoch 42, batch 690, train_loss 0.696\n",
      "epoch 42, batch 700, train_loss 0.705\n",
      "epoch 42, batch 710, train_loss 0.698\n",
      "epoch 42, batch 720, train_loss 0.685\n",
      "epoch 42, batch 730, train_loss 0.699\n",
      "epoch 42, batch 740, train_loss 0.702\n",
      "epoch 42, batch 750, train_loss 0.692\n",
      "epoch 42, batch 760, train_loss 0.692\n",
      "epoch 42, batch 770, train_loss 0.688\n",
      "epoch 42, batch 780, train_loss 0.694\n",
      "epoch 42, batch 790, train_loss 0.693\n",
      "epoch 42, batch 800, train_loss 0.695\n",
      "epoch 42, batch 810, train_loss 0.696\n",
      "epoch 42, batch 820, train_loss 0.699\n",
      "epoch 42, batch 830, train_loss 0.690\n",
      "epoch 42, batch 840, train_loss 0.692\n",
      "epoch 42, batch 850, train_loss 0.693\n",
      "epoch 42, batch 860, train_loss 0.686\n",
      "epoch 42, batch 870, train_loss 0.686\n",
      "epoch 42, batch 880, train_loss 0.700\n",
      "epoch 42, batch 890, train_loss 0.699\n",
      "epoch 42, batch 900, train_loss 0.692\n",
      "epoch 42, batch 910, train_loss 0.696\n",
      "epoch 42, batch 920, train_loss 0.692\n",
      "epoch 42, batch 930, train_loss 0.692\n",
      "epoch 42, batch 940, train_loss 0.689\n",
      "epoch 42, batch 950, train_loss 0.689\n",
      "epoch 42, batch 960, train_loss 0.697\n",
      "epoch 42, batch 970, train_loss 0.690\n",
      "epoch 42, batch 980, train_loss 0.697\n",
      "epoch 42, batch 990, train_loss 0.692\n",
      "epoch 42, batch 1000, train_loss 0.697\n",
      "epoch 42, batch 1010, train_loss 0.686\n",
      "epoch 42, batch 1020, train_loss 0.687\n",
      "epoch 42, batch 1030, train_loss 0.692\n",
      "epoch 42, batch 1040, train_loss 0.696\n",
      "epoch 42, batch 1050, train_loss 0.689\n",
      "epoch 42, batch 1060, train_loss 0.692\n",
      "epoch 42, batch 1070, train_loss 0.695\n",
      "epoch 42, batch 1080, train_loss 0.693\n",
      "epoch 42, batch 1090, train_loss 0.684\n",
      "epoch 42, batch 1100, train_loss 0.691\n",
      "epoch 42, batch 1110, train_loss 0.693\n",
      "epoch 42, batch 1120, train_loss 0.692\n",
      "epoch 42, batch 1130, train_loss 0.685\n",
      "epoch 42, batch 1140, train_loss 0.698\n",
      "epoch 42, batch 1150, train_loss 0.702\n",
      "epoch 42, batch 1160, train_loss 0.696\n",
      "epoch 42, batch 1170, train_loss 0.707\n",
      "epoch 42, batch 1180, train_loss 0.701\n",
      "epoch 42, batch 1190, train_loss 0.691\n",
      "epoch    42, train_loss 0.693, valid_loss 0.723, train_accuracy  70.22%, valid_accuracy  68.56%\n",
      "epoch 43, batch 0, train_loss 0.692\n",
      "epoch 43, batch 10, train_loss 0.693\n",
      "epoch 43, batch 20, train_loss 0.701\n",
      "epoch 43, batch 30, train_loss 0.694\n",
      "epoch 43, batch 40, train_loss 0.683\n",
      "epoch 43, batch 50, train_loss 0.699\n",
      "epoch 43, batch 60, train_loss 0.695\n",
      "epoch 43, batch 70, train_loss 0.691\n",
      "epoch 43, batch 80, train_loss 0.691\n",
      "epoch 43, batch 90, train_loss 0.700\n",
      "epoch 43, batch 100, train_loss 0.706\n",
      "epoch 43, batch 110, train_loss 0.689\n",
      "epoch 43, batch 120, train_loss 0.700\n",
      "epoch 43, batch 130, train_loss 0.689\n",
      "epoch 43, batch 140, train_loss 0.689\n",
      "epoch 43, batch 150, train_loss 0.698\n",
      "epoch 43, batch 160, train_loss 0.695\n",
      "epoch 43, batch 170, train_loss 0.691\n",
      "epoch 43, batch 180, train_loss 0.689\n",
      "epoch 43, batch 190, train_loss 0.687\n",
      "epoch 43, batch 200, train_loss 0.695\n",
      "epoch 43, batch 210, train_loss 0.692\n",
      "epoch 43, batch 220, train_loss 0.694\n",
      "epoch 43, batch 230, train_loss 0.698\n",
      "epoch 43, batch 240, train_loss 0.701\n",
      "epoch 43, batch 250, train_loss 0.690\n",
      "epoch 43, batch 260, train_loss 0.695\n",
      "epoch 43, batch 270, train_loss 0.683\n",
      "epoch 43, batch 280, train_loss 0.703\n",
      "epoch 43, batch 290, train_loss 0.691\n",
      "epoch 43, batch 300, train_loss 0.689\n",
      "epoch 43, batch 310, train_loss 0.696\n",
      "epoch 43, batch 320, train_loss 0.702\n",
      "epoch 43, batch 330, train_loss 0.694\n",
      "epoch 43, batch 340, train_loss 0.698\n",
      "epoch 43, batch 350, train_loss 0.696\n",
      "epoch 43, batch 360, train_loss 0.696\n",
      "epoch 43, batch 370, train_loss 0.699\n",
      "epoch 43, batch 380, train_loss 0.685\n",
      "epoch 43, batch 390, train_loss 0.699\n",
      "epoch 43, batch 400, train_loss 0.694\n",
      "epoch 43, batch 410, train_loss 0.688\n",
      "epoch 43, batch 420, train_loss 0.691\n",
      "epoch 43, batch 430, train_loss 0.696\n",
      "epoch 43, batch 440, train_loss 0.703\n",
      "epoch 43, batch 450, train_loss 0.693\n",
      "epoch 43, batch 460, train_loss 0.700\n",
      "epoch 43, batch 470, train_loss 0.691\n",
      "epoch 43, batch 480, train_loss 0.689\n",
      "epoch 43, batch 490, train_loss 0.698\n",
      "epoch 43, batch 500, train_loss 0.695\n",
      "epoch 43, batch 510, train_loss 0.689\n",
      "epoch 43, batch 520, train_loss 0.694\n",
      "epoch 43, batch 530, train_loss 0.707\n",
      "epoch 43, batch 540, train_loss 0.697\n",
      "epoch 43, batch 550, train_loss 0.694\n",
      "epoch 43, batch 560, train_loss 0.701\n",
      "epoch 43, batch 570, train_loss 0.690\n",
      "epoch 43, batch 580, train_loss 0.686\n",
      "epoch 43, batch 590, train_loss 0.697\n",
      "epoch 43, batch 600, train_loss 0.694\n",
      "epoch 43, batch 610, train_loss 0.688\n",
      "epoch 43, batch 620, train_loss 0.689\n",
      "epoch 43, batch 630, train_loss 0.681\n",
      "epoch 43, batch 640, train_loss 0.687\n",
      "epoch 43, batch 650, train_loss 0.699\n",
      "epoch 43, batch 660, train_loss 0.696\n",
      "epoch 43, batch 670, train_loss 0.688\n",
      "epoch 43, batch 680, train_loss 0.691\n",
      "epoch 43, batch 690, train_loss 0.694\n",
      "epoch 43, batch 700, train_loss 0.690\n",
      "epoch 43, batch 710, train_loss 0.695\n",
      "epoch 43, batch 720, train_loss 0.689\n",
      "epoch 43, batch 730, train_loss 0.694\n",
      "epoch 43, batch 740, train_loss 0.697\n",
      "epoch 43, batch 750, train_loss 0.693\n",
      "epoch 43, batch 760, train_loss 0.691\n",
      "epoch 43, batch 770, train_loss 0.703\n",
      "epoch 43, batch 780, train_loss 0.695\n",
      "epoch 43, batch 790, train_loss 0.688\n",
      "epoch 43, batch 800, train_loss 0.690\n",
      "epoch 43, batch 810, train_loss 0.687\n",
      "epoch 43, batch 820, train_loss 0.682\n",
      "epoch 43, batch 830, train_loss 0.691\n",
      "epoch 43, batch 840, train_loss 0.694\n",
      "epoch 43, batch 850, train_loss 0.684\n",
      "epoch 43, batch 860, train_loss 0.692\n",
      "epoch 43, batch 870, train_loss 0.698\n",
      "epoch 43, batch 880, train_loss 0.703\n",
      "epoch 43, batch 890, train_loss 0.704\n",
      "epoch 43, batch 900, train_loss 0.692\n",
      "epoch 43, batch 910, train_loss 0.690\n",
      "epoch 43, batch 920, train_loss 0.694\n",
      "epoch 43, batch 930, train_loss 0.692\n",
      "epoch 43, batch 940, train_loss 0.695\n",
      "epoch 43, batch 950, train_loss 0.695\n",
      "epoch 43, batch 960, train_loss 0.696\n",
      "epoch 43, batch 970, train_loss 0.698\n",
      "epoch 43, batch 980, train_loss 0.691\n",
      "epoch 43, batch 990, train_loss 0.680\n",
      "epoch 43, batch 1000, train_loss 0.687\n",
      "epoch 43, batch 1010, train_loss 0.697\n",
      "epoch 43, batch 1020, train_loss 0.686\n",
      "epoch 43, batch 1030, train_loss 0.689\n",
      "epoch 43, batch 1040, train_loss 0.695\n",
      "epoch 43, batch 1050, train_loss 0.703\n",
      "epoch 43, batch 1060, train_loss 0.696\n",
      "epoch 43, batch 1070, train_loss 0.697\n",
      "epoch 43, batch 1080, train_loss 0.694\n",
      "epoch 43, batch 1090, train_loss 0.692\n",
      "epoch 43, batch 1100, train_loss 0.695\n",
      "epoch 43, batch 1110, train_loss 0.696\n",
      "epoch 43, batch 1120, train_loss 0.690\n",
      "epoch 43, batch 1130, train_loss 0.695\n",
      "epoch 43, batch 1140, train_loss 0.695\n",
      "epoch 43, batch 1150, train_loss 0.690\n",
      "epoch 43, batch 1160, train_loss 0.702\n",
      "epoch 43, batch 1170, train_loss 0.696\n",
      "epoch 43, batch 1180, train_loss 0.694\n",
      "epoch 43, batch 1190, train_loss 0.696\n",
      "epoch    43, train_loss 0.693, valid_loss 0.722, train_accuracy  70.24%, valid_accuracy  68.60%\n",
      "epoch 44, batch 0, train_loss 0.694\n",
      "epoch 44, batch 10, train_loss 0.693\n",
      "epoch 44, batch 20, train_loss 0.696\n",
      "epoch 44, batch 30, train_loss 0.699\n",
      "epoch 44, batch 40, train_loss 0.691\n",
      "epoch 44, batch 50, train_loss 0.694\n",
      "epoch 44, batch 60, train_loss 0.690\n",
      "epoch 44, batch 70, train_loss 0.691\n",
      "epoch 44, batch 80, train_loss 0.691\n",
      "epoch 44, batch 90, train_loss 0.688\n",
      "epoch 44, batch 100, train_loss 0.686\n",
      "epoch 44, batch 110, train_loss 0.690\n",
      "epoch 44, batch 120, train_loss 0.690\n",
      "epoch 44, batch 130, train_loss 0.698\n",
      "epoch 44, batch 140, train_loss 0.695\n",
      "epoch 44, batch 150, train_loss 0.687\n",
      "epoch 44, batch 160, train_loss 0.694\n",
      "epoch 44, batch 170, train_loss 0.700\n",
      "epoch 44, batch 180, train_loss 0.694\n",
      "epoch 44, batch 190, train_loss 0.693\n",
      "epoch 44, batch 200, train_loss 0.692\n",
      "epoch 44, batch 210, train_loss 0.699\n",
      "epoch 44, batch 220, train_loss 0.694\n",
      "epoch 44, batch 230, train_loss 0.689\n",
      "epoch 44, batch 240, train_loss 0.685\n",
      "epoch 44, batch 250, train_loss 0.687\n",
      "epoch 44, batch 260, train_loss 0.688\n",
      "epoch 44, batch 270, train_loss 0.696\n",
      "epoch 44, batch 280, train_loss 0.697\n",
      "epoch 44, batch 290, train_loss 0.693\n",
      "epoch 44, batch 300, train_loss 0.686\n",
      "epoch 44, batch 310, train_loss 0.708\n",
      "epoch 44, batch 320, train_loss 0.692\n",
      "epoch 44, batch 330, train_loss 0.690\n",
      "epoch 44, batch 340, train_loss 0.687\n",
      "epoch 44, batch 350, train_loss 0.683\n",
      "epoch 44, batch 360, train_loss 0.697\n",
      "epoch 44, batch 370, train_loss 0.694\n",
      "epoch 44, batch 380, train_loss 0.699\n",
      "epoch 44, batch 390, train_loss 0.694\n",
      "epoch 44, batch 400, train_loss 0.694\n",
      "epoch 44, batch 410, train_loss 0.692\n",
      "epoch 44, batch 420, train_loss 0.699\n",
      "epoch 44, batch 430, train_loss 0.692\n",
      "epoch 44, batch 440, train_loss 0.695\n",
      "epoch 44, batch 450, train_loss 0.698\n",
      "epoch 44, batch 460, train_loss 0.689\n",
      "epoch 44, batch 470, train_loss 0.687\n",
      "epoch 44, batch 480, train_loss 0.694\n",
      "epoch 44, batch 490, train_loss 0.692\n",
      "epoch 44, batch 500, train_loss 0.690\n",
      "epoch 44, batch 510, train_loss 0.687\n",
      "epoch 44, batch 520, train_loss 0.689\n",
      "epoch 44, batch 530, train_loss 0.691\n",
      "epoch 44, batch 540, train_loss 0.692\n",
      "epoch 44, batch 550, train_loss 0.697\n",
      "epoch 44, batch 560, train_loss 0.693\n",
      "epoch 44, batch 570, train_loss 0.693\n",
      "epoch 44, batch 580, train_loss 0.690\n",
      "epoch 44, batch 590, train_loss 0.690\n",
      "epoch 44, batch 600, train_loss 0.696\n",
      "epoch 44, batch 610, train_loss 0.689\n",
      "epoch 44, batch 620, train_loss 0.692\n",
      "epoch 44, batch 630, train_loss 0.690\n",
      "epoch 44, batch 640, train_loss 0.694\n",
      "epoch 44, batch 650, train_loss 0.689\n",
      "epoch 44, batch 660, train_loss 0.705\n",
      "epoch 44, batch 670, train_loss 0.689\n",
      "epoch 44, batch 680, train_loss 0.694\n",
      "epoch 44, batch 690, train_loss 0.688\n",
      "epoch 44, batch 700, train_loss 0.690\n",
      "epoch 44, batch 710, train_loss 0.692\n",
      "epoch 44, batch 720, train_loss 0.683\n",
      "epoch 44, batch 730, train_loss 0.695\n",
      "epoch 44, batch 740, train_loss 0.699\n",
      "epoch 44, batch 750, train_loss 0.689\n",
      "epoch 44, batch 760, train_loss 0.699\n",
      "epoch 44, batch 770, train_loss 0.686\n",
      "epoch 44, batch 780, train_loss 0.699\n",
      "epoch 44, batch 790, train_loss 0.695\n",
      "epoch 44, batch 800, train_loss 0.697\n",
      "epoch 44, batch 810, train_loss 0.693\n",
      "epoch 44, batch 820, train_loss 0.694\n",
      "epoch 44, batch 830, train_loss 0.698\n",
      "epoch 44, batch 840, train_loss 0.696\n",
      "epoch 44, batch 850, train_loss 0.698\n",
      "epoch 44, batch 860, train_loss 0.688\n",
      "epoch 44, batch 870, train_loss 0.691\n",
      "epoch 44, batch 880, train_loss 0.694\n",
      "epoch 44, batch 890, train_loss 0.697\n",
      "epoch 44, batch 900, train_loss 0.699\n",
      "epoch 44, batch 910, train_loss 0.696\n",
      "epoch 44, batch 920, train_loss 0.686\n",
      "epoch 44, batch 930, train_loss 0.695\n",
      "epoch 44, batch 940, train_loss 0.699\n",
      "epoch 44, batch 950, train_loss 0.703\n",
      "epoch 44, batch 960, train_loss 0.694\n",
      "epoch 44, batch 970, train_loss 0.696\n",
      "epoch 44, batch 980, train_loss 0.696\n",
      "epoch 44, batch 990, train_loss 0.691\n",
      "epoch 44, batch 1000, train_loss 0.693\n",
      "epoch 44, batch 1010, train_loss 0.690\n",
      "epoch 44, batch 1020, train_loss 0.694\n",
      "epoch 44, batch 1030, train_loss 0.697\n",
      "epoch 44, batch 1040, train_loss 0.680\n",
      "epoch 44, batch 1050, train_loss 0.690\n",
      "epoch 44, batch 1060, train_loss 0.694\n",
      "epoch 44, batch 1070, train_loss 0.692\n",
      "epoch 44, batch 1080, train_loss 0.688\n",
      "epoch 44, batch 1090, train_loss 0.700\n",
      "epoch 44, batch 1100, train_loss 0.709\n",
      "epoch 44, batch 1110, train_loss 0.696\n",
      "epoch 44, batch 1120, train_loss 0.692\n",
      "epoch 44, batch 1130, train_loss 0.693\n",
      "epoch 44, batch 1140, train_loss 0.703\n",
      "epoch 44, batch 1150, train_loss 0.695\n",
      "epoch 44, batch 1160, train_loss 0.697\n",
      "epoch 44, batch 1170, train_loss 0.697\n",
      "epoch 44, batch 1180, train_loss 0.686\n",
      "epoch 44, batch 1190, train_loss 0.701\n",
      "epoch    44, train_loss 0.694, valid_loss 0.724, train_accuracy  70.19%, valid_accuracy  68.49%\n",
      "epoch 45, batch 0, train_loss 0.697\n",
      "epoch 45, batch 10, train_loss 0.699\n",
      "epoch 45, batch 20, train_loss 0.693\n",
      "epoch 45, batch 30, train_loss 0.690\n",
      "epoch 45, batch 40, train_loss 0.686\n",
      "epoch 45, batch 50, train_loss 0.701\n",
      "epoch 45, batch 60, train_loss 0.691\n",
      "epoch 45, batch 70, train_loss 0.702\n",
      "epoch 45, batch 80, train_loss 0.699\n",
      "epoch 45, batch 90, train_loss 0.700\n",
      "epoch 45, batch 100, train_loss 0.693\n",
      "epoch 45, batch 110, train_loss 0.699\n",
      "epoch 45, batch 120, train_loss 0.697\n",
      "epoch 45, batch 130, train_loss 0.691\n",
      "epoch 45, batch 140, train_loss 0.699\n",
      "epoch 45, batch 150, train_loss 0.696\n",
      "epoch 45, batch 160, train_loss 0.690\n",
      "epoch 45, batch 170, train_loss 0.686\n",
      "epoch 45, batch 180, train_loss 0.690\n",
      "epoch 45, batch 190, train_loss 0.693\n",
      "epoch 45, batch 200, train_loss 0.687\n",
      "epoch 45, batch 210, train_loss 0.698\n",
      "epoch 45, batch 220, train_loss 0.684\n",
      "epoch 45, batch 230, train_loss 0.689\n",
      "epoch 45, batch 240, train_loss 0.687\n",
      "epoch 45, batch 250, train_loss 0.697\n",
      "epoch 45, batch 260, train_loss 0.700\n",
      "epoch 45, batch 270, train_loss 0.693\n",
      "epoch 45, batch 280, train_loss 0.699\n",
      "epoch 45, batch 290, train_loss 0.695\n",
      "epoch 45, batch 300, train_loss 0.701\n",
      "epoch 45, batch 310, train_loss 0.701\n",
      "epoch 45, batch 320, train_loss 0.698\n",
      "epoch 45, batch 330, train_loss 0.695\n",
      "epoch 45, batch 340, train_loss 0.686\n",
      "epoch 45, batch 350, train_loss 0.698\n",
      "epoch 45, batch 360, train_loss 0.684\n",
      "epoch 45, batch 370, train_loss 0.687\n",
      "epoch 45, batch 380, train_loss 0.688\n",
      "epoch 45, batch 390, train_loss 0.694\n",
      "epoch 45, batch 400, train_loss 0.700\n",
      "epoch 45, batch 410, train_loss 0.691\n",
      "epoch 45, batch 420, train_loss 0.696\n",
      "epoch 45, batch 430, train_loss 0.695\n",
      "epoch 45, batch 440, train_loss 0.694\n",
      "epoch 45, batch 450, train_loss 0.694\n",
      "epoch 45, batch 460, train_loss 0.697\n",
      "epoch 45, batch 470, train_loss 0.701\n",
      "epoch 45, batch 480, train_loss 0.698\n",
      "epoch 45, batch 490, train_loss 0.696\n",
      "epoch 45, batch 500, train_loss 0.693\n",
      "epoch 45, batch 510, train_loss 0.686\n",
      "epoch 45, batch 520, train_loss 0.692\n",
      "epoch 45, batch 530, train_loss 0.694\n",
      "epoch 45, batch 540, train_loss 0.698\n",
      "epoch 45, batch 550, train_loss 0.689\n",
      "epoch 45, batch 560, train_loss 0.698\n",
      "epoch 45, batch 570, train_loss 0.691\n",
      "epoch 45, batch 580, train_loss 0.698\n",
      "epoch 45, batch 590, train_loss 0.688\n",
      "epoch 45, batch 600, train_loss 0.690\n",
      "epoch 45, batch 610, train_loss 0.688\n",
      "epoch 45, batch 620, train_loss 0.691\n",
      "epoch 45, batch 630, train_loss 0.699\n",
      "epoch 45, batch 640, train_loss 0.688\n",
      "epoch 45, batch 650, train_loss 0.710\n",
      "epoch 45, batch 660, train_loss 0.699\n",
      "epoch 45, batch 670, train_loss 0.688\n",
      "epoch 45, batch 680, train_loss 0.694\n",
      "epoch 45, batch 690, train_loss 0.694\n",
      "epoch 45, batch 700, train_loss 0.690\n",
      "epoch 45, batch 710, train_loss 0.705\n",
      "epoch 45, batch 720, train_loss 0.693\n",
      "epoch 45, batch 730, train_loss 0.690\n",
      "epoch 45, batch 740, train_loss 0.694\n",
      "epoch 45, batch 750, train_loss 0.691\n",
      "epoch 45, batch 760, train_loss 0.704\n",
      "epoch 45, batch 770, train_loss 0.696\n",
      "epoch 45, batch 780, train_loss 0.696\n",
      "epoch 45, batch 790, train_loss 0.702\n",
      "epoch 45, batch 800, train_loss 0.687\n",
      "epoch 45, batch 810, train_loss 0.694\n",
      "epoch 45, batch 820, train_loss 0.696\n",
      "epoch 45, batch 830, train_loss 0.694\n",
      "epoch 45, batch 840, train_loss 0.697\n",
      "epoch 45, batch 850, train_loss 0.699\n",
      "epoch 45, batch 860, train_loss 0.703\n",
      "epoch 45, batch 870, train_loss 0.702\n",
      "epoch 45, batch 880, train_loss 0.704\n",
      "epoch 45, batch 890, train_loss 0.685\n",
      "epoch 45, batch 900, train_loss 0.691\n",
      "epoch 45, batch 910, train_loss 0.694\n",
      "epoch 45, batch 920, train_loss 0.688\n",
      "epoch 45, batch 930, train_loss 0.686\n",
      "epoch 45, batch 940, train_loss 0.691\n",
      "epoch 45, batch 950, train_loss 0.696\n",
      "epoch 45, batch 960, train_loss 0.692\n",
      "epoch 45, batch 970, train_loss 0.702\n",
      "epoch 45, batch 980, train_loss 0.689\n",
      "epoch 45, batch 990, train_loss 0.690\n",
      "epoch 45, batch 1000, train_loss 0.696\n",
      "epoch 45, batch 1010, train_loss 0.704\n",
      "epoch 45, batch 1020, train_loss 0.691\n",
      "epoch 45, batch 1030, train_loss 0.692\n",
      "epoch 45, batch 1040, train_loss 0.700\n",
      "epoch 45, batch 1050, train_loss 0.691\n",
      "epoch 45, batch 1060, train_loss 0.699\n",
      "epoch 45, batch 1070, train_loss 0.687\n",
      "epoch 45, batch 1080, train_loss 0.704\n",
      "epoch 45, batch 1090, train_loss 0.701\n",
      "epoch 45, batch 1100, train_loss 0.694\n",
      "epoch 45, batch 1110, train_loss 0.689\n",
      "epoch 45, batch 1120, train_loss 0.691\n",
      "epoch 45, batch 1130, train_loss 0.697\n",
      "epoch 45, batch 1140, train_loss 0.689\n",
      "epoch 45, batch 1150, train_loss 0.686\n",
      "epoch 45, batch 1160, train_loss 0.686\n",
      "epoch 45, batch 1170, train_loss 0.688\n",
      "epoch 45, batch 1180, train_loss 0.700\n",
      "epoch 45, batch 1190, train_loss 0.695\n",
      "epoch    45, train_loss 0.692, valid_loss 0.722, train_accuracy  70.28%, valid_accuracy  68.58%\n",
      "epoch 46, batch 0, train_loss 0.696\n",
      "epoch 46, batch 10, train_loss 0.696\n",
      "epoch 46, batch 20, train_loss 0.696\n",
      "epoch 46, batch 30, train_loss 0.690\n",
      "epoch 46, batch 40, train_loss 0.691\n",
      "epoch 46, batch 50, train_loss 0.701\n",
      "epoch 46, batch 60, train_loss 0.693\n",
      "epoch 46, batch 70, train_loss 0.689\n",
      "epoch 46, batch 80, train_loss 0.683\n",
      "epoch 46, batch 90, train_loss 0.687\n",
      "epoch 46, batch 100, train_loss 0.695\n",
      "epoch 46, batch 110, train_loss 0.702\n",
      "epoch 46, batch 120, train_loss 0.690\n",
      "epoch 46, batch 130, train_loss 0.698\n",
      "epoch 46, batch 140, train_loss 0.706\n",
      "epoch 46, batch 150, train_loss 0.691\n",
      "epoch 46, batch 160, train_loss 0.684\n",
      "epoch 46, batch 170, train_loss 0.695\n",
      "epoch 46, batch 180, train_loss 0.701\n",
      "epoch 46, batch 190, train_loss 0.688\n",
      "epoch 46, batch 200, train_loss 0.693\n",
      "epoch 46, batch 210, train_loss 0.700\n",
      "epoch 46, batch 220, train_loss 0.699\n",
      "epoch 46, batch 230, train_loss 0.693\n",
      "epoch 46, batch 240, train_loss 0.697\n",
      "epoch 46, batch 250, train_loss 0.692\n",
      "epoch 46, batch 260, train_loss 0.685\n",
      "epoch 46, batch 270, train_loss 0.685\n",
      "epoch 46, batch 280, train_loss 0.693\n",
      "epoch 46, batch 290, train_loss 0.689\n",
      "epoch 46, batch 300, train_loss 0.691\n",
      "epoch 46, batch 310, train_loss 0.701\n",
      "epoch 46, batch 320, train_loss 0.690\n",
      "epoch 46, batch 330, train_loss 0.692\n",
      "epoch 46, batch 340, train_loss 0.685\n",
      "epoch 46, batch 350, train_loss 0.690\n",
      "epoch 46, batch 360, train_loss 0.703\n",
      "epoch 46, batch 370, train_loss 0.694\n",
      "epoch 46, batch 380, train_loss 0.695\n",
      "epoch 46, batch 390, train_loss 0.696\n",
      "epoch 46, batch 400, train_loss 0.701\n",
      "epoch 46, batch 410, train_loss 0.696\n",
      "epoch 46, batch 420, train_loss 0.688\n",
      "epoch 46, batch 430, train_loss 0.692\n",
      "epoch 46, batch 440, train_loss 0.699\n",
      "epoch 46, batch 450, train_loss 0.700\n",
      "epoch 46, batch 460, train_loss 0.690\n",
      "epoch 46, batch 470, train_loss 0.690\n",
      "epoch 46, batch 480, train_loss 0.692\n",
      "epoch 46, batch 490, train_loss 0.689\n",
      "epoch 46, batch 500, train_loss 0.684\n",
      "epoch 46, batch 510, train_loss 0.691\n",
      "epoch 46, batch 520, train_loss 0.694\n",
      "epoch 46, batch 530, train_loss 0.695\n",
      "epoch 46, batch 540, train_loss 0.695\n",
      "epoch 46, batch 550, train_loss 0.700\n",
      "epoch 46, batch 560, train_loss 0.696\n",
      "epoch 46, batch 570, train_loss 0.697\n",
      "epoch 46, batch 580, train_loss 0.693\n",
      "epoch 46, batch 590, train_loss 0.682\n",
      "epoch 46, batch 600, train_loss 0.698\n",
      "epoch 46, batch 610, train_loss 0.696\n",
      "epoch 46, batch 620, train_loss 0.689\n",
      "epoch 46, batch 630, train_loss 0.704\n",
      "epoch 46, batch 640, train_loss 0.689\n",
      "epoch 46, batch 650, train_loss 0.694\n",
      "epoch 46, batch 660, train_loss 0.701\n",
      "epoch 46, batch 670, train_loss 0.702\n",
      "epoch 46, batch 680, train_loss 0.689\n",
      "epoch 46, batch 690, train_loss 0.698\n",
      "epoch 46, batch 700, train_loss 0.700\n",
      "epoch 46, batch 710, train_loss 0.692\n",
      "epoch 46, batch 720, train_loss 0.693\n",
      "epoch 46, batch 730, train_loss 0.694\n",
      "epoch 46, batch 740, train_loss 0.690\n",
      "epoch 46, batch 750, train_loss 0.685\n",
      "epoch 46, batch 760, train_loss 0.701\n",
      "epoch 46, batch 770, train_loss 0.691\n",
      "epoch 46, batch 780, train_loss 0.680\n",
      "epoch 46, batch 790, train_loss 0.694\n",
      "epoch 46, batch 800, train_loss 0.695\n",
      "epoch 46, batch 810, train_loss 0.698\n",
      "epoch 46, batch 820, train_loss 0.692\n",
      "epoch 46, batch 830, train_loss 0.700\n",
      "epoch 46, batch 840, train_loss 0.692\n",
      "epoch 46, batch 850, train_loss 0.695\n",
      "epoch 46, batch 860, train_loss 0.692\n",
      "epoch 46, batch 870, train_loss 0.695\n",
      "epoch 46, batch 880, train_loss 0.685\n",
      "epoch 46, batch 890, train_loss 0.692\n",
      "epoch 46, batch 900, train_loss 0.689\n",
      "epoch 46, batch 910, train_loss 0.698\n",
      "epoch 46, batch 920, train_loss 0.696\n",
      "epoch 46, batch 930, train_loss 0.694\n",
      "epoch 46, batch 940, train_loss 0.696\n",
      "epoch 46, batch 950, train_loss 0.693\n",
      "epoch 46, batch 960, train_loss 0.693\n",
      "epoch 46, batch 970, train_loss 0.692\n",
      "epoch 46, batch 980, train_loss 0.706\n",
      "epoch 46, batch 990, train_loss 0.694\n",
      "epoch 46, batch 1000, train_loss 0.677\n",
      "epoch 46, batch 1010, train_loss 0.698\n",
      "epoch 46, batch 1020, train_loss 0.690\n",
      "epoch 46, batch 1030, train_loss 0.703\n",
      "epoch 46, batch 1040, train_loss 0.695\n",
      "epoch 46, batch 1050, train_loss 0.700\n",
      "epoch 46, batch 1060, train_loss 0.696\n",
      "epoch 46, batch 1070, train_loss 0.684\n",
      "epoch 46, batch 1080, train_loss 0.685\n",
      "epoch 46, batch 1090, train_loss 0.694\n",
      "epoch 46, batch 1100, train_loss 0.699\n",
      "epoch 46, batch 1110, train_loss 0.692\n",
      "epoch 46, batch 1120, train_loss 0.682\n",
      "epoch 46, batch 1130, train_loss 0.693\n",
      "epoch 46, batch 1140, train_loss 0.698\n",
      "epoch 46, batch 1150, train_loss 0.686\n",
      "epoch 46, batch 1160, train_loss 0.695\n",
      "epoch 46, batch 1170, train_loss 0.694\n",
      "epoch 46, batch 1180, train_loss 0.697\n",
      "epoch 46, batch 1190, train_loss 0.699\n",
      "epoch    46, train_loss 0.692, valid_loss 0.722, train_accuracy  70.27%, valid_accuracy  68.58%\n",
      "epoch 47, batch 0, train_loss 0.688\n",
      "epoch 47, batch 10, train_loss 0.690\n",
      "epoch 47, batch 20, train_loss 0.695\n",
      "epoch 47, batch 30, train_loss 0.693\n",
      "epoch 47, batch 40, train_loss 0.689\n",
      "epoch 47, batch 50, train_loss 0.688\n",
      "epoch 47, batch 60, train_loss 0.686\n",
      "epoch 47, batch 70, train_loss 0.694\n",
      "epoch 47, batch 80, train_loss 0.693\n",
      "epoch 47, batch 90, train_loss 0.685\n",
      "epoch 47, batch 100, train_loss 0.688\n",
      "epoch 47, batch 110, train_loss 0.694\n",
      "epoch 47, batch 120, train_loss 0.694\n",
      "epoch 47, batch 130, train_loss 0.698\n",
      "epoch 47, batch 140, train_loss 0.694\n",
      "epoch 47, batch 150, train_loss 0.694\n",
      "epoch 47, batch 160, train_loss 0.690\n",
      "epoch 47, batch 170, train_loss 0.693\n",
      "epoch 47, batch 180, train_loss 0.682\n",
      "epoch 47, batch 190, train_loss 0.699\n",
      "epoch 47, batch 200, train_loss 0.683\n",
      "epoch 47, batch 210, train_loss 0.702\n",
      "epoch 47, batch 220, train_loss 0.693\n",
      "epoch 47, batch 230, train_loss 0.687\n",
      "epoch 47, batch 240, train_loss 0.691\n",
      "epoch 47, batch 250, train_loss 0.697\n",
      "epoch 47, batch 260, train_loss 0.693\n",
      "epoch 47, batch 270, train_loss 0.690\n",
      "epoch 47, batch 280, train_loss 0.694\n",
      "epoch 47, batch 290, train_loss 0.691\n",
      "epoch 47, batch 300, train_loss 0.700\n",
      "epoch 47, batch 310, train_loss 0.691\n",
      "epoch 47, batch 320, train_loss 0.693\n",
      "epoch 47, batch 330, train_loss 0.692\n",
      "epoch 47, batch 340, train_loss 0.692\n",
      "epoch 47, batch 350, train_loss 0.689\n",
      "epoch 47, batch 360, train_loss 0.692\n",
      "epoch 47, batch 370, train_loss 0.698\n",
      "epoch 47, batch 380, train_loss 0.695\n",
      "epoch 47, batch 390, train_loss 0.688\n",
      "epoch 47, batch 400, train_loss 0.700\n",
      "epoch 47, batch 410, train_loss 0.698\n",
      "epoch 47, batch 420, train_loss 0.705\n",
      "epoch 47, batch 430, train_loss 0.691\n",
      "epoch 47, batch 440, train_loss 0.693\n",
      "epoch 47, batch 450, train_loss 0.699\n",
      "epoch 47, batch 460, train_loss 0.693\n",
      "epoch 47, batch 470, train_loss 0.693\n",
      "epoch 47, batch 480, train_loss 0.706\n",
      "epoch 47, batch 490, train_loss 0.691\n",
      "epoch 47, batch 500, train_loss 0.681\n",
      "epoch 47, batch 510, train_loss 0.693\n",
      "epoch 47, batch 520, train_loss 0.700\n",
      "epoch 47, batch 530, train_loss 0.698\n",
      "epoch 47, batch 540, train_loss 0.689\n",
      "epoch 47, batch 550, train_loss 0.678\n",
      "epoch 47, batch 560, train_loss 0.701\n",
      "epoch 47, batch 570, train_loss 0.695\n",
      "epoch 47, batch 580, train_loss 0.693\n",
      "epoch 47, batch 590, train_loss 0.688\n",
      "epoch 47, batch 600, train_loss 0.700\n",
      "epoch 47, batch 610, train_loss 0.690\n",
      "epoch 47, batch 620, train_loss 0.678\n",
      "epoch 47, batch 630, train_loss 0.699\n",
      "epoch 47, batch 640, train_loss 0.694\n",
      "epoch 47, batch 650, train_loss 0.705\n",
      "epoch 47, batch 660, train_loss 0.692\n",
      "epoch 47, batch 670, train_loss 0.693\n",
      "epoch 47, batch 680, train_loss 0.694\n",
      "epoch 47, batch 690, train_loss 0.693\n",
      "epoch 47, batch 700, train_loss 0.693\n",
      "epoch 47, batch 710, train_loss 0.705\n",
      "epoch 47, batch 720, train_loss 0.694\n",
      "epoch 47, batch 730, train_loss 0.685\n",
      "epoch 47, batch 740, train_loss 0.689\n",
      "epoch 47, batch 750, train_loss 0.688\n",
      "epoch 47, batch 760, train_loss 0.689\n",
      "epoch 47, batch 770, train_loss 0.693\n",
      "epoch 47, batch 780, train_loss 0.693\n",
      "epoch 47, batch 790, train_loss 0.693\n",
      "epoch 47, batch 800, train_loss 0.700\n",
      "epoch 47, batch 810, train_loss 0.704\n",
      "epoch 47, batch 820, train_loss 0.699\n",
      "epoch 47, batch 830, train_loss 0.691\n",
      "epoch 47, batch 840, train_loss 0.693\n",
      "epoch 47, batch 850, train_loss 0.694\n",
      "epoch 47, batch 860, train_loss 0.698\n",
      "epoch 47, batch 870, train_loss 0.696\n",
      "epoch 47, batch 880, train_loss 0.692\n",
      "epoch 47, batch 890, train_loss 0.703\n",
      "epoch 47, batch 900, train_loss 0.698\n",
      "epoch 47, batch 910, train_loss 0.692\n",
      "epoch 47, batch 920, train_loss 0.697\n",
      "epoch 47, batch 930, train_loss 0.701\n",
      "epoch 47, batch 940, train_loss 0.686\n",
      "epoch 47, batch 950, train_loss 0.697\n",
      "epoch 47, batch 960, train_loss 0.688\n",
      "epoch 47, batch 970, train_loss 0.686\n",
      "epoch 47, batch 980, train_loss 0.695\n",
      "epoch 47, batch 990, train_loss 0.695\n",
      "epoch 47, batch 1000, train_loss 0.693\n",
      "epoch 47, batch 1010, train_loss 0.698\n",
      "epoch 47, batch 1020, train_loss 0.685\n",
      "epoch 47, batch 1030, train_loss 0.697\n",
      "epoch 47, batch 1040, train_loss 0.695\n",
      "epoch 47, batch 1050, train_loss 0.692\n",
      "epoch 47, batch 1060, train_loss 0.699\n",
      "epoch 47, batch 1070, train_loss 0.696\n",
      "epoch 47, batch 1080, train_loss 0.681\n",
      "epoch 47, batch 1090, train_loss 0.688\n",
      "epoch 47, batch 1100, train_loss 0.695\n",
      "epoch 47, batch 1110, train_loss 0.694\n",
      "epoch 47, batch 1120, train_loss 0.700\n",
      "epoch 47, batch 1130, train_loss 0.691\n",
      "epoch 47, batch 1140, train_loss 0.691\n",
      "epoch 47, batch 1150, train_loss 0.693\n",
      "epoch 47, batch 1160, train_loss 0.702\n",
      "epoch 47, batch 1170, train_loss 0.700\n",
      "epoch 47, batch 1180, train_loss 0.703\n",
      "epoch 47, batch 1190, train_loss 0.682\n",
      "epoch    47, train_loss 0.692, valid_loss 0.723, train_accuracy  70.27%, valid_accuracy  68.58%\n",
      "epoch 48, batch 0, train_loss 0.690\n",
      "epoch 48, batch 10, train_loss 0.692\n",
      "epoch 48, batch 20, train_loss 0.691\n",
      "epoch 48, batch 30, train_loss 0.691\n",
      "epoch 48, batch 40, train_loss 0.695\n",
      "epoch 48, batch 50, train_loss 0.694\n",
      "epoch 48, batch 60, train_loss 0.699\n",
      "epoch 48, batch 70, train_loss 0.705\n",
      "epoch 48, batch 80, train_loss 0.690\n",
      "epoch 48, batch 90, train_loss 0.694\n",
      "epoch 48, batch 100, train_loss 0.683\n",
      "epoch 48, batch 110, train_loss 0.689\n",
      "epoch 48, batch 120, train_loss 0.695\n",
      "epoch 48, batch 130, train_loss 0.683\n",
      "epoch 48, batch 140, train_loss 0.696\n",
      "epoch 48, batch 150, train_loss 0.689\n",
      "epoch 48, batch 160, train_loss 0.690\n",
      "epoch 48, batch 170, train_loss 0.691\n",
      "epoch 48, batch 180, train_loss 0.691\n",
      "epoch 48, batch 190, train_loss 0.693\n",
      "epoch 48, batch 200, train_loss 0.698\n",
      "epoch 48, batch 210, train_loss 0.697\n",
      "epoch 48, batch 220, train_loss 0.689\n",
      "epoch 48, batch 230, train_loss 0.699\n",
      "epoch 48, batch 240, train_loss 0.688\n",
      "epoch 48, batch 250, train_loss 0.703\n",
      "epoch 48, batch 260, train_loss 0.700\n",
      "epoch 48, batch 270, train_loss 0.691\n",
      "epoch 48, batch 280, train_loss 0.704\n",
      "epoch 48, batch 290, train_loss 0.689\n",
      "epoch 48, batch 300, train_loss 0.695\n",
      "epoch 48, batch 310, train_loss 0.692\n",
      "epoch 48, batch 320, train_loss 0.692\n",
      "epoch 48, batch 330, train_loss 0.697\n",
      "epoch 48, batch 340, train_loss 0.687\n",
      "epoch 48, batch 350, train_loss 0.689\n",
      "epoch 48, batch 360, train_loss 0.692\n",
      "epoch 48, batch 370, train_loss 0.700\n",
      "epoch 48, batch 380, train_loss 0.696\n",
      "epoch 48, batch 390, train_loss 0.706\n",
      "epoch 48, batch 400, train_loss 0.699\n",
      "epoch 48, batch 410, train_loss 0.695\n",
      "epoch 48, batch 420, train_loss 0.675\n",
      "epoch 48, batch 430, train_loss 0.696\n",
      "epoch 48, batch 440, train_loss 0.689\n",
      "epoch 48, batch 450, train_loss 0.698\n",
      "epoch 48, batch 460, train_loss 0.698\n",
      "epoch 48, batch 470, train_loss 0.692\n",
      "epoch 48, batch 480, train_loss 0.688\n",
      "epoch 48, batch 490, train_loss 0.700\n",
      "epoch 48, batch 500, train_loss 0.707\n",
      "epoch 48, batch 510, train_loss 0.685\n",
      "epoch 48, batch 520, train_loss 0.679\n",
      "epoch 48, batch 530, train_loss 0.696\n",
      "epoch 48, batch 540, train_loss 0.700\n",
      "epoch 48, batch 550, train_loss 0.697\n",
      "epoch 48, batch 560, train_loss 0.689\n",
      "epoch 48, batch 570, train_loss 0.693\n",
      "epoch 48, batch 580, train_loss 0.683\n",
      "epoch 48, batch 590, train_loss 0.691\n",
      "epoch 48, batch 600, train_loss 0.694\n",
      "epoch 48, batch 610, train_loss 0.698\n",
      "epoch 48, batch 620, train_loss 0.693\n",
      "epoch 48, batch 630, train_loss 0.693\n",
      "epoch 48, batch 640, train_loss 0.693\n",
      "epoch 48, batch 650, train_loss 0.706\n",
      "epoch 48, batch 660, train_loss 0.691\n",
      "epoch 48, batch 670, train_loss 0.697\n",
      "epoch 48, batch 680, train_loss 0.692\n",
      "epoch 48, batch 690, train_loss 0.700\n",
      "epoch 48, batch 700, train_loss 0.692\n",
      "epoch 48, batch 710, train_loss 0.691\n",
      "epoch 48, batch 720, train_loss 0.694\n",
      "epoch 48, batch 730, train_loss 0.681\n",
      "epoch 48, batch 740, train_loss 0.697\n",
      "epoch 48, batch 750, train_loss 0.701\n",
      "epoch 48, batch 760, train_loss 0.698\n",
      "epoch 48, batch 770, train_loss 0.696\n",
      "epoch 48, batch 780, train_loss 0.687\n",
      "epoch 48, batch 790, train_loss 0.688\n",
      "epoch 48, batch 800, train_loss 0.686\n",
      "epoch 48, batch 810, train_loss 0.695\n",
      "epoch 48, batch 820, train_loss 0.686\n",
      "epoch 48, batch 830, train_loss 0.686\n",
      "epoch 48, batch 840, train_loss 0.695\n",
      "epoch 48, batch 850, train_loss 0.694\n",
      "epoch 48, batch 860, train_loss 0.701\n",
      "epoch 48, batch 870, train_loss 0.690\n",
      "epoch 48, batch 880, train_loss 0.683\n",
      "epoch 48, batch 890, train_loss 0.679\n",
      "epoch 48, batch 900, train_loss 0.690\n",
      "epoch 48, batch 910, train_loss 0.696\n",
      "epoch 48, batch 920, train_loss 0.691\n",
      "epoch 48, batch 930, train_loss 0.693\n",
      "epoch 48, batch 940, train_loss 0.687\n",
      "epoch 48, batch 950, train_loss 0.687\n",
      "epoch 48, batch 960, train_loss 0.698\n",
      "epoch 48, batch 970, train_loss 0.692\n",
      "epoch 48, batch 980, train_loss 0.705\n",
      "epoch 48, batch 990, train_loss 0.700\n",
      "epoch 48, batch 1000, train_loss 0.693\n",
      "epoch 48, batch 1010, train_loss 0.682\n",
      "epoch 48, batch 1020, train_loss 0.697\n",
      "epoch 48, batch 1030, train_loss 0.702\n",
      "epoch 48, batch 1040, train_loss 0.706\n",
      "epoch 48, batch 1050, train_loss 0.693\n",
      "epoch 48, batch 1060, train_loss 0.702\n",
      "epoch 48, batch 1070, train_loss 0.694\n",
      "epoch 48, batch 1080, train_loss 0.693\n",
      "epoch 48, batch 1090, train_loss 0.688\n",
      "epoch 48, batch 1100, train_loss 0.677\n",
      "epoch 48, batch 1110, train_loss 0.697\n",
      "epoch 48, batch 1120, train_loss 0.696\n",
      "epoch 48, batch 1130, train_loss 0.696\n",
      "epoch 48, batch 1140, train_loss 0.684\n",
      "epoch 48, batch 1150, train_loss 0.690\n",
      "epoch 48, batch 1160, train_loss 0.681\n",
      "epoch 48, batch 1170, train_loss 0.686\n",
      "epoch 48, batch 1180, train_loss 0.697\n",
      "epoch 48, batch 1190, train_loss 0.692\n",
      "epoch    48, train_loss 0.695, valid_loss 0.725, train_accuracy  70.14%, valid_accuracy  68.47%\n",
      "epoch 49, batch 0, train_loss 0.689\n",
      "epoch 49, batch 10, train_loss 0.694\n",
      "epoch 49, batch 20, train_loss 0.691\n",
      "epoch 49, batch 30, train_loss 0.702\n",
      "epoch 49, batch 40, train_loss 0.692\n",
      "epoch 49, batch 50, train_loss 0.691\n",
      "epoch 49, batch 60, train_loss 0.686\n",
      "epoch 49, batch 70, train_loss 0.696\n",
      "epoch 49, batch 80, train_loss 0.688\n",
      "epoch 49, batch 90, train_loss 0.695\n",
      "epoch 49, batch 100, train_loss 0.698\n",
      "epoch 49, batch 110, train_loss 0.710\n",
      "epoch 49, batch 120, train_loss 0.694\n",
      "epoch 49, batch 130, train_loss 0.695\n",
      "epoch 49, batch 140, train_loss 0.697\n",
      "epoch 49, batch 150, train_loss 0.690\n",
      "epoch 49, batch 160, train_loss 0.697\n",
      "epoch 49, batch 170, train_loss 0.693\n",
      "epoch 49, batch 180, train_loss 0.698\n",
      "epoch 49, batch 190, train_loss 0.691\n",
      "epoch 49, batch 200, train_loss 0.694\n",
      "epoch 49, batch 210, train_loss 0.699\n",
      "epoch 49, batch 220, train_loss 0.706\n",
      "epoch 49, batch 230, train_loss 0.691\n",
      "epoch 49, batch 240, train_loss 0.699\n",
      "epoch 49, batch 250, train_loss 0.700\n",
      "epoch 49, batch 260, train_loss 0.686\n",
      "epoch 49, batch 270, train_loss 0.697\n",
      "epoch 49, batch 280, train_loss 0.693\n",
      "epoch 49, batch 290, train_loss 0.694\n",
      "epoch 49, batch 300, train_loss 0.699\n",
      "epoch 49, batch 310, train_loss 0.696\n",
      "epoch 49, batch 320, train_loss 0.694\n",
      "epoch 49, batch 330, train_loss 0.693\n",
      "epoch 49, batch 340, train_loss 0.694\n",
      "epoch 49, batch 350, train_loss 0.690\n",
      "epoch 49, batch 360, train_loss 0.689\n",
      "epoch 49, batch 370, train_loss 0.704\n",
      "epoch 49, batch 380, train_loss 0.695\n",
      "epoch 49, batch 390, train_loss 0.696\n",
      "epoch 49, batch 400, train_loss 0.703\n",
      "epoch 49, batch 410, train_loss 0.699\n",
      "epoch 49, batch 420, train_loss 0.691\n",
      "epoch 49, batch 430, train_loss 0.692\n",
      "epoch 49, batch 440, train_loss 0.702\n",
      "epoch 49, batch 450, train_loss 0.695\n",
      "epoch 49, batch 460, train_loss 0.686\n",
      "epoch 49, batch 470, train_loss 0.702\n",
      "epoch 49, batch 480, train_loss 0.696\n",
      "epoch 49, batch 490, train_loss 0.701\n",
      "epoch 49, batch 500, train_loss 0.689\n",
      "epoch 49, batch 510, train_loss 0.688\n",
      "epoch 49, batch 520, train_loss 0.692\n",
      "epoch 49, batch 530, train_loss 0.691\n",
      "epoch 49, batch 540, train_loss 0.692\n",
      "epoch 49, batch 550, train_loss 0.685\n",
      "epoch 49, batch 560, train_loss 0.697\n",
      "epoch 49, batch 570, train_loss 0.699\n",
      "epoch 49, batch 580, train_loss 0.698\n",
      "epoch 49, batch 590, train_loss 0.692\n",
      "epoch 49, batch 600, train_loss 0.688\n",
      "epoch 49, batch 610, train_loss 0.690\n",
      "epoch 49, batch 620, train_loss 0.692\n",
      "epoch 49, batch 630, train_loss 0.698\n",
      "epoch 49, batch 640, train_loss 0.693\n",
      "epoch 49, batch 650, train_loss 0.685\n",
      "epoch 49, batch 660, train_loss 0.698\n",
      "epoch 49, batch 670, train_loss 0.697\n",
      "epoch 49, batch 680, train_loss 0.697\n",
      "epoch 49, batch 690, train_loss 0.701\n",
      "epoch 49, batch 700, train_loss 0.689\n",
      "epoch 49, batch 710, train_loss 0.693\n",
      "epoch 49, batch 720, train_loss 0.705\n",
      "epoch 49, batch 730, train_loss 0.689\n",
      "epoch 49, batch 740, train_loss 0.701\n",
      "epoch 49, batch 750, train_loss 0.698\n",
      "epoch 49, batch 760, train_loss 0.705\n",
      "epoch 49, batch 770, train_loss 0.698\n",
      "epoch 49, batch 780, train_loss 0.704\n",
      "epoch 49, batch 790, train_loss 0.688\n",
      "epoch 49, batch 800, train_loss 0.689\n",
      "epoch 49, batch 810, train_loss 0.683\n",
      "epoch 49, batch 820, train_loss 0.698\n",
      "epoch 49, batch 830, train_loss 0.697\n",
      "epoch 49, batch 840, train_loss 0.683\n",
      "epoch 49, batch 850, train_loss 0.689\n",
      "epoch 49, batch 860, train_loss 0.684\n",
      "epoch 49, batch 870, train_loss 0.699\n",
      "epoch 49, batch 880, train_loss 0.690\n",
      "epoch 49, batch 890, train_loss 0.687\n",
      "epoch 49, batch 900, train_loss 0.695\n",
      "epoch 49, batch 910, train_loss 0.697\n",
      "epoch 49, batch 920, train_loss 0.695\n",
      "epoch 49, batch 930, train_loss 0.690\n",
      "epoch 49, batch 940, train_loss 0.683\n",
      "epoch 49, batch 950, train_loss 0.694\n",
      "epoch 49, batch 960, train_loss 0.695\n",
      "epoch 49, batch 970, train_loss 0.691\n",
      "epoch 49, batch 980, train_loss 0.697\n",
      "epoch 49, batch 990, train_loss 0.696\n",
      "epoch 49, batch 1000, train_loss 0.701\n",
      "epoch 49, batch 1010, train_loss 0.701\n",
      "epoch 49, batch 1020, train_loss 0.695\n",
      "epoch 49, batch 1030, train_loss 0.690\n",
      "epoch 49, batch 1040, train_loss 0.688\n",
      "epoch 49, batch 1050, train_loss 0.692\n",
      "epoch 49, batch 1060, train_loss 0.693\n",
      "epoch 49, batch 1070, train_loss 0.697\n",
      "epoch 49, batch 1080, train_loss 0.692\n",
      "epoch 49, batch 1090, train_loss 0.685\n",
      "epoch 49, batch 1100, train_loss 0.695\n",
      "epoch 49, batch 1110, train_loss 0.691\n",
      "epoch 49, batch 1120, train_loss 0.690\n",
      "epoch 49, batch 1130, train_loss 0.684\n",
      "epoch 49, batch 1140, train_loss 0.696\n",
      "epoch 49, batch 1150, train_loss 0.691\n",
      "epoch 49, batch 1160, train_loss 0.698\n",
      "epoch 49, batch 1170, train_loss 0.690\n",
      "epoch 49, batch 1180, train_loss 0.698\n",
      "epoch 49, batch 1190, train_loss 0.697\n",
      "epoch    49, train_loss 0.692, valid_loss 0.722, train_accuracy  70.26%, valid_accuracy  68.59%\n"
     ]
    }
   ],
   "source": [
    "## set the number of epochs and batch size\n",
    "## you can change both as needed\n",
    "num_epochs = 50\n",
    "batch_size = 1024 * 16\n",
    "\n",
    "train_loss_record = []\n",
    "valid_loss_record = []\n",
    "train_accuracy_record = []\n",
    "valid_accuracy_record = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    idx = np.arange(len(train_xs))\n",
    "    np.random.shuffle(idx)\n",
    "    train_xs = train_xs[idx]\n",
    "    train_ys = train_ys[idx]\n",
    "\n",
    "    num_batches = len(train_xs) // batch_size\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = train_xs[start_idx:end_idx]\n",
    "        batch_ys = train_ys[start_idx:end_idx]\n",
    "\n",
    "        model, opt_state, loss_value = make_step(model, batch_xs, batch_ys, opt_state)\n",
    "\n",
    "        if idx_batch % 10 == 0:\n",
    "            print(f\"epoch {epoch:5>d}, batch {idx_batch:5>d}, train_loss {loss_value:5.3f}\")\n",
    "\n",
    "    \n",
    "    train_loss = compute_average_loss(model, train_xs, train_ys)\n",
    "    train_accuracy = compute_accuracy(model, train_xs, train_ys)\n",
    "\n",
    "    valid_loss = compute_average_loss(model, valid_xs, valid_ys)\n",
    "    valid_accuracy = compute_accuracy(model, valid_xs, valid_ys)\n",
    "    \n",
    "    print(f\"epoch {epoch:>5d}, train_loss {train_loss:5.3f}, valid_loss {valid_loss:5.3f}, train_accuracy {train_accuracy:7.2%}, valid_accuracy {valid_accuracy:7.2%}\")\n",
    "\n",
    "    train_loss_record.append(train_loss)\n",
    "    valid_loss_record.append(valid_loss)\n",
    "    train_accuracy_record.append(train_accuracy)\n",
    "    valid_accuracy_record.append(valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and accuracy curves during training\n",
    "To visualize the training process, we plot the loss and accuracy curves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(train_loss_record, label=\"train_loss\")\n",
    "plt.plot(valid_loss_record, label=\"valid_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(train_accuracy_record, label=\"train_accuracy\")\n",
    "plt.plot(valid_accuracy_record, label=\"valid_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the loss and accuracy curves, answer the following question**:\n",
    "\n",
    "As the number of epochs increases, what happens to the loss and accuracy on the training and validation datasets? Why the behavior of the loss/accuracy curves is different on the training and validation datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer to the above question_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model with the highest validation accuracy\n",
    "To avoid overfitting, we save the model with the highest validation accuracy after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0, train_loss 1.128\n",
      "epoch 0, batch 10, train_loss 1.036\n",
      "epoch 0, batch 20, train_loss 0.986\n",
      "epoch 0, batch 30, train_loss 0.954\n",
      "epoch 0, batch 40, train_loss 0.925\n",
      "epoch 0, batch 50, train_loss 0.906\n",
      "epoch 0, batch 60, train_loss 0.895\n",
      "epoch 0, batch 70, train_loss 0.877\n",
      "epoch 0, batch 80, train_loss 0.861\n",
      "epoch 0, batch 90, train_loss 0.858\n",
      "epoch 0, batch 100, train_loss 0.850\n",
      "epoch 0, batch 110, train_loss 0.841\n",
      "epoch 0, batch 120, train_loss 0.828\n",
      "epoch 0, batch 130, train_loss 0.839\n",
      "epoch 0, batch 140, train_loss 0.836\n",
      "epoch 0, batch 150, train_loss 0.825\n",
      "epoch 0, batch 160, train_loss 0.819\n",
      "epoch 0, batch 170, train_loss 0.822\n",
      "epoch 0, batch 180, train_loss 0.821\n",
      "epoch 0, batch 190, train_loss 0.812\n",
      "epoch 0, batch 200, train_loss 0.807\n",
      "epoch 0, batch 210, train_loss 0.809\n",
      "epoch 0, batch 220, train_loss 0.808\n",
      "epoch 0, batch 230, train_loss 0.803\n",
      "epoch 0, batch 240, train_loss 0.793\n",
      "epoch 0, batch 250, train_loss 0.783\n",
      "epoch 0, batch 260, train_loss 0.793\n",
      "epoch 0, batch 270, train_loss 0.787\n",
      "epoch 0, batch 280, train_loss 0.780\n",
      "epoch 0, batch 290, train_loss 0.780\n",
      "epoch 0, batch 300, train_loss 0.775\n",
      "epoch 0, batch 310, train_loss 0.775\n",
      "epoch 0, batch 320, train_loss 0.775\n",
      "epoch 0, batch 330, train_loss 0.774\n",
      "epoch 0, batch 340, train_loss 0.763\n",
      "epoch 0, batch 350, train_loss 0.766\n",
      "epoch 0, batch 360, train_loss 0.768\n",
      "epoch 0, batch 370, train_loss 0.756\n",
      "epoch 0, batch 380, train_loss 0.766\n",
      "epoch 0, batch 390, train_loss 0.757\n",
      "epoch 0, batch 400, train_loss 0.756\n",
      "epoch 0, batch 410, train_loss 0.769\n",
      "epoch 0, batch 420, train_loss 0.760\n",
      "epoch 0, batch 430, train_loss 0.757\n",
      "epoch 0, batch 440, train_loss 0.751\n",
      "epoch 0, batch 450, train_loss 0.751\n",
      "epoch 0, batch 460, train_loss 0.758\n",
      "epoch 0, batch 470, train_loss 0.763\n",
      "epoch 0, batch 480, train_loss 0.758\n",
      "epoch 0, batch 490, train_loss 0.761\n",
      "epoch 0, batch 500, train_loss 0.746\n",
      "epoch 0, batch 510, train_loss 0.749\n",
      "epoch 0, batch 520, train_loss 0.752\n",
      "epoch 0, batch 530, train_loss 0.756\n",
      "epoch 0, batch 540, train_loss 0.756\n",
      "epoch 0, batch 550, train_loss 0.752\n",
      "epoch 0, batch 560, train_loss 0.747\n",
      "epoch 0, batch 570, train_loss 0.749\n",
      "epoch 0, batch 580, train_loss 0.746\n",
      "epoch 0, batch 590, train_loss 0.750\n",
      "epoch 0, batch 600, train_loss 0.753\n",
      "epoch 0, batch 610, train_loss 0.746\n",
      "epoch 0, batch 620, train_loss 0.743\n",
      "epoch 0, batch 630, train_loss 0.741\n",
      "epoch 0, batch 640, train_loss 0.742\n",
      "epoch 0, batch 650, train_loss 0.742\n",
      "epoch 0, batch 660, train_loss 0.744\n",
      "epoch 0, batch 670, train_loss 0.744\n",
      "epoch 0, batch 680, train_loss 0.747\n",
      "epoch 0, batch 690, train_loss 0.732\n",
      "epoch 0, batch 700, train_loss 0.737\n",
      "epoch 0, batch 710, train_loss 0.746\n",
      "epoch 0, batch 720, train_loss 0.734\n",
      "epoch 0, batch 730, train_loss 0.737\n",
      "epoch 0, batch 740, train_loss 0.746\n",
      "epoch 0, batch 750, train_loss 0.734\n",
      "epoch 0, batch 760, train_loss 0.739\n",
      "epoch 0, batch 770, train_loss 0.735\n",
      "epoch 0, batch 780, train_loss 0.739\n",
      "epoch 0, batch 790, train_loss 0.741\n",
      "epoch 0, batch 800, train_loss 0.737\n",
      "epoch 0, batch 810, train_loss 0.736\n",
      "epoch 0, batch 820, train_loss 0.740\n",
      "epoch 0, batch 830, train_loss 0.734\n",
      "epoch 0, batch 840, train_loss 0.723\n",
      "epoch 0, batch 850, train_loss 0.735\n",
      "epoch 0, batch 860, train_loss 0.745\n",
      "epoch 0, batch 870, train_loss 0.740\n",
      "epoch 0, batch 880, train_loss 0.744\n",
      "epoch 0, batch 890, train_loss 0.725\n",
      "epoch 0, batch 900, train_loss 0.730\n",
      "epoch 0, batch 910, train_loss 0.740\n",
      "epoch 0, batch 920, train_loss 0.728\n",
      "epoch 0, batch 930, train_loss 0.725\n",
      "epoch 0, batch 940, train_loss 0.736\n",
      "epoch 0, batch 950, train_loss 0.738\n",
      "epoch 0, batch 960, train_loss 0.737\n",
      "epoch 0, batch 970, train_loss 0.744\n",
      "epoch 0, batch 980, train_loss 0.732\n",
      "epoch 0, batch 990, train_loss 0.726\n",
      "epoch 0, batch 1000, train_loss 0.715\n",
      "epoch 0, batch 1010, train_loss 0.736\n",
      "epoch 0, batch 1020, train_loss 0.725\n",
      "epoch 0, batch 1030, train_loss 0.731\n",
      "epoch 0, batch 1040, train_loss 0.728\n",
      "epoch 0, batch 1050, train_loss 0.722\n",
      "epoch 0, batch 1060, train_loss 0.735\n",
      "epoch 0, batch 1070, train_loss 0.729\n",
      "epoch 0, batch 1080, train_loss 0.738\n",
      "epoch 0, batch 1090, train_loss 0.726\n",
      "epoch 0, batch 1100, train_loss 0.729\n",
      "epoch 0, batch 1110, train_loss 0.729\n",
      "epoch 0, batch 1120, train_loss 0.737\n",
      "epoch 0, batch 1130, train_loss 0.727\n",
      "epoch 0, batch 1140, train_loss 0.742\n",
      "epoch 0, batch 1150, train_loss 0.730\n",
      "epoch 0, batch 1160, train_loss 0.735\n",
      "epoch 0, batch 1170, train_loss 0.734\n",
      "epoch 0, batch 1180, train_loss 0.735\n",
      "epoch 0, batch 1190, train_loss 0.724\n",
      "epoch 0, train_loss 0.729, valid_loss 0.740, train_accuracy  68.29%, valid_accuracy  67.68%\n",
      "model saved with highest valid accuracy:  67.68%\n",
      "epoch 1, batch 0, train_loss 0.728\n",
      "epoch 1, batch 10, train_loss 0.726\n",
      "epoch 1, batch 20, train_loss 0.720\n",
      "epoch 1, batch 30, train_loss 0.732\n",
      "epoch 1, batch 40, train_loss 0.720\n",
      "epoch 1, batch 50, train_loss 0.734\n",
      "epoch 1, batch 60, train_loss 0.729\n",
      "epoch 1, batch 70, train_loss 0.735\n",
      "epoch 1, batch 80, train_loss 0.728\n",
      "epoch 1, batch 90, train_loss 0.733\n",
      "epoch 1, batch 100, train_loss 0.719\n",
      "epoch 1, batch 110, train_loss 0.738\n",
      "epoch 1, batch 120, train_loss 0.726\n",
      "epoch 1, batch 130, train_loss 0.730\n",
      "epoch 1, batch 140, train_loss 0.737\n",
      "epoch 1, batch 150, train_loss 0.730\n",
      "epoch 1, batch 160, train_loss 0.715\n",
      "epoch 1, batch 170, train_loss 0.726\n",
      "epoch 1, batch 180, train_loss 0.728\n",
      "epoch 1, batch 190, train_loss 0.723\n",
      "epoch 1, batch 200, train_loss 0.730\n",
      "epoch 1, batch 210, train_loss 0.728\n",
      "epoch 1, batch 220, train_loss 0.721\n",
      "epoch 1, batch 230, train_loss 0.727\n",
      "epoch 1, batch 240, train_loss 0.729\n",
      "epoch 1, batch 250, train_loss 0.729\n",
      "epoch 1, batch 260, train_loss 0.732\n",
      "epoch 1, batch 270, train_loss 0.728\n",
      "epoch 1, batch 280, train_loss 0.722\n",
      "epoch 1, batch 290, train_loss 0.720\n",
      "epoch 1, batch 300, train_loss 0.720\n",
      "epoch 1, batch 310, train_loss 0.721\n",
      "epoch 1, batch 320, train_loss 0.713\n",
      "epoch 1, batch 330, train_loss 0.717\n",
      "epoch 1, batch 340, train_loss 0.710\n",
      "epoch 1, batch 350, train_loss 0.727\n",
      "epoch 1, batch 360, train_loss 0.733\n",
      "epoch 1, batch 370, train_loss 0.725\n",
      "epoch 1, batch 380, train_loss 0.712\n",
      "epoch 1, batch 390, train_loss 0.723\n",
      "epoch 1, batch 400, train_loss 0.722\n",
      "epoch 1, batch 410, train_loss 0.720\n",
      "epoch 1, batch 420, train_loss 0.724\n",
      "epoch 1, batch 430, train_loss 0.724\n",
      "epoch 1, batch 440, train_loss 0.716\n",
      "epoch 1, batch 450, train_loss 0.715\n",
      "epoch 1, batch 460, train_loss 0.730\n",
      "epoch 1, batch 470, train_loss 0.719\n",
      "epoch 1, batch 480, train_loss 0.726\n",
      "epoch 1, batch 490, train_loss 0.732\n",
      "epoch 1, batch 500, train_loss 0.723\n",
      "epoch 1, batch 510, train_loss 0.721\n",
      "epoch 1, batch 520, train_loss 0.721\n",
      "epoch 1, batch 530, train_loss 0.727\n",
      "epoch 1, batch 540, train_loss 0.726\n",
      "epoch 1, batch 550, train_loss 0.713\n",
      "epoch 1, batch 560, train_loss 0.724\n",
      "epoch 1, batch 570, train_loss 0.723\n",
      "epoch 1, batch 580, train_loss 0.718\n",
      "epoch 1, batch 590, train_loss 0.716\n",
      "epoch 1, batch 600, train_loss 0.726\n",
      "epoch 1, batch 610, train_loss 0.725\n",
      "epoch 1, batch 620, train_loss 0.716\n",
      "epoch 1, batch 630, train_loss 0.728\n",
      "epoch 1, batch 640, train_loss 0.717\n",
      "epoch 1, batch 650, train_loss 0.720\n",
      "epoch 1, batch 660, train_loss 0.725\n",
      "epoch 1, batch 670, train_loss 0.724\n",
      "epoch 1, batch 680, train_loss 0.719\n",
      "epoch 1, batch 690, train_loss 0.726\n",
      "epoch 1, batch 700, train_loss 0.716\n",
      "epoch 1, batch 710, train_loss 0.718\n",
      "epoch 1, batch 720, train_loss 0.715\n",
      "epoch 1, batch 730, train_loss 0.717\n",
      "epoch 1, batch 740, train_loss 0.719\n",
      "epoch 1, batch 750, train_loss 0.728\n",
      "epoch 1, batch 760, train_loss 0.725\n",
      "epoch 1, batch 770, train_loss 0.715\n",
      "epoch 1, batch 780, train_loss 0.725\n",
      "epoch 1, batch 790, train_loss 0.721\n",
      "epoch 1, batch 800, train_loss 0.717\n",
      "epoch 1, batch 810, train_loss 0.716\n",
      "epoch 1, batch 820, train_loss 0.716\n",
      "epoch 1, batch 830, train_loss 0.729\n",
      "epoch 1, batch 840, train_loss 0.707\n",
      "epoch 1, batch 850, train_loss 0.722\n",
      "epoch 1, batch 860, train_loss 0.710\n",
      "epoch 1, batch 870, train_loss 0.714\n",
      "epoch 1, batch 880, train_loss 0.728\n",
      "epoch 1, batch 890, train_loss 0.720\n",
      "epoch 1, batch 900, train_loss 0.724\n",
      "epoch 1, batch 910, train_loss 0.716\n",
      "epoch 1, batch 920, train_loss 0.723\n",
      "epoch 1, batch 930, train_loss 0.717\n",
      "epoch 1, batch 940, train_loss 0.708\n",
      "epoch 1, batch 950, train_loss 0.725\n",
      "epoch 1, batch 960, train_loss 0.715\n",
      "epoch 1, batch 970, train_loss 0.721\n",
      "epoch 1, batch 980, train_loss 0.714\n",
      "epoch 1, batch 990, train_loss 0.726\n",
      "epoch 1, batch 1000, train_loss 0.725\n",
      "epoch 1, batch 1010, train_loss 0.711\n",
      "epoch 1, batch 1020, train_loss 0.715\n",
      "epoch 1, batch 1030, train_loss 0.716\n",
      "epoch 1, batch 1040, train_loss 0.724\n",
      "epoch 1, batch 1050, train_loss 0.716\n",
      "epoch 1, batch 1060, train_loss 0.723\n",
      "epoch 1, batch 1070, train_loss 0.729\n",
      "epoch 1, batch 1080, train_loss 0.718\n",
      "epoch 1, batch 1090, train_loss 0.713\n",
      "epoch 1, batch 1100, train_loss 0.718\n",
      "epoch 1, batch 1110, train_loss 0.718\n",
      "epoch 1, batch 1120, train_loss 0.723\n",
      "epoch 1, batch 1130, train_loss 0.726\n",
      "epoch 1, batch 1140, train_loss 0.721\n",
      "epoch 1, batch 1150, train_loss 0.708\n",
      "epoch 1, batch 1160, train_loss 0.722\n",
      "epoch 1, batch 1170, train_loss 0.719\n",
      "epoch 1, batch 1180, train_loss 0.719\n",
      "epoch 1, batch 1190, train_loss 0.718\n",
      "epoch 1, train_loss 0.719, valid_loss 0.732, train_accuracy  68.81%, valid_accuracy  68.10%\n",
      "model saved with highest valid accuracy:  68.10%\n",
      "epoch 2, batch 0, train_loss 0.717\n",
      "epoch 2, batch 10, train_loss 0.720\n",
      "epoch 2, batch 20, train_loss 0.724\n",
      "epoch 2, batch 30, train_loss 0.720\n",
      "epoch 2, batch 40, train_loss 0.725\n",
      "epoch 2, batch 50, train_loss 0.716\n",
      "epoch 2, batch 60, train_loss 0.720\n",
      "epoch 2, batch 70, train_loss 0.720\n",
      "epoch 2, batch 80, train_loss 0.715\n",
      "epoch 2, batch 90, train_loss 0.712\n",
      "epoch 2, batch 100, train_loss 0.717\n",
      "epoch 2, batch 110, train_loss 0.721\n",
      "epoch 2, batch 120, train_loss 0.717\n",
      "epoch 2, batch 130, train_loss 0.724\n",
      "epoch 2, batch 140, train_loss 0.718\n",
      "epoch 2, batch 150, train_loss 0.713\n",
      "epoch 2, batch 160, train_loss 0.709\n",
      "epoch 2, batch 170, train_loss 0.721\n",
      "epoch 2, batch 180, train_loss 0.715\n",
      "epoch 2, batch 190, train_loss 0.718\n",
      "epoch 2, batch 200, train_loss 0.712\n",
      "epoch 2, batch 210, train_loss 0.725\n",
      "epoch 2, batch 220, train_loss 0.720\n",
      "epoch 2, batch 230, train_loss 0.716\n",
      "epoch 2, batch 240, train_loss 0.711\n",
      "epoch 2, batch 250, train_loss 0.719\n",
      "epoch 2, batch 260, train_loss 0.723\n",
      "epoch 2, batch 270, train_loss 0.711\n",
      "epoch 2, batch 280, train_loss 0.715\n",
      "epoch 2, batch 290, train_loss 0.709\n",
      "epoch 2, batch 300, train_loss 0.712\n",
      "epoch 2, batch 310, train_loss 0.719\n",
      "epoch 2, batch 320, train_loss 0.710\n",
      "epoch 2, batch 330, train_loss 0.717\n",
      "epoch 2, batch 340, train_loss 0.710\n",
      "epoch 2, batch 350, train_loss 0.717\n",
      "epoch 2, batch 360, train_loss 0.723\n",
      "epoch 2, batch 370, train_loss 0.720\n",
      "epoch 2, batch 380, train_loss 0.712\n",
      "epoch 2, batch 390, train_loss 0.712\n",
      "epoch 2, batch 400, train_loss 0.710\n",
      "epoch 2, batch 410, train_loss 0.717\n",
      "epoch 2, batch 420, train_loss 0.713\n",
      "epoch 2, batch 430, train_loss 0.720\n",
      "epoch 2, batch 440, train_loss 0.716\n",
      "epoch 2, batch 450, train_loss 0.718\n",
      "epoch 2, batch 460, train_loss 0.706\n",
      "epoch 2, batch 470, train_loss 0.705\n",
      "epoch 2, batch 480, train_loss 0.715\n",
      "epoch 2, batch 490, train_loss 0.715\n",
      "epoch 2, batch 500, train_loss 0.707\n",
      "epoch 2, batch 510, train_loss 0.705\n",
      "epoch 2, batch 520, train_loss 0.718\n",
      "epoch 2, batch 530, train_loss 0.708\n",
      "epoch 2, batch 540, train_loss 0.723\n",
      "epoch 2, batch 550, train_loss 0.717\n",
      "epoch 2, batch 560, train_loss 0.707\n",
      "epoch 2, batch 570, train_loss 0.724\n",
      "epoch 2, batch 580, train_loss 0.717\n",
      "epoch 2, batch 590, train_loss 0.716\n",
      "epoch 2, batch 600, train_loss 0.718\n",
      "epoch 2, batch 610, train_loss 0.715\n",
      "epoch 2, batch 620, train_loss 0.713\n",
      "epoch 2, batch 630, train_loss 0.726\n",
      "epoch 2, batch 640, train_loss 0.714\n",
      "epoch 2, batch 650, train_loss 0.725\n",
      "epoch 2, batch 660, train_loss 0.719\n",
      "epoch 2, batch 670, train_loss 0.721\n",
      "epoch 2, batch 680, train_loss 0.725\n",
      "epoch 2, batch 690, train_loss 0.719\n",
      "epoch 2, batch 700, train_loss 0.714\n",
      "epoch 2, batch 710, train_loss 0.709\n",
      "epoch 2, batch 720, train_loss 0.711\n",
      "epoch 2, batch 730, train_loss 0.713\n",
      "epoch 2, batch 740, train_loss 0.708\n",
      "epoch 2, batch 750, train_loss 0.721\n",
      "epoch 2, batch 760, train_loss 0.718\n",
      "epoch 2, batch 770, train_loss 0.712\n",
      "epoch 2, batch 780, train_loss 0.716\n",
      "epoch 2, batch 790, train_loss 0.716\n",
      "epoch 2, batch 800, train_loss 0.699\n",
      "epoch 2, batch 810, train_loss 0.716\n",
      "epoch 2, batch 820, train_loss 0.730\n",
      "epoch 2, batch 830, train_loss 0.713\n",
      "epoch 2, batch 840, train_loss 0.719\n",
      "epoch 2, batch 850, train_loss 0.720\n",
      "epoch 2, batch 860, train_loss 0.718\n",
      "epoch 2, batch 870, train_loss 0.719\n",
      "epoch 2, batch 880, train_loss 0.714\n",
      "epoch 2, batch 890, train_loss 0.717\n",
      "epoch 2, batch 900, train_loss 0.725\n",
      "epoch 2, batch 910, train_loss 0.710\n",
      "epoch 2, batch 920, train_loss 0.717\n",
      "epoch 2, batch 930, train_loss 0.714\n",
      "epoch 2, batch 940, train_loss 0.712\n",
      "epoch 2, batch 950, train_loss 0.721\n",
      "epoch 2, batch 960, train_loss 0.718\n",
      "epoch 2, batch 970, train_loss 0.720\n",
      "epoch 2, batch 980, train_loss 0.715\n",
      "epoch 2, batch 990, train_loss 0.722\n",
      "epoch 2, batch 1000, train_loss 0.713\n",
      "epoch 2, batch 1010, train_loss 0.711\n",
      "epoch 2, batch 1020, train_loss 0.705\n",
      "epoch 2, batch 1030, train_loss 0.721\n",
      "epoch 2, batch 1040, train_loss 0.722\n",
      "epoch 2, batch 1050, train_loss 0.716\n",
      "epoch 2, batch 1060, train_loss 0.718\n",
      "epoch 2, batch 1070, train_loss 0.711\n",
      "epoch 2, batch 1080, train_loss 0.713\n",
      "epoch 2, batch 1090, train_loss 0.726\n",
      "epoch 2, batch 1100, train_loss 0.711\n",
      "epoch 2, batch 1110, train_loss 0.713\n",
      "epoch 2, batch 1120, train_loss 0.723\n",
      "epoch 2, batch 1130, train_loss 0.713\n",
      "epoch 2, batch 1140, train_loss 0.698\n",
      "epoch 2, batch 1150, train_loss 0.718\n",
      "epoch 2, batch 1160, train_loss 0.713\n",
      "epoch 2, batch 1170, train_loss 0.710\n",
      "epoch 2, batch 1180, train_loss 0.713\n",
      "epoch 2, batch 1190, train_loss 0.717\n",
      "epoch 2, train_loss 0.712, valid_loss 0.727, train_accuracy  69.18%, valid_accuracy  68.42%\n",
      "model saved with highest valid accuracy:  68.42%\n",
      "epoch 3, batch 0, train_loss 0.718\n",
      "epoch 3, batch 10, train_loss 0.716\n",
      "epoch 3, batch 20, train_loss 0.706\n",
      "epoch 3, batch 30, train_loss 0.701\n",
      "epoch 3, batch 40, train_loss 0.709\n",
      "epoch 3, batch 50, train_loss 0.711\n",
      "epoch 3, batch 60, train_loss 0.720\n",
      "epoch 3, batch 70, train_loss 0.717\n",
      "epoch 3, batch 80, train_loss 0.714\n",
      "epoch 3, batch 90, train_loss 0.706\n",
      "epoch 3, batch 100, train_loss 0.711\n",
      "epoch 3, batch 110, train_loss 0.706\n",
      "epoch 3, batch 120, train_loss 0.719\n",
      "epoch 3, batch 130, train_loss 0.714\n",
      "epoch 3, batch 140, train_loss 0.713\n",
      "epoch 3, batch 150, train_loss 0.711\n",
      "epoch 3, batch 160, train_loss 0.709\n",
      "epoch 3, batch 170, train_loss 0.710\n",
      "epoch 3, batch 180, train_loss 0.706\n",
      "epoch 3, batch 190, train_loss 0.709\n",
      "epoch 3, batch 200, train_loss 0.706\n",
      "epoch 3, batch 210, train_loss 0.717\n",
      "epoch 3, batch 220, train_loss 0.709\n",
      "epoch 3, batch 230, train_loss 0.714\n",
      "epoch 3, batch 240, train_loss 0.718\n",
      "epoch 3, batch 250, train_loss 0.708\n",
      "epoch 3, batch 260, train_loss 0.708\n",
      "epoch 3, batch 270, train_loss 0.708\n",
      "epoch 3, batch 280, train_loss 0.706\n",
      "epoch 3, batch 290, train_loss 0.717\n",
      "epoch 3, batch 300, train_loss 0.708\n",
      "epoch 3, batch 310, train_loss 0.705\n",
      "epoch 3, batch 320, train_loss 0.710\n",
      "epoch 3, batch 330, train_loss 0.711\n",
      "epoch 3, batch 340, train_loss 0.705\n",
      "epoch 3, batch 350, train_loss 0.709\n",
      "epoch 3, batch 360, train_loss 0.711\n",
      "epoch 3, batch 370, train_loss 0.710\n",
      "epoch 3, batch 380, train_loss 0.709\n",
      "epoch 3, batch 390, train_loss 0.713\n",
      "epoch 3, batch 400, train_loss 0.710\n",
      "epoch 3, batch 410, train_loss 0.716\n",
      "epoch 3, batch 420, train_loss 0.710\n",
      "epoch 3, batch 430, train_loss 0.709\n",
      "epoch 3, batch 440, train_loss 0.714\n",
      "epoch 3, batch 450, train_loss 0.718\n",
      "epoch 3, batch 460, train_loss 0.702\n",
      "epoch 3, batch 470, train_loss 0.704\n",
      "epoch 3, batch 480, train_loss 0.704\n",
      "epoch 3, batch 490, train_loss 0.716\n",
      "epoch 3, batch 500, train_loss 0.704\n",
      "epoch 3, batch 510, train_loss 0.704\n",
      "epoch 3, batch 520, train_loss 0.707\n",
      "epoch 3, batch 530, train_loss 0.721\n",
      "epoch 3, batch 540, train_loss 0.713\n",
      "epoch 3, batch 550, train_loss 0.704\n",
      "epoch 3, batch 560, train_loss 0.721\n",
      "epoch 3, batch 570, train_loss 0.705\n",
      "epoch 3, batch 580, train_loss 0.713\n",
      "epoch 3, batch 590, train_loss 0.707\n",
      "epoch 3, batch 600, train_loss 0.713\n",
      "epoch 3, batch 610, train_loss 0.717\n",
      "epoch 3, batch 620, train_loss 0.702\n",
      "epoch 3, batch 630, train_loss 0.713\n",
      "epoch 3, batch 640, train_loss 0.710\n",
      "epoch 3, batch 650, train_loss 0.715\n",
      "epoch 3, batch 660, train_loss 0.711\n",
      "epoch 3, batch 670, train_loss 0.701\n",
      "epoch 3, batch 680, train_loss 0.705\n",
      "epoch 3, batch 690, train_loss 0.708\n",
      "epoch 3, batch 700, train_loss 0.707\n",
      "epoch 3, batch 710, train_loss 0.709\n",
      "epoch 3, batch 720, train_loss 0.711\n",
      "epoch 3, batch 730, train_loss 0.708\n",
      "epoch 3, batch 740, train_loss 0.710\n",
      "epoch 3, batch 750, train_loss 0.709\n",
      "epoch 3, batch 760, train_loss 0.701\n",
      "epoch 3, batch 770, train_loss 0.712\n",
      "epoch 3, batch 780, train_loss 0.711\n",
      "epoch 3, batch 790, train_loss 0.709\n",
      "epoch 3, batch 800, train_loss 0.721\n",
      "epoch 3, batch 810, train_loss 0.714\n",
      "epoch 3, batch 820, train_loss 0.720\n",
      "epoch 3, batch 830, train_loss 0.711\n",
      "epoch 3, batch 840, train_loss 0.711\n",
      "epoch 3, batch 850, train_loss 0.707\n",
      "epoch 3, batch 860, train_loss 0.716\n",
      "epoch 3, batch 870, train_loss 0.710\n",
      "epoch 3, batch 880, train_loss 0.716\n",
      "epoch 3, batch 890, train_loss 0.715\n",
      "epoch 3, batch 900, train_loss 0.709\n",
      "epoch 3, batch 910, train_loss 0.710\n",
      "epoch 3, batch 920, train_loss 0.705\n",
      "epoch 3, batch 930, train_loss 0.712\n",
      "epoch 3, batch 940, train_loss 0.707\n",
      "epoch 3, batch 950, train_loss 0.711\n",
      "epoch 3, batch 960, train_loss 0.709\n",
      "epoch 3, batch 970, train_loss 0.708\n",
      "epoch 3, batch 980, train_loss 0.709\n",
      "epoch 3, batch 990, train_loss 0.715\n",
      "epoch 3, batch 1000, train_loss 0.715\n",
      "epoch 3, batch 1010, train_loss 0.717\n",
      "epoch 3, batch 1020, train_loss 0.715\n",
      "epoch 3, batch 1030, train_loss 0.707\n",
      "epoch 3, batch 1040, train_loss 0.716\n",
      "epoch 3, batch 1050, train_loss 0.715\n",
      "epoch 3, batch 1060, train_loss 0.710\n",
      "epoch 3, batch 1070, train_loss 0.711\n",
      "epoch 3, batch 1080, train_loss 0.711\n",
      "epoch 3, batch 1090, train_loss 0.718\n",
      "epoch 3, batch 1100, train_loss 0.706\n",
      "epoch 3, batch 1110, train_loss 0.711\n",
      "epoch 3, batch 1120, train_loss 0.703\n",
      "epoch 3, batch 1130, train_loss 0.707\n",
      "epoch 3, batch 1140, train_loss 0.721\n",
      "epoch 3, batch 1150, train_loss 0.700\n",
      "epoch 3, batch 1160, train_loss 0.708\n",
      "epoch 3, batch 1170, train_loss 0.712\n",
      "epoch 3, batch 1180, train_loss 0.704\n",
      "epoch 3, batch 1190, train_loss 0.704\n",
      "epoch 3, train_loss 0.709, valid_loss 0.725, train_accuracy  69.34%, valid_accuracy  68.49%\n",
      "model saved with highest valid accuracy:  68.49%\n",
      "epoch 4, batch 0, train_loss 0.705\n",
      "epoch 4, batch 10, train_loss 0.713\n",
      "epoch 4, batch 20, train_loss 0.706\n",
      "epoch 4, batch 30, train_loss 0.713\n",
      "epoch 4, batch 40, train_loss 0.713\n",
      "epoch 4, batch 50, train_loss 0.700\n",
      "epoch 4, batch 60, train_loss 0.709\n",
      "epoch 4, batch 70, train_loss 0.711\n",
      "epoch 4, batch 80, train_loss 0.705\n",
      "epoch 4, batch 90, train_loss 0.713\n",
      "epoch 4, batch 100, train_loss 0.714\n",
      "epoch 4, batch 110, train_loss 0.706\n",
      "epoch 4, batch 120, train_loss 0.711\n",
      "epoch 4, batch 130, train_loss 0.707\n",
      "epoch 4, batch 140, train_loss 0.705\n",
      "epoch 4, batch 150, train_loss 0.709\n",
      "epoch 4, batch 160, train_loss 0.707\n",
      "epoch 4, batch 170, train_loss 0.702\n",
      "epoch 4, batch 180, train_loss 0.713\n",
      "epoch 4, batch 190, train_loss 0.711\n",
      "epoch 4, batch 200, train_loss 0.717\n",
      "epoch 4, batch 210, train_loss 0.709\n",
      "epoch 4, batch 220, train_loss 0.708\n",
      "epoch 4, batch 230, train_loss 0.708\n",
      "epoch 4, batch 240, train_loss 0.709\n",
      "epoch 4, batch 250, train_loss 0.707\n",
      "epoch 4, batch 260, train_loss 0.711\n",
      "epoch 4, batch 270, train_loss 0.705\n",
      "epoch 4, batch 280, train_loss 0.712\n",
      "epoch 4, batch 290, train_loss 0.697\n",
      "epoch 4, batch 300, train_loss 0.696\n",
      "epoch 4, batch 310, train_loss 0.712\n",
      "epoch 4, batch 320, train_loss 0.704\n",
      "epoch 4, batch 330, train_loss 0.709\n",
      "epoch 4, batch 340, train_loss 0.701\n",
      "epoch 4, batch 350, train_loss 0.699\n",
      "epoch 4, batch 360, train_loss 0.708\n",
      "epoch 4, batch 370, train_loss 0.709\n",
      "epoch 4, batch 380, train_loss 0.710\n",
      "epoch 4, batch 390, train_loss 0.706\n",
      "epoch 4, batch 400, train_loss 0.722\n",
      "epoch 4, batch 410, train_loss 0.709\n",
      "epoch 4, batch 420, train_loss 0.701\n",
      "epoch 4, batch 430, train_loss 0.706\n",
      "epoch 4, batch 440, train_loss 0.718\n",
      "epoch 4, batch 450, train_loss 0.708\n",
      "epoch 4, batch 460, train_loss 0.713\n",
      "epoch 4, batch 470, train_loss 0.716\n",
      "epoch 4, batch 480, train_loss 0.714\n",
      "epoch 4, batch 490, train_loss 0.701\n",
      "epoch 4, batch 500, train_loss 0.701\n",
      "epoch 4, batch 510, train_loss 0.721\n",
      "epoch 4, batch 520, train_loss 0.700\n",
      "epoch 4, batch 530, train_loss 0.713\n",
      "epoch 4, batch 540, train_loss 0.701\n",
      "epoch 4, batch 550, train_loss 0.704\n",
      "epoch 4, batch 560, train_loss 0.703\n",
      "epoch 4, batch 570, train_loss 0.710\n",
      "epoch 4, batch 580, train_loss 0.705\n",
      "epoch 4, batch 590, train_loss 0.711\n",
      "epoch 4, batch 600, train_loss 0.707\n",
      "epoch 4, batch 610, train_loss 0.717\n",
      "epoch 4, batch 620, train_loss 0.710\n",
      "epoch 4, batch 630, train_loss 0.715\n",
      "epoch 4, batch 640, train_loss 0.711\n",
      "epoch 4, batch 650, train_loss 0.699\n",
      "epoch 4, batch 660, train_loss 0.711\n",
      "epoch 4, batch 670, train_loss 0.706\n",
      "epoch 4, batch 680, train_loss 0.706\n",
      "epoch 4, batch 690, train_loss 0.711\n",
      "epoch 4, batch 700, train_loss 0.708\n",
      "epoch 4, batch 710, train_loss 0.709\n",
      "epoch 4, batch 720, train_loss 0.704\n",
      "epoch 4, batch 730, train_loss 0.705\n",
      "epoch 4, batch 740, train_loss 0.701\n",
      "epoch 4, batch 750, train_loss 0.711\n",
      "epoch 4, batch 760, train_loss 0.706\n",
      "epoch 4, batch 770, train_loss 0.712\n",
      "epoch 4, batch 780, train_loss 0.708\n",
      "epoch 4, batch 790, train_loss 0.711\n",
      "epoch 4, batch 800, train_loss 0.704\n",
      "epoch 4, batch 810, train_loss 0.695\n",
      "epoch 4, batch 820, train_loss 0.703\n",
      "epoch 4, batch 830, train_loss 0.707\n",
      "epoch 4, batch 840, train_loss 0.709\n",
      "epoch 4, batch 850, train_loss 0.703\n",
      "epoch 4, batch 860, train_loss 0.710\n",
      "epoch 4, batch 870, train_loss 0.708\n",
      "epoch 4, batch 880, train_loss 0.705\n",
      "epoch 4, batch 890, train_loss 0.704\n",
      "epoch 4, batch 900, train_loss 0.718\n",
      "epoch 4, batch 910, train_loss 0.713\n",
      "epoch 4, batch 920, train_loss 0.705\n",
      "epoch 4, batch 930, train_loss 0.705\n",
      "epoch 4, batch 940, train_loss 0.713\n",
      "epoch 4, batch 950, train_loss 0.697\n",
      "epoch 4, batch 960, train_loss 0.711\n",
      "epoch 4, batch 970, train_loss 0.713\n",
      "epoch 4, batch 980, train_loss 0.710\n",
      "epoch 4, batch 990, train_loss 0.710\n",
      "epoch 4, batch 1000, train_loss 0.708\n",
      "epoch 4, batch 1010, train_loss 0.704\n",
      "epoch 4, batch 1020, train_loss 0.716\n",
      "epoch 4, batch 1030, train_loss 0.702\n",
      "epoch 4, batch 1040, train_loss 0.709\n",
      "epoch 4, batch 1050, train_loss 0.713\n",
      "epoch 4, batch 1060, train_loss 0.706\n",
      "epoch 4, batch 1070, train_loss 0.707\n",
      "epoch 4, batch 1080, train_loss 0.714\n",
      "epoch 4, batch 1090, train_loss 0.712\n",
      "epoch 4, batch 1100, train_loss 0.714\n",
      "epoch 4, batch 1110, train_loss 0.700\n",
      "epoch 4, batch 1120, train_loss 0.693\n",
      "epoch 4, batch 1130, train_loss 0.704\n",
      "epoch 4, batch 1140, train_loss 0.699\n",
      "epoch 4, batch 1150, train_loss 0.717\n",
      "epoch 4, batch 1160, train_loss 0.709\n",
      "epoch 4, batch 1170, train_loss 0.701\n",
      "epoch 4, batch 1180, train_loss 0.714\n",
      "epoch 4, batch 1190, train_loss 0.710\n",
      "epoch 4, train_loss 0.707, valid_loss 0.724, train_accuracy  69.43%, valid_accuracy  68.52%\n",
      "model saved with highest valid accuracy:  68.52%\n",
      "epoch 5, batch 0, train_loss 0.697\n",
      "epoch 5, batch 10, train_loss 0.705\n",
      "epoch 5, batch 20, train_loss 0.710\n",
      "epoch 5, batch 30, train_loss 0.715\n",
      "epoch 5, batch 40, train_loss 0.718\n",
      "epoch 5, batch 50, train_loss 0.710\n",
      "epoch 5, batch 60, train_loss 0.712\n",
      "epoch 5, batch 70, train_loss 0.714\n",
      "epoch 5, batch 80, train_loss 0.706\n",
      "epoch 5, batch 90, train_loss 0.716\n",
      "epoch 5, batch 100, train_loss 0.703\n",
      "epoch 5, batch 110, train_loss 0.709\n",
      "epoch 5, batch 120, train_loss 0.712\n",
      "epoch 5, batch 130, train_loss 0.710\n",
      "epoch 5, batch 140, train_loss 0.716\n",
      "epoch 5, batch 150, train_loss 0.708\n",
      "epoch 5, batch 160, train_loss 0.704\n",
      "epoch 5, batch 170, train_loss 0.703\n",
      "epoch 5, batch 180, train_loss 0.712\n",
      "epoch 5, batch 190, train_loss 0.706\n",
      "epoch 5, batch 200, train_loss 0.703\n",
      "epoch 5, batch 210, train_loss 0.714\n",
      "epoch 5, batch 220, train_loss 0.703\n",
      "epoch 5, batch 230, train_loss 0.705\n",
      "epoch 5, batch 240, train_loss 0.716\n",
      "epoch 5, batch 250, train_loss 0.716\n",
      "epoch 5, batch 260, train_loss 0.706\n",
      "epoch 5, batch 270, train_loss 0.713\n",
      "epoch 5, batch 280, train_loss 0.702\n",
      "epoch 5, batch 290, train_loss 0.714\n",
      "epoch 5, batch 300, train_loss 0.709\n",
      "epoch 5, batch 310, train_loss 0.709\n",
      "epoch 5, batch 320, train_loss 0.708\n",
      "epoch 5, batch 330, train_loss 0.700\n",
      "epoch 5, batch 340, train_loss 0.708\n",
      "epoch 5, batch 350, train_loss 0.713\n",
      "epoch 5, batch 360, train_loss 0.716\n",
      "epoch 5, batch 370, train_loss 0.707\n",
      "epoch 5, batch 380, train_loss 0.711\n",
      "epoch 5, batch 390, train_loss 0.697\n",
      "epoch 5, batch 400, train_loss 0.710\n",
      "epoch 5, batch 410, train_loss 0.711\n",
      "epoch 5, batch 420, train_loss 0.708\n",
      "epoch 5, batch 430, train_loss 0.707\n",
      "epoch 5, batch 440, train_loss 0.708\n",
      "epoch 5, batch 450, train_loss 0.712\n",
      "epoch 5, batch 460, train_loss 0.710\n",
      "epoch 5, batch 470, train_loss 0.694\n",
      "epoch 5, batch 480, train_loss 0.704\n",
      "epoch 5, batch 490, train_loss 0.703\n",
      "epoch 5, batch 500, train_loss 0.706\n",
      "epoch 5, batch 510, train_loss 0.694\n",
      "epoch 5, batch 520, train_loss 0.721\n",
      "epoch 5, batch 530, train_loss 0.710\n",
      "epoch 5, batch 540, train_loss 0.706\n",
      "epoch 5, batch 550, train_loss 0.705\n",
      "epoch 5, batch 560, train_loss 0.695\n",
      "epoch 5, batch 570, train_loss 0.712\n",
      "epoch 5, batch 580, train_loss 0.703\n",
      "epoch 5, batch 590, train_loss 0.703\n",
      "epoch 5, batch 600, train_loss 0.706\n",
      "epoch 5, batch 610, train_loss 0.705\n",
      "epoch 5, batch 620, train_loss 0.720\n",
      "epoch 5, batch 630, train_loss 0.707\n",
      "epoch 5, batch 640, train_loss 0.703\n",
      "epoch 5, batch 650, train_loss 0.714\n",
      "epoch 5, batch 660, train_loss 0.702\n",
      "epoch 5, batch 670, train_loss 0.701\n",
      "epoch 5, batch 680, train_loss 0.703\n",
      "epoch 5, batch 690, train_loss 0.715\n",
      "epoch 5, batch 700, train_loss 0.698\n",
      "epoch 5, batch 710, train_loss 0.704\n",
      "epoch 5, batch 720, train_loss 0.706\n",
      "epoch 5, batch 730, train_loss 0.713\n",
      "epoch 5, batch 740, train_loss 0.702\n",
      "epoch 5, batch 750, train_loss 0.710\n",
      "epoch 5, batch 760, train_loss 0.715\n",
      "epoch 5, batch 770, train_loss 0.707\n",
      "epoch 5, batch 780, train_loss 0.702\n",
      "epoch 5, batch 790, train_loss 0.706\n",
      "epoch 5, batch 800, train_loss 0.708\n",
      "epoch 5, batch 810, train_loss 0.704\n",
      "epoch 5, batch 820, train_loss 0.716\n",
      "epoch 5, batch 830, train_loss 0.705\n",
      "epoch 5, batch 840, train_loss 0.712\n",
      "epoch 5, batch 850, train_loss 0.718\n",
      "epoch 5, batch 860, train_loss 0.695\n",
      "epoch 5, batch 870, train_loss 0.706\n",
      "epoch 5, batch 880, train_loss 0.707\n",
      "epoch 5, batch 890, train_loss 0.711\n",
      "epoch 5, batch 900, train_loss 0.714\n",
      "epoch 5, batch 910, train_loss 0.697\n",
      "epoch 5, batch 920, train_loss 0.705\n",
      "epoch 5, batch 930, train_loss 0.707\n",
      "epoch 5, batch 940, train_loss 0.712\n",
      "epoch 5, batch 950, train_loss 0.699\n",
      "epoch 5, batch 960, train_loss 0.709\n",
      "epoch 5, batch 970, train_loss 0.710\n",
      "epoch 5, batch 980, train_loss 0.712\n",
      "epoch 5, batch 990, train_loss 0.713\n",
      "epoch 5, batch 1000, train_loss 0.713\n",
      "epoch 5, batch 1010, train_loss 0.701\n",
      "epoch 5, batch 1020, train_loss 0.708\n",
      "epoch 5, batch 1030, train_loss 0.698\n",
      "epoch 5, batch 1040, train_loss 0.713\n",
      "epoch 5, batch 1050, train_loss 0.699\n",
      "epoch 5, batch 1060, train_loss 0.707\n",
      "epoch 5, batch 1070, train_loss 0.703\n",
      "epoch 5, batch 1080, train_loss 0.709\n",
      "epoch 5, batch 1090, train_loss 0.704\n",
      "epoch 5, batch 1100, train_loss 0.715\n",
      "epoch 5, batch 1110, train_loss 0.706\n",
      "epoch 5, batch 1120, train_loss 0.710\n",
      "epoch 5, batch 1130, train_loss 0.703\n",
      "epoch 5, batch 1140, train_loss 0.707\n",
      "epoch 5, batch 1150, train_loss 0.709\n",
      "epoch 5, batch 1160, train_loss 0.704\n",
      "epoch 5, batch 1170, train_loss 0.699\n",
      "epoch 5, batch 1180, train_loss 0.702\n",
      "epoch 5, batch 1190, train_loss 0.703\n",
      "epoch 5, train_loss 0.706, valid_loss 0.724, train_accuracy  69.48%, valid_accuracy  68.51%\n",
      "epoch 6, batch 0, train_loss 0.704\n",
      "epoch 6, batch 10, train_loss 0.713\n",
      "epoch 6, batch 20, train_loss 0.708\n",
      "epoch 6, batch 30, train_loss 0.701\n",
      "epoch 6, batch 40, train_loss 0.702\n",
      "epoch 6, batch 50, train_loss 0.708\n",
      "epoch 6, batch 60, train_loss 0.699\n",
      "epoch 6, batch 70, train_loss 0.703\n",
      "epoch 6, batch 80, train_loss 0.704\n",
      "epoch 6, batch 90, train_loss 0.704\n",
      "epoch 6, batch 100, train_loss 0.704\n",
      "epoch 6, batch 110, train_loss 0.708\n",
      "epoch 6, batch 120, train_loss 0.709\n",
      "epoch 6, batch 130, train_loss 0.700\n",
      "epoch 6, batch 140, train_loss 0.707\n",
      "epoch 6, batch 150, train_loss 0.707\n",
      "epoch 6, batch 160, train_loss 0.709\n",
      "epoch 6, batch 170, train_loss 0.703\n",
      "epoch 6, batch 180, train_loss 0.706\n",
      "epoch 6, batch 190, train_loss 0.706\n",
      "epoch 6, batch 200, train_loss 0.710\n",
      "epoch 6, batch 210, train_loss 0.698\n",
      "epoch 6, batch 220, train_loss 0.705\n",
      "epoch 6, batch 230, train_loss 0.709\n",
      "epoch 6, batch 240, train_loss 0.707\n",
      "epoch 6, batch 250, train_loss 0.701\n",
      "epoch 6, batch 260, train_loss 0.703\n",
      "epoch 6, batch 270, train_loss 0.711\n",
      "epoch 6, batch 280, train_loss 0.710\n",
      "epoch 6, batch 290, train_loss 0.709\n",
      "epoch 6, batch 300, train_loss 0.700\n",
      "epoch 6, batch 310, train_loss 0.710\n",
      "epoch 6, batch 320, train_loss 0.702\n",
      "epoch 6, batch 330, train_loss 0.712\n",
      "epoch 6, batch 340, train_loss 0.713\n",
      "epoch 6, batch 350, train_loss 0.697\n",
      "epoch 6, batch 360, train_loss 0.708\n",
      "epoch 6, batch 370, train_loss 0.713\n",
      "epoch 6, batch 380, train_loss 0.705\n",
      "epoch 6, batch 390, train_loss 0.708\n",
      "epoch 6, batch 400, train_loss 0.715\n",
      "epoch 6, batch 410, train_loss 0.710\n",
      "epoch 6, batch 420, train_loss 0.707\n",
      "epoch 6, batch 430, train_loss 0.703\n",
      "epoch 6, batch 440, train_loss 0.708\n",
      "epoch 6, batch 450, train_loss 0.707\n",
      "epoch 6, batch 460, train_loss 0.704\n",
      "epoch 6, batch 470, train_loss 0.705\n",
      "epoch 6, batch 480, train_loss 0.691\n",
      "epoch 6, batch 490, train_loss 0.708\n",
      "epoch 6, batch 500, train_loss 0.704\n",
      "epoch 6, batch 510, train_loss 0.701\n",
      "epoch 6, batch 520, train_loss 0.710\n",
      "epoch 6, batch 530, train_loss 0.697\n",
      "epoch 6, batch 540, train_loss 0.701\n",
      "epoch 6, batch 550, train_loss 0.705\n",
      "epoch 6, batch 560, train_loss 0.706\n",
      "epoch 6, batch 570, train_loss 0.703\n",
      "epoch 6, batch 580, train_loss 0.709\n",
      "epoch 6, batch 590, train_loss 0.700\n",
      "epoch 6, batch 600, train_loss 0.717\n",
      "epoch 6, batch 610, train_loss 0.714\n",
      "epoch 6, batch 620, train_loss 0.705\n",
      "epoch 6, batch 630, train_loss 0.711\n",
      "epoch 6, batch 640, train_loss 0.715\n",
      "epoch 6, batch 650, train_loss 0.699\n",
      "epoch 6, batch 660, train_loss 0.707\n",
      "epoch 6, batch 670, train_loss 0.705\n",
      "epoch 6, batch 680, train_loss 0.701\n",
      "epoch 6, batch 690, train_loss 0.712\n",
      "epoch 6, batch 700, train_loss 0.705\n",
      "epoch 6, batch 710, train_loss 0.711\n",
      "epoch 6, batch 720, train_loss 0.716\n",
      "epoch 6, batch 730, train_loss 0.701\n",
      "epoch 6, batch 740, train_loss 0.707\n",
      "epoch 6, batch 750, train_loss 0.697\n",
      "epoch 6, batch 760, train_loss 0.715\n",
      "epoch 6, batch 770, train_loss 0.697\n",
      "epoch 6, batch 780, train_loss 0.709\n",
      "epoch 6, batch 790, train_loss 0.704\n",
      "epoch 6, batch 800, train_loss 0.700\n",
      "epoch 6, batch 810, train_loss 0.715\n",
      "epoch 6, batch 820, train_loss 0.709\n",
      "epoch 6, batch 830, train_loss 0.696\n",
      "epoch 6, batch 840, train_loss 0.706\n",
      "epoch 6, batch 850, train_loss 0.701\n",
      "epoch 6, batch 860, train_loss 0.703\n",
      "epoch 6, batch 870, train_loss 0.702\n",
      "epoch 6, batch 880, train_loss 0.698\n",
      "epoch 6, batch 890, train_loss 0.703\n",
      "epoch 6, batch 900, train_loss 0.705\n",
      "epoch 6, batch 910, train_loss 0.704\n",
      "epoch 6, batch 920, train_loss 0.703\n",
      "epoch 6, batch 930, train_loss 0.713\n",
      "epoch 6, batch 940, train_loss 0.694\n",
      "epoch 6, batch 950, train_loss 0.703\n",
      "epoch 6, batch 960, train_loss 0.712\n",
      "epoch 6, batch 970, train_loss 0.712\n",
      "epoch 6, batch 980, train_loss 0.714\n",
      "epoch 6, batch 990, train_loss 0.711\n",
      "epoch 6, batch 1000, train_loss 0.698\n",
      "epoch 6, batch 1010, train_loss 0.706\n",
      "epoch 6, batch 1020, train_loss 0.713\n",
      "epoch 6, batch 1030, train_loss 0.693\n",
      "epoch 6, batch 1040, train_loss 0.705\n",
      "epoch 6, batch 1050, train_loss 0.691\n",
      "epoch 6, batch 1060, train_loss 0.697\n",
      "epoch 6, batch 1070, train_loss 0.698\n",
      "epoch 6, batch 1080, train_loss 0.699\n",
      "epoch 6, batch 1090, train_loss 0.692\n",
      "epoch 6, batch 1100, train_loss 0.698\n",
      "epoch 6, batch 1110, train_loss 0.701\n",
      "epoch 6, batch 1120, train_loss 0.703\n",
      "epoch 6, batch 1130, train_loss 0.711\n",
      "epoch 6, batch 1140, train_loss 0.699\n",
      "epoch 6, batch 1150, train_loss 0.709\n",
      "epoch 6, batch 1160, train_loss 0.702\n",
      "epoch 6, batch 1170, train_loss 0.699\n",
      "epoch 6, batch 1180, train_loss 0.704\n",
      "epoch 6, batch 1190, train_loss 0.698\n",
      "epoch 6, train_loss 0.704, valid_loss 0.723, train_accuracy  69.57%, valid_accuracy  68.53%\n",
      "model saved with highest valid accuracy:  68.53%\n",
      "epoch 7, batch 0, train_loss 0.700\n",
      "epoch 7, batch 10, train_loss 0.703\n",
      "epoch 7, batch 20, train_loss 0.708\n",
      "epoch 7, batch 30, train_loss 0.712\n",
      "epoch 7, batch 40, train_loss 0.708\n",
      "epoch 7, batch 50, train_loss 0.705\n",
      "epoch 7, batch 60, train_loss 0.699\n",
      "epoch 7, batch 70, train_loss 0.706\n",
      "epoch 7, batch 80, train_loss 0.709\n",
      "epoch 7, batch 90, train_loss 0.702\n",
      "epoch 7, batch 100, train_loss 0.708\n",
      "epoch 7, batch 110, train_loss 0.697\n",
      "epoch 7, batch 120, train_loss 0.712\n",
      "epoch 7, batch 130, train_loss 0.714\n",
      "epoch 7, batch 140, train_loss 0.702\n",
      "epoch 7, batch 150, train_loss 0.697\n",
      "epoch 7, batch 160, train_loss 0.708\n",
      "epoch 7, batch 170, train_loss 0.705\n",
      "epoch 7, batch 180, train_loss 0.702\n",
      "epoch 7, batch 190, train_loss 0.706\n",
      "epoch 7, batch 200, train_loss 0.698\n",
      "epoch 7, batch 210, train_loss 0.707\n",
      "epoch 7, batch 220, train_loss 0.705\n",
      "epoch 7, batch 230, train_loss 0.699\n",
      "epoch 7, batch 240, train_loss 0.696\n",
      "epoch 7, batch 250, train_loss 0.705\n",
      "epoch 7, batch 260, train_loss 0.701\n",
      "epoch 7, batch 270, train_loss 0.702\n",
      "epoch 7, batch 280, train_loss 0.700\n",
      "epoch 7, batch 290, train_loss 0.704\n",
      "epoch 7, batch 300, train_loss 0.705\n",
      "epoch 7, batch 310, train_loss 0.696\n",
      "epoch 7, batch 320, train_loss 0.706\n",
      "epoch 7, batch 330, train_loss 0.700\n",
      "epoch 7, batch 340, train_loss 0.698\n",
      "epoch 7, batch 350, train_loss 0.713\n",
      "epoch 7, batch 360, train_loss 0.712\n",
      "epoch 7, batch 370, train_loss 0.701\n",
      "epoch 7, batch 380, train_loss 0.700\n",
      "epoch 7, batch 390, train_loss 0.705\n",
      "epoch 7, batch 400, train_loss 0.706\n",
      "epoch 7, batch 410, train_loss 0.706\n",
      "epoch 7, batch 420, train_loss 0.710\n",
      "epoch 7, batch 430, train_loss 0.705\n",
      "epoch 7, batch 440, train_loss 0.704\n",
      "epoch 7, batch 450, train_loss 0.707\n",
      "epoch 7, batch 460, train_loss 0.700\n",
      "epoch 7, batch 470, train_loss 0.707\n",
      "epoch 7, batch 480, train_loss 0.698\n",
      "epoch 7, batch 490, train_loss 0.705\n",
      "epoch 7, batch 500, train_loss 0.703\n",
      "epoch 7, batch 510, train_loss 0.706\n",
      "epoch 7, batch 520, train_loss 0.709\n",
      "epoch 7, batch 530, train_loss 0.702\n",
      "epoch 7, batch 540, train_loss 0.702\n",
      "epoch 7, batch 550, train_loss 0.706\n",
      "epoch 7, batch 560, train_loss 0.697\n",
      "epoch 7, batch 570, train_loss 0.710\n",
      "epoch 7, batch 580, train_loss 0.700\n",
      "epoch 7, batch 590, train_loss 0.707\n",
      "epoch 7, batch 600, train_loss 0.701\n",
      "epoch 7, batch 610, train_loss 0.715\n",
      "epoch 7, batch 620, train_loss 0.705\n",
      "epoch 7, batch 630, train_loss 0.700\n",
      "epoch 7, batch 640, train_loss 0.697\n",
      "epoch 7, batch 650, train_loss 0.703\n",
      "epoch 7, batch 660, train_loss 0.703\n",
      "epoch 7, batch 670, train_loss 0.695\n",
      "epoch 7, batch 680, train_loss 0.705\n",
      "epoch 7, batch 690, train_loss 0.701\n",
      "epoch 7, batch 700, train_loss 0.700\n",
      "epoch 7, batch 710, train_loss 0.709\n",
      "epoch 7, batch 720, train_loss 0.705\n",
      "epoch 7, batch 730, train_loss 0.699\n",
      "epoch 7, batch 740, train_loss 0.696\n",
      "epoch 7, batch 750, train_loss 0.693\n",
      "epoch 7, batch 760, train_loss 0.705\n",
      "epoch 7, batch 770, train_loss 0.702\n",
      "epoch 7, batch 780, train_loss 0.705\n",
      "epoch 7, batch 790, train_loss 0.707\n",
      "epoch 7, batch 800, train_loss 0.706\n",
      "epoch 7, batch 810, train_loss 0.701\n",
      "epoch 7, batch 820, train_loss 0.711\n",
      "epoch 7, batch 830, train_loss 0.704\n",
      "epoch 7, batch 840, train_loss 0.703\n",
      "epoch 7, batch 850, train_loss 0.704\n",
      "epoch 7, batch 860, train_loss 0.709\n",
      "epoch 7, batch 870, train_loss 0.703\n",
      "epoch 7, batch 880, train_loss 0.705\n",
      "epoch 7, batch 890, train_loss 0.706\n",
      "epoch 7, batch 900, train_loss 0.704\n",
      "epoch 7, batch 910, train_loss 0.705\n",
      "epoch 7, batch 920, train_loss 0.700\n",
      "epoch 7, batch 930, train_loss 0.702\n",
      "epoch 7, batch 940, train_loss 0.708\n",
      "epoch 7, batch 950, train_loss 0.702\n",
      "epoch 7, batch 960, train_loss 0.701\n",
      "epoch 7, batch 970, train_loss 0.701\n",
      "epoch 7, batch 980, train_loss 0.705\n",
      "epoch 7, batch 990, train_loss 0.706\n",
      "epoch 7, batch 1000, train_loss 0.704\n",
      "epoch 7, batch 1010, train_loss 0.707\n",
      "epoch 7, batch 1020, train_loss 0.707\n",
      "epoch 7, batch 1030, train_loss 0.706\n",
      "epoch 7, batch 1040, train_loss 0.695\n",
      "epoch 7, batch 1050, train_loss 0.698\n",
      "epoch 7, batch 1060, train_loss 0.706\n",
      "epoch 7, batch 1070, train_loss 0.700\n",
      "epoch 7, batch 1080, train_loss 0.716\n",
      "epoch 7, batch 1090, train_loss 0.699\n",
      "epoch 7, batch 1100, train_loss 0.707\n",
      "epoch 7, batch 1110, train_loss 0.698\n",
      "epoch 7, batch 1120, train_loss 0.703\n",
      "epoch 7, batch 1130, train_loss 0.699\n",
      "epoch 7, batch 1140, train_loss 0.698\n",
      "epoch 7, batch 1150, train_loss 0.715\n",
      "epoch 7, batch 1160, train_loss 0.699\n",
      "epoch 7, batch 1170, train_loss 0.702\n",
      "epoch 7, batch 1180, train_loss 0.703\n",
      "epoch 7, batch 1190, train_loss 0.708\n",
      "epoch 7, train_loss 0.702, valid_loss 0.722, train_accuracy  69.67%, valid_accuracy  68.58%\n",
      "model saved with highest valid accuracy:  68.58%\n",
      "epoch 8, batch 0, train_loss 0.710\n",
      "epoch 8, batch 10, train_loss 0.708\n",
      "epoch 8, batch 20, train_loss 0.704\n",
      "epoch 8, batch 30, train_loss 0.704\n",
      "epoch 8, batch 40, train_loss 0.700\n",
      "epoch 8, batch 50, train_loss 0.700\n",
      "epoch 8, batch 60, train_loss 0.704\n",
      "epoch 8, batch 70, train_loss 0.706\n",
      "epoch 8, batch 80, train_loss 0.701\n",
      "epoch 8, batch 90, train_loss 0.708\n",
      "epoch 8, batch 100, train_loss 0.705\n",
      "epoch 8, batch 110, train_loss 0.695\n",
      "epoch 8, batch 120, train_loss 0.696\n",
      "epoch 8, batch 130, train_loss 0.702\n",
      "epoch 8, batch 140, train_loss 0.699\n",
      "epoch 8, batch 150, train_loss 0.707\n",
      "epoch 8, batch 160, train_loss 0.704\n",
      "epoch 8, batch 170, train_loss 0.713\n",
      "epoch 8, batch 180, train_loss 0.706\n",
      "epoch 8, batch 190, train_loss 0.705\n",
      "epoch 8, batch 200, train_loss 0.698\n",
      "epoch 8, batch 210, train_loss 0.702\n",
      "epoch 8, batch 220, train_loss 0.707\n",
      "epoch 8, batch 230, train_loss 0.710\n",
      "epoch 8, batch 240, train_loss 0.704\n",
      "epoch 8, batch 250, train_loss 0.699\n",
      "epoch 8, batch 260, train_loss 0.708\n",
      "epoch 8, batch 270, train_loss 0.697\n",
      "epoch 8, batch 280, train_loss 0.708\n",
      "epoch 8, batch 290, train_loss 0.706\n",
      "epoch 8, batch 300, train_loss 0.695\n",
      "epoch 8, batch 310, train_loss 0.700\n",
      "epoch 8, batch 320, train_loss 0.699\n",
      "epoch 8, batch 330, train_loss 0.707\n",
      "epoch 8, batch 340, train_loss 0.706\n",
      "epoch 8, batch 350, train_loss 0.693\n",
      "epoch 8, batch 360, train_loss 0.695\n",
      "epoch 8, batch 370, train_loss 0.693\n",
      "epoch 8, batch 380, train_loss 0.711\n",
      "epoch 8, batch 390, train_loss 0.712\n",
      "epoch 8, batch 400, train_loss 0.709\n",
      "epoch 8, batch 410, train_loss 0.706\n",
      "epoch 8, batch 420, train_loss 0.706\n",
      "epoch 8, batch 430, train_loss 0.685\n",
      "epoch 8, batch 440, train_loss 0.708\n",
      "epoch 8, batch 450, train_loss 0.715\n",
      "epoch 8, batch 460, train_loss 0.703\n",
      "epoch 8, batch 470, train_loss 0.700\n",
      "epoch 8, batch 480, train_loss 0.716\n",
      "epoch 8, batch 490, train_loss 0.714\n",
      "epoch 8, batch 500, train_loss 0.701\n",
      "epoch 8, batch 510, train_loss 0.698\n",
      "epoch 8, batch 520, train_loss 0.700\n",
      "epoch 8, batch 530, train_loss 0.701\n",
      "epoch 8, batch 540, train_loss 0.709\n",
      "epoch 8, batch 550, train_loss 0.713\n",
      "epoch 8, batch 560, train_loss 0.703\n",
      "epoch 8, batch 570, train_loss 0.696\n",
      "epoch 8, batch 580, train_loss 0.711\n",
      "epoch 8, batch 590, train_loss 0.708\n",
      "epoch 8, batch 600, train_loss 0.705\n",
      "epoch 8, batch 610, train_loss 0.705\n",
      "epoch 8, batch 620, train_loss 0.703\n",
      "epoch 8, batch 630, train_loss 0.699\n",
      "epoch 8, batch 640, train_loss 0.702\n",
      "epoch 8, batch 650, train_loss 0.709\n",
      "epoch 8, batch 660, train_loss 0.708\n",
      "epoch 8, batch 670, train_loss 0.698\n",
      "epoch 8, batch 680, train_loss 0.706\n",
      "epoch 8, batch 690, train_loss 0.702\n",
      "epoch 8, batch 700, train_loss 0.702\n",
      "epoch 8, batch 710, train_loss 0.684\n",
      "epoch 8, batch 720, train_loss 0.703\n",
      "epoch 8, batch 730, train_loss 0.703\n",
      "epoch 8, batch 740, train_loss 0.711\n",
      "epoch 8, batch 750, train_loss 0.696\n",
      "epoch 8, batch 760, train_loss 0.706\n",
      "epoch 8, batch 770, train_loss 0.699\n",
      "epoch 8, batch 780, train_loss 0.707\n",
      "epoch 8, batch 790, train_loss 0.703\n",
      "epoch 8, batch 800, train_loss 0.699\n",
      "epoch 8, batch 810, train_loss 0.710\n",
      "epoch 8, batch 820, train_loss 0.701\n",
      "epoch 8, batch 830, train_loss 0.702\n",
      "epoch 8, batch 840, train_loss 0.718\n",
      "epoch 8, batch 850, train_loss 0.698\n",
      "epoch 8, batch 860, train_loss 0.701\n",
      "epoch 8, batch 870, train_loss 0.700\n",
      "epoch 8, batch 880, train_loss 0.702\n",
      "epoch 8, batch 890, train_loss 0.700\n",
      "epoch 8, batch 900, train_loss 0.708\n",
      "epoch 8, batch 910, train_loss 0.705\n",
      "epoch 8, batch 920, train_loss 0.695\n",
      "epoch 8, batch 930, train_loss 0.701\n",
      "epoch 8, batch 940, train_loss 0.706\n",
      "epoch 8, batch 950, train_loss 0.701\n",
      "epoch 8, batch 960, train_loss 0.697\n",
      "epoch 8, batch 970, train_loss 0.712\n",
      "epoch 8, batch 980, train_loss 0.700\n",
      "epoch 8, batch 990, train_loss 0.696\n",
      "epoch 8, batch 1000, train_loss 0.702\n",
      "epoch 8, batch 1010, train_loss 0.703\n",
      "epoch 8, batch 1020, train_loss 0.709\n",
      "epoch 8, batch 1030, train_loss 0.702\n",
      "epoch 8, batch 1040, train_loss 0.685\n",
      "epoch 8, batch 1050, train_loss 0.694\n",
      "epoch 8, batch 1060, train_loss 0.699\n",
      "epoch 8, batch 1070, train_loss 0.709\n",
      "epoch 8, batch 1080, train_loss 0.698\n",
      "epoch 8, batch 1090, train_loss 0.698\n",
      "epoch 8, batch 1100, train_loss 0.705\n",
      "epoch 8, batch 1110, train_loss 0.705\n",
      "epoch 8, batch 1120, train_loss 0.703\n",
      "epoch 8, batch 1130, train_loss 0.708\n",
      "epoch 8, batch 1140, train_loss 0.703\n",
      "epoch 8, batch 1150, train_loss 0.692\n",
      "epoch 8, batch 1160, train_loss 0.705\n",
      "epoch 8, batch 1170, train_loss 0.706\n",
      "epoch 8, batch 1180, train_loss 0.696\n",
      "epoch 8, batch 1190, train_loss 0.710\n",
      "epoch 8, train_loss 0.702, valid_loss 0.722, train_accuracy  69.70%, valid_accuracy  68.59%\n",
      "model saved with highest valid accuracy:  68.59%\n",
      "epoch 9, batch 0, train_loss 0.713\n",
      "epoch 9, batch 10, train_loss 0.706\n",
      "epoch 9, batch 20, train_loss 0.702\n",
      "epoch 9, batch 30, train_loss 0.699\n",
      "epoch 9, batch 40, train_loss 0.694\n",
      "epoch 9, batch 50, train_loss 0.701\n",
      "epoch 9, batch 60, train_loss 0.703\n",
      "epoch 9, batch 70, train_loss 0.691\n",
      "epoch 9, batch 80, train_loss 0.696\n",
      "epoch 9, batch 90, train_loss 0.711\n",
      "epoch 9, batch 100, train_loss 0.694\n",
      "epoch 9, batch 110, train_loss 0.708\n",
      "epoch 9, batch 120, train_loss 0.705\n",
      "epoch 9, batch 130, train_loss 0.698\n",
      "epoch 9, batch 140, train_loss 0.702\n",
      "epoch 9, batch 150, train_loss 0.705\n",
      "epoch 9, batch 160, train_loss 0.704\n",
      "epoch 9, batch 170, train_loss 0.705\n",
      "epoch 9, batch 180, train_loss 0.709\n",
      "epoch 9, batch 190, train_loss 0.702\n",
      "epoch 9, batch 200, train_loss 0.701\n",
      "epoch 9, batch 210, train_loss 0.696\n",
      "epoch 9, batch 220, train_loss 0.696\n",
      "epoch 9, batch 230, train_loss 0.708\n",
      "epoch 9, batch 240, train_loss 0.710\n",
      "epoch 9, batch 250, train_loss 0.709\n",
      "epoch 9, batch 260, train_loss 0.685\n",
      "epoch 9, batch 270, train_loss 0.707\n",
      "epoch 9, batch 280, train_loss 0.694\n",
      "epoch 9, batch 290, train_loss 0.710\n",
      "epoch 9, batch 300, train_loss 0.697\n",
      "epoch 9, batch 310, train_loss 0.710\n",
      "epoch 9, batch 320, train_loss 0.708\n",
      "epoch 9, batch 330, train_loss 0.704\n",
      "epoch 9, batch 340, train_loss 0.707\n",
      "epoch 9, batch 350, train_loss 0.710\n",
      "epoch 9, batch 360, train_loss 0.700\n",
      "epoch 9, batch 370, train_loss 0.697\n",
      "epoch 9, batch 380, train_loss 0.695\n",
      "epoch 9, batch 390, train_loss 0.702\n",
      "epoch 9, batch 400, train_loss 0.689\n",
      "epoch 9, batch 410, train_loss 0.715\n",
      "epoch 9, batch 420, train_loss 0.710\n",
      "epoch 9, batch 430, train_loss 0.703\n",
      "epoch 9, batch 440, train_loss 0.706\n",
      "epoch 9, batch 450, train_loss 0.708\n",
      "epoch 9, batch 460, train_loss 0.701\n",
      "epoch 9, batch 470, train_loss 0.692\n",
      "epoch 9, batch 480, train_loss 0.694\n",
      "epoch 9, batch 490, train_loss 0.712\n",
      "epoch 9, batch 500, train_loss 0.702\n",
      "epoch 9, batch 510, train_loss 0.706\n",
      "epoch 9, batch 520, train_loss 0.691\n",
      "epoch 9, batch 530, train_loss 0.700\n",
      "epoch 9, batch 540, train_loss 0.709\n",
      "epoch 9, batch 550, train_loss 0.707\n",
      "epoch 9, batch 560, train_loss 0.704\n",
      "epoch 9, batch 570, train_loss 0.693\n",
      "epoch 9, batch 580, train_loss 0.702\n",
      "epoch 9, batch 590, train_loss 0.694\n",
      "epoch 9, batch 600, train_loss 0.709\n",
      "epoch 9, batch 610, train_loss 0.699\n",
      "epoch 9, batch 620, train_loss 0.698\n",
      "epoch 9, batch 630, train_loss 0.710\n",
      "epoch 9, batch 640, train_loss 0.705\n",
      "epoch 9, batch 650, train_loss 0.707\n",
      "epoch 9, batch 660, train_loss 0.703\n",
      "epoch 9, batch 670, train_loss 0.704\n",
      "epoch 9, batch 680, train_loss 0.707\n",
      "epoch 9, batch 690, train_loss 0.700\n",
      "epoch 9, batch 700, train_loss 0.698\n",
      "epoch 9, batch 710, train_loss 0.704\n",
      "epoch 9, batch 720, train_loss 0.703\n",
      "epoch 9, batch 730, train_loss 0.706\n",
      "epoch 9, batch 740, train_loss 0.694\n",
      "epoch 9, batch 750, train_loss 0.702\n",
      "epoch 9, batch 760, train_loss 0.705\n",
      "epoch 9, batch 770, train_loss 0.705\n",
      "epoch 9, batch 780, train_loss 0.699\n",
      "epoch 9, batch 790, train_loss 0.702\n",
      "epoch 9, batch 800, train_loss 0.704\n",
      "epoch 9, batch 810, train_loss 0.697\n",
      "epoch 9, batch 820, train_loss 0.704\n",
      "epoch 9, batch 830, train_loss 0.708\n",
      "epoch 9, batch 840, train_loss 0.703\n",
      "epoch 9, batch 850, train_loss 0.696\n",
      "epoch 9, batch 860, train_loss 0.698\n",
      "epoch 9, batch 870, train_loss 0.704\n",
      "epoch 9, batch 880, train_loss 0.699\n",
      "epoch 9, batch 890, train_loss 0.708\n",
      "epoch 9, batch 900, train_loss 0.701\n",
      "epoch 9, batch 910, train_loss 0.704\n",
      "epoch 9, batch 920, train_loss 0.706\n",
      "epoch 9, batch 930, train_loss 0.706\n",
      "epoch 9, batch 940, train_loss 0.703\n",
      "epoch 9, batch 950, train_loss 0.708\n",
      "epoch 9, batch 960, train_loss 0.702\n",
      "epoch 9, batch 970, train_loss 0.700\n",
      "epoch 9, batch 980, train_loss 0.696\n",
      "epoch 9, batch 990, train_loss 0.697\n",
      "epoch 9, batch 1000, train_loss 0.704\n",
      "epoch 9, batch 1010, train_loss 0.705\n",
      "epoch 9, batch 1020, train_loss 0.696\n",
      "epoch 9, batch 1030, train_loss 0.694\n",
      "epoch 9, batch 1040, train_loss 0.701\n",
      "epoch 9, batch 1050, train_loss 0.699\n",
      "epoch 9, batch 1060, train_loss 0.694\n",
      "epoch 9, batch 1070, train_loss 0.699\n",
      "epoch 9, batch 1080, train_loss 0.706\n",
      "epoch 9, batch 1090, train_loss 0.698\n",
      "epoch 9, batch 1100, train_loss 0.706\n",
      "epoch 9, batch 1110, train_loss 0.699\n",
      "epoch 9, batch 1120, train_loss 0.704\n",
      "epoch 9, batch 1130, train_loss 0.709\n",
      "epoch 9, batch 1140, train_loss 0.706\n",
      "epoch 9, batch 1150, train_loss 0.698\n",
      "epoch 9, batch 1160, train_loss 0.701\n",
      "epoch 9, batch 1170, train_loss 0.704\n",
      "epoch 9, batch 1180, train_loss 0.702\n",
      "epoch 9, batch 1190, train_loss 0.691\n",
      "epoch 9, train_loss 0.701, valid_loss 0.722, train_accuracy  69.74%, valid_accuracy  68.61%\n",
      "model saved with highest valid accuracy:  68.61%\n",
      "epoch 10, batch 0, train_loss 0.696\n",
      "epoch 10, batch 10, train_loss 0.699\n",
      "epoch 10, batch 20, train_loss 0.695\n",
      "epoch 10, batch 30, train_loss 0.699\n",
      "epoch 10, batch 40, train_loss 0.698\n",
      "epoch 10, batch 50, train_loss 0.694\n",
      "epoch 10, batch 60, train_loss 0.712\n",
      "epoch 10, batch 70, train_loss 0.708\n",
      "epoch 10, batch 80, train_loss 0.702\n",
      "epoch 10, batch 90, train_loss 0.692\n",
      "epoch 10, batch 100, train_loss 0.709\n",
      "epoch 10, batch 110, train_loss 0.705\n",
      "epoch 10, batch 120, train_loss 0.706\n",
      "epoch 10, batch 130, train_loss 0.696\n",
      "epoch 10, batch 140, train_loss 0.705\n",
      "epoch 10, batch 150, train_loss 0.699\n",
      "epoch 10, batch 160, train_loss 0.703\n",
      "epoch 10, batch 170, train_loss 0.706\n",
      "epoch 10, batch 180, train_loss 0.704\n",
      "epoch 10, batch 190, train_loss 0.702\n",
      "epoch 10, batch 200, train_loss 0.695\n",
      "epoch 10, batch 210, train_loss 0.690\n",
      "epoch 10, batch 220, train_loss 0.706\n",
      "epoch 10, batch 230, train_loss 0.703\n",
      "epoch 10, batch 240, train_loss 0.706\n",
      "epoch 10, batch 250, train_loss 0.699\n",
      "epoch 10, batch 260, train_loss 0.704\n",
      "epoch 10, batch 270, train_loss 0.704\n",
      "epoch 10, batch 280, train_loss 0.703\n",
      "epoch 10, batch 290, train_loss 0.700\n",
      "epoch 10, batch 300, train_loss 0.696\n",
      "epoch 10, batch 310, train_loss 0.712\n",
      "epoch 10, batch 320, train_loss 0.698\n",
      "epoch 10, batch 330, train_loss 0.704\n",
      "epoch 10, batch 340, train_loss 0.704\n",
      "epoch 10, batch 350, train_loss 0.708\n",
      "epoch 10, batch 360, train_loss 0.701\n",
      "epoch 10, batch 370, train_loss 0.707\n",
      "epoch 10, batch 380, train_loss 0.705\n",
      "epoch 10, batch 390, train_loss 0.685\n",
      "epoch 10, batch 400, train_loss 0.709\n",
      "epoch 10, batch 410, train_loss 0.703\n",
      "epoch 10, batch 420, train_loss 0.690\n",
      "epoch 10, batch 430, train_loss 0.698\n",
      "epoch 10, batch 440, train_loss 0.700\n",
      "epoch 10, batch 450, train_loss 0.705\n",
      "epoch 10, batch 460, train_loss 0.702\n",
      "epoch 10, batch 470, train_loss 0.705\n",
      "epoch 10, batch 480, train_loss 0.704\n",
      "epoch 10, batch 490, train_loss 0.703\n",
      "epoch 10, batch 500, train_loss 0.711\n",
      "epoch 10, batch 510, train_loss 0.705\n",
      "epoch 10, batch 520, train_loss 0.693\n",
      "epoch 10, batch 530, train_loss 0.699\n",
      "epoch 10, batch 540, train_loss 0.698\n",
      "epoch 10, batch 550, train_loss 0.703\n",
      "epoch 10, batch 560, train_loss 0.697\n",
      "epoch 10, batch 570, train_loss 0.706\n",
      "epoch 10, batch 580, train_loss 0.703\n",
      "epoch 10, batch 590, train_loss 0.709\n",
      "epoch 10, batch 600, train_loss 0.699\n",
      "epoch 10, batch 610, train_loss 0.696\n",
      "epoch 10, batch 620, train_loss 0.705\n",
      "epoch 10, batch 630, train_loss 0.708\n",
      "epoch 10, batch 640, train_loss 0.698\n",
      "epoch 10, batch 650, train_loss 0.703\n",
      "epoch 10, batch 660, train_loss 0.708\n",
      "epoch 10, batch 670, train_loss 0.702\n",
      "epoch 10, batch 680, train_loss 0.707\n",
      "epoch 10, batch 690, train_loss 0.700\n",
      "epoch 10, batch 700, train_loss 0.704\n",
      "epoch 10, batch 710, train_loss 0.692\n",
      "epoch 10, batch 720, train_loss 0.694\n",
      "epoch 10, batch 730, train_loss 0.702\n",
      "epoch 10, batch 740, train_loss 0.701\n",
      "epoch 10, batch 750, train_loss 0.697\n",
      "epoch 10, batch 760, train_loss 0.708\n",
      "epoch 10, batch 770, train_loss 0.695\n",
      "epoch 10, batch 780, train_loss 0.693\n",
      "epoch 10, batch 790, train_loss 0.699\n",
      "epoch 10, batch 800, train_loss 0.696\n",
      "epoch 10, batch 810, train_loss 0.693\n",
      "epoch 10, batch 820, train_loss 0.701\n",
      "epoch 10, batch 830, train_loss 0.713\n",
      "epoch 10, batch 840, train_loss 0.704\n",
      "epoch 10, batch 850, train_loss 0.703\n",
      "epoch 10, batch 860, train_loss 0.702\n",
      "epoch 10, batch 870, train_loss 0.699\n",
      "epoch 10, batch 880, train_loss 0.710\n",
      "epoch 10, batch 890, train_loss 0.698\n",
      "epoch 10, batch 900, train_loss 0.707\n",
      "epoch 10, batch 910, train_loss 0.698\n",
      "epoch 10, batch 920, train_loss 0.697\n",
      "epoch 10, batch 930, train_loss 0.701\n",
      "epoch 10, batch 940, train_loss 0.702\n",
      "epoch 10, batch 950, train_loss 0.698\n",
      "epoch 10, batch 960, train_loss 0.703\n",
      "epoch 10, batch 970, train_loss 0.699\n",
      "epoch 10, batch 980, train_loss 0.693\n",
      "epoch 10, batch 990, train_loss 0.699\n",
      "epoch 10, batch 1000, train_loss 0.696\n",
      "epoch 10, batch 1010, train_loss 0.704\n",
      "epoch 10, batch 1020, train_loss 0.697\n",
      "epoch 10, batch 1030, train_loss 0.696\n",
      "epoch 10, batch 1040, train_loss 0.694\n",
      "epoch 10, batch 1050, train_loss 0.704\n",
      "epoch 10, batch 1060, train_loss 0.702\n",
      "epoch 10, batch 1070, train_loss 0.703\n",
      "epoch 10, batch 1080, train_loss 0.702\n",
      "epoch 10, batch 1090, train_loss 0.703\n",
      "epoch 10, batch 1100, train_loss 0.705\n",
      "epoch 10, batch 1110, train_loss 0.708\n",
      "epoch 10, batch 1120, train_loss 0.702\n",
      "epoch 10, batch 1130, train_loss 0.707\n",
      "epoch 10, batch 1140, train_loss 0.700\n",
      "epoch 10, batch 1150, train_loss 0.694\n",
      "epoch 10, batch 1160, train_loss 0.701\n",
      "epoch 10, batch 1170, train_loss 0.704\n",
      "epoch 10, batch 1180, train_loss 0.703\n",
      "epoch 10, batch 1190, train_loss 0.705\n",
      "epoch 10, train_loss 0.699, valid_loss 0.722, train_accuracy  69.83%, valid_accuracy  68.62%\n",
      "model saved with highest valid accuracy:  68.62%\n",
      "epoch 11, batch 0, train_loss 0.689\n",
      "epoch 11, batch 10, train_loss 0.701\n",
      "epoch 11, batch 20, train_loss 0.709\n",
      "epoch 11, batch 30, train_loss 0.693\n",
      "epoch 11, batch 40, train_loss 0.702\n",
      "epoch 11, batch 50, train_loss 0.704\n",
      "epoch 11, batch 60, train_loss 0.687\n",
      "epoch 11, batch 70, train_loss 0.696\n",
      "epoch 11, batch 80, train_loss 0.694\n",
      "epoch 11, batch 90, train_loss 0.705\n",
      "epoch 11, batch 100, train_loss 0.689\n",
      "epoch 11, batch 110, train_loss 0.698\n",
      "epoch 11, batch 120, train_loss 0.698\n",
      "epoch 11, batch 130, train_loss 0.705\n",
      "epoch 11, batch 140, train_loss 0.710\n",
      "epoch 11, batch 150, train_loss 0.703\n",
      "epoch 11, batch 160, train_loss 0.699\n",
      "epoch 11, batch 170, train_loss 0.701\n",
      "epoch 11, batch 180, train_loss 0.704\n",
      "epoch 11, batch 190, train_loss 0.705\n",
      "epoch 11, batch 200, train_loss 0.707\n",
      "epoch 11, batch 210, train_loss 0.699\n",
      "epoch 11, batch 220, train_loss 0.707\n",
      "epoch 11, batch 230, train_loss 0.710\n",
      "epoch 11, batch 240, train_loss 0.697\n",
      "epoch 11, batch 250, train_loss 0.701\n",
      "epoch 11, batch 260, train_loss 0.697\n",
      "epoch 11, batch 270, train_loss 0.699\n",
      "epoch 11, batch 280, train_loss 0.712\n",
      "epoch 11, batch 290, train_loss 0.693\n",
      "epoch 11, batch 300, train_loss 0.708\n",
      "epoch 11, batch 310, train_loss 0.699\n",
      "epoch 11, batch 320, train_loss 0.703\n",
      "epoch 11, batch 330, train_loss 0.701\n",
      "epoch 11, batch 340, train_loss 0.705\n",
      "epoch 11, batch 350, train_loss 0.696\n",
      "epoch 11, batch 360, train_loss 0.702\n",
      "epoch 11, batch 370, train_loss 0.709\n",
      "epoch 11, batch 380, train_loss 0.706\n",
      "epoch 11, batch 390, train_loss 0.701\n",
      "epoch 11, batch 400, train_loss 0.699\n",
      "epoch 11, batch 410, train_loss 0.699\n",
      "epoch 11, batch 420, train_loss 0.699\n",
      "epoch 11, batch 430, train_loss 0.701\n",
      "epoch 11, batch 440, train_loss 0.699\n",
      "epoch 11, batch 450, train_loss 0.701\n",
      "epoch 11, batch 460, train_loss 0.695\n",
      "epoch 11, batch 470, train_loss 0.701\n",
      "epoch 11, batch 480, train_loss 0.690\n",
      "epoch 11, batch 490, train_loss 0.703\n",
      "epoch 11, batch 500, train_loss 0.702\n",
      "epoch 11, batch 510, train_loss 0.707\n",
      "epoch 11, batch 520, train_loss 0.694\n",
      "epoch 11, batch 530, train_loss 0.713\n",
      "epoch 11, batch 540, train_loss 0.713\n",
      "epoch 11, batch 550, train_loss 0.693\n",
      "epoch 11, batch 560, train_loss 0.709\n",
      "epoch 11, batch 570, train_loss 0.696\n",
      "epoch 11, batch 580, train_loss 0.697\n",
      "epoch 11, batch 590, train_loss 0.693\n",
      "epoch 11, batch 600, train_loss 0.704\n",
      "epoch 11, batch 610, train_loss 0.700\n",
      "epoch 11, batch 620, train_loss 0.701\n",
      "epoch 11, batch 630, train_loss 0.708\n",
      "epoch 11, batch 640, train_loss 0.701\n",
      "epoch 11, batch 650, train_loss 0.704\n",
      "epoch 11, batch 660, train_loss 0.695\n",
      "epoch 11, batch 670, train_loss 0.700\n",
      "epoch 11, batch 680, train_loss 0.697\n",
      "epoch 11, batch 690, train_loss 0.696\n",
      "epoch 11, batch 700, train_loss 0.710\n",
      "epoch 11, batch 710, train_loss 0.691\n",
      "epoch 11, batch 720, train_loss 0.697\n",
      "epoch 11, batch 730, train_loss 0.701\n",
      "epoch 11, batch 740, train_loss 0.707\n",
      "epoch 11, batch 750, train_loss 0.711\n",
      "epoch 11, batch 760, train_loss 0.700\n",
      "epoch 11, batch 770, train_loss 0.699\n",
      "epoch 11, batch 780, train_loss 0.697\n",
      "epoch 11, batch 790, train_loss 0.699\n",
      "epoch 11, batch 800, train_loss 0.706\n",
      "epoch 11, batch 810, train_loss 0.710\n",
      "epoch 11, batch 820, train_loss 0.709\n",
      "epoch 11, batch 830, train_loss 0.702\n",
      "epoch 11, batch 840, train_loss 0.706\n",
      "epoch 11, batch 850, train_loss 0.696\n",
      "epoch 11, batch 860, train_loss 0.712\n",
      "epoch 11, batch 870, train_loss 0.707\n",
      "epoch 11, batch 880, train_loss 0.712\n",
      "epoch 11, batch 890, train_loss 0.696\n",
      "epoch 11, batch 900, train_loss 0.690\n",
      "epoch 11, batch 910, train_loss 0.692\n",
      "epoch 11, batch 920, train_loss 0.695\n",
      "epoch 11, batch 930, train_loss 0.691\n",
      "epoch 11, batch 940, train_loss 0.696\n",
      "epoch 11, batch 950, train_loss 0.695\n",
      "epoch 11, batch 960, train_loss 0.698\n",
      "epoch 11, batch 970, train_loss 0.702\n",
      "epoch 11, batch 980, train_loss 0.698\n",
      "epoch 11, batch 990, train_loss 0.698\n",
      "epoch 11, batch 1000, train_loss 0.708\n",
      "epoch 11, batch 1010, train_loss 0.707\n",
      "epoch 11, batch 1020, train_loss 0.712\n",
      "epoch 11, batch 1030, train_loss 0.692\n",
      "epoch 11, batch 1040, train_loss 0.695\n",
      "epoch 11, batch 1050, train_loss 0.697\n",
      "epoch 11, batch 1060, train_loss 0.705\n",
      "epoch 11, batch 1070, train_loss 0.705\n",
      "epoch 11, batch 1080, train_loss 0.703\n",
      "epoch 11, batch 1090, train_loss 0.705\n",
      "epoch 11, batch 1100, train_loss 0.697\n",
      "epoch 11, batch 1110, train_loss 0.696\n",
      "epoch 11, batch 1120, train_loss 0.702\n",
      "epoch 11, batch 1130, train_loss 0.704\n",
      "epoch 11, batch 1140, train_loss 0.693\n",
      "epoch 11, batch 1150, train_loss 0.713\n",
      "epoch 11, batch 1160, train_loss 0.706\n",
      "epoch 11, batch 1170, train_loss 0.700\n",
      "epoch 11, batch 1180, train_loss 0.705\n",
      "epoch 11, batch 1190, train_loss 0.704\n",
      "epoch 11, train_loss 0.699, valid_loss 0.723, train_accuracy  69.85%, valid_accuracy  68.57%\n",
      "epoch 12, batch 0, train_loss 0.696\n",
      "epoch 12, batch 10, train_loss 0.692\n",
      "epoch 12, batch 20, train_loss 0.693\n",
      "epoch 12, batch 30, train_loss 0.705\n",
      "epoch 12, batch 40, train_loss 0.694\n",
      "epoch 12, batch 50, train_loss 0.706\n",
      "epoch 12, batch 60, train_loss 0.703\n",
      "epoch 12, batch 70, train_loss 0.707\n",
      "epoch 12, batch 80, train_loss 0.699\n",
      "epoch 12, batch 90, train_loss 0.702\n",
      "epoch 12, batch 100, train_loss 0.699\n",
      "epoch 12, batch 110, train_loss 0.704\n",
      "epoch 12, batch 120, train_loss 0.696\n",
      "epoch 12, batch 130, train_loss 0.696\n",
      "epoch 12, batch 140, train_loss 0.695\n",
      "epoch 12, batch 150, train_loss 0.700\n",
      "epoch 12, batch 160, train_loss 0.695\n",
      "epoch 12, batch 170, train_loss 0.702\n",
      "epoch 12, batch 180, train_loss 0.698\n",
      "epoch 12, batch 190, train_loss 0.696\n",
      "epoch 12, batch 200, train_loss 0.706\n",
      "epoch 12, batch 210, train_loss 0.706\n",
      "epoch 12, batch 220, train_loss 0.692\n",
      "epoch 12, batch 230, train_loss 0.702\n",
      "epoch 12, batch 240, train_loss 0.697\n",
      "epoch 12, batch 250, train_loss 0.700\n",
      "epoch 12, batch 260, train_loss 0.694\n",
      "epoch 12, batch 270, train_loss 0.695\n",
      "epoch 12, batch 280, train_loss 0.698\n",
      "epoch 12, batch 290, train_loss 0.696\n",
      "epoch 12, batch 300, train_loss 0.703\n",
      "epoch 12, batch 310, train_loss 0.693\n",
      "epoch 12, batch 320, train_loss 0.697\n",
      "epoch 12, batch 330, train_loss 0.696\n",
      "epoch 12, batch 340, train_loss 0.688\n",
      "epoch 12, batch 350, train_loss 0.703\n",
      "epoch 12, batch 360, train_loss 0.695\n",
      "epoch 12, batch 370, train_loss 0.699\n",
      "epoch 12, batch 380, train_loss 0.699\n",
      "epoch 12, batch 390, train_loss 0.693\n",
      "epoch 12, batch 400, train_loss 0.698\n",
      "epoch 12, batch 410, train_loss 0.702\n",
      "epoch 12, batch 420, train_loss 0.695\n",
      "epoch 12, batch 430, train_loss 0.700\n",
      "epoch 12, batch 440, train_loss 0.701\n",
      "epoch 12, batch 450, train_loss 0.688\n",
      "epoch 12, batch 460, train_loss 0.704\n",
      "epoch 12, batch 470, train_loss 0.696\n",
      "epoch 12, batch 480, train_loss 0.693\n",
      "epoch 12, batch 490, train_loss 0.697\n",
      "epoch 12, batch 500, train_loss 0.696\n",
      "epoch 12, batch 510, train_loss 0.702\n",
      "epoch 12, batch 520, train_loss 0.697\n",
      "epoch 12, batch 530, train_loss 0.711\n",
      "epoch 12, batch 540, train_loss 0.702\n",
      "epoch 12, batch 550, train_loss 0.695\n",
      "epoch 12, batch 560, train_loss 0.700\n",
      "epoch 12, batch 570, train_loss 0.702\n",
      "epoch 12, batch 580, train_loss 0.701\n",
      "epoch 12, batch 590, train_loss 0.702\n",
      "epoch 12, batch 600, train_loss 0.696\n",
      "epoch 12, batch 610, train_loss 0.703\n",
      "epoch 12, batch 620, train_loss 0.700\n",
      "epoch 12, batch 630, train_loss 0.691\n",
      "epoch 12, batch 640, train_loss 0.705\n",
      "epoch 12, batch 650, train_loss 0.702\n",
      "epoch 12, batch 660, train_loss 0.695\n",
      "epoch 12, batch 670, train_loss 0.696\n",
      "epoch 12, batch 680, train_loss 0.713\n",
      "epoch 12, batch 690, train_loss 0.702\n",
      "epoch 12, batch 700, train_loss 0.703\n",
      "epoch 12, batch 710, train_loss 0.690\n",
      "epoch 12, batch 720, train_loss 0.700\n",
      "epoch 12, batch 730, train_loss 0.698\n",
      "epoch 12, batch 740, train_loss 0.708\n",
      "epoch 12, batch 750, train_loss 0.707\n",
      "epoch 12, batch 760, train_loss 0.703\n",
      "epoch 12, batch 770, train_loss 0.698\n",
      "epoch 12, batch 780, train_loss 0.701\n",
      "epoch 12, batch 790, train_loss 0.703\n",
      "epoch 12, batch 800, train_loss 0.704\n",
      "epoch 12, batch 810, train_loss 0.701\n",
      "epoch 12, batch 820, train_loss 0.703\n",
      "epoch 12, batch 830, train_loss 0.686\n",
      "epoch 12, batch 840, train_loss 0.707\n",
      "epoch 12, batch 850, train_loss 0.701\n",
      "epoch 12, batch 860, train_loss 0.693\n",
      "epoch 12, batch 870, train_loss 0.711\n",
      "epoch 12, batch 880, train_loss 0.700\n",
      "epoch 12, batch 890, train_loss 0.695\n",
      "epoch 12, batch 900, train_loss 0.700\n",
      "epoch 12, batch 910, train_loss 0.697\n",
      "epoch 12, batch 920, train_loss 0.693\n",
      "epoch 12, batch 930, train_loss 0.704\n",
      "epoch 12, batch 940, train_loss 0.694\n",
      "epoch 12, batch 950, train_loss 0.698\n",
      "epoch 12, batch 960, train_loss 0.708\n",
      "epoch 12, batch 970, train_loss 0.701\n",
      "epoch 12, batch 980, train_loss 0.702\n",
      "epoch 12, batch 990, train_loss 0.706\n",
      "epoch 12, batch 1000, train_loss 0.706\n",
      "epoch 12, batch 1010, train_loss 0.704\n",
      "epoch 12, batch 1020, train_loss 0.696\n",
      "epoch 12, batch 1030, train_loss 0.704\n",
      "epoch 12, batch 1040, train_loss 0.691\n",
      "epoch 12, batch 1050, train_loss 0.692\n",
      "epoch 12, batch 1060, train_loss 0.691\n",
      "epoch 12, batch 1070, train_loss 0.694\n",
      "epoch 12, batch 1080, train_loss 0.693\n",
      "epoch 12, batch 1090, train_loss 0.693\n",
      "epoch 12, batch 1100, train_loss 0.692\n",
      "epoch 12, batch 1110, train_loss 0.705\n",
      "epoch 12, batch 1120, train_loss 0.701\n",
      "epoch 12, batch 1130, train_loss 0.694\n",
      "epoch 12, batch 1140, train_loss 0.697\n",
      "epoch 12, batch 1150, train_loss 0.710\n",
      "epoch 12, batch 1160, train_loss 0.697\n",
      "epoch 12, batch 1170, train_loss 0.690\n",
      "epoch 12, batch 1180, train_loss 0.705\n",
      "epoch 12, batch 1190, train_loss 0.695\n",
      "epoch 12, train_loss 0.699, valid_loss 0.722, train_accuracy  69.86%, valid_accuracy  68.56%\n",
      "epoch 13, batch 0, train_loss 0.702\n",
      "epoch 13, batch 10, train_loss 0.689\n",
      "epoch 13, batch 20, train_loss 0.700\n",
      "epoch 13, batch 30, train_loss 0.695\n",
      "epoch 13, batch 40, train_loss 0.696\n",
      "epoch 13, batch 50, train_loss 0.700\n",
      "epoch 13, batch 60, train_loss 0.693\n",
      "epoch 13, batch 70, train_loss 0.703\n",
      "epoch 13, batch 80, train_loss 0.692\n",
      "epoch 13, batch 90, train_loss 0.683\n",
      "epoch 13, batch 100, train_loss 0.705\n",
      "epoch 13, batch 110, train_loss 0.695\n",
      "epoch 13, batch 120, train_loss 0.695\n",
      "epoch 13, batch 130, train_loss 0.703\n",
      "epoch 13, batch 140, train_loss 0.694\n",
      "epoch 13, batch 150, train_loss 0.707\n",
      "epoch 13, batch 160, train_loss 0.695\n",
      "epoch 13, batch 170, train_loss 0.703\n",
      "epoch 13, batch 180, train_loss 0.701\n",
      "epoch 13, batch 190, train_loss 0.715\n",
      "epoch 13, batch 200, train_loss 0.702\n",
      "epoch 13, batch 210, train_loss 0.694\n",
      "epoch 13, batch 220, train_loss 0.709\n",
      "epoch 13, batch 230, train_loss 0.698\n",
      "epoch 13, batch 240, train_loss 0.700\n",
      "epoch 13, batch 250, train_loss 0.706\n",
      "epoch 13, batch 260, train_loss 0.689\n",
      "epoch 13, batch 270, train_loss 0.702\n",
      "epoch 13, batch 280, train_loss 0.704\n",
      "epoch 13, batch 290, train_loss 0.691\n",
      "epoch 13, batch 300, train_loss 0.699\n",
      "epoch 13, batch 310, train_loss 0.702\n",
      "epoch 13, batch 320, train_loss 0.706\n",
      "epoch 13, batch 330, train_loss 0.690\n",
      "epoch 13, batch 340, train_loss 0.703\n",
      "epoch 13, batch 350, train_loss 0.717\n",
      "epoch 13, batch 360, train_loss 0.704\n",
      "epoch 13, batch 370, train_loss 0.698\n",
      "epoch 13, batch 380, train_loss 0.694\n",
      "epoch 13, batch 390, train_loss 0.709\n",
      "epoch 13, batch 400, train_loss 0.697\n",
      "epoch 13, batch 410, train_loss 0.698\n",
      "epoch 13, batch 420, train_loss 0.693\n",
      "epoch 13, batch 430, train_loss 0.701\n",
      "epoch 13, batch 440, train_loss 0.703\n",
      "epoch 13, batch 450, train_loss 0.690\n",
      "epoch 13, batch 460, train_loss 0.712\n",
      "epoch 13, batch 470, train_loss 0.696\n",
      "epoch 13, batch 480, train_loss 0.698\n",
      "epoch 13, batch 490, train_loss 0.701\n",
      "epoch 13, batch 500, train_loss 0.702\n",
      "epoch 13, batch 510, train_loss 0.702\n",
      "epoch 13, batch 520, train_loss 0.699\n",
      "epoch 13, batch 530, train_loss 0.702\n",
      "epoch 13, batch 540, train_loss 0.693\n",
      "epoch 13, batch 550, train_loss 0.701\n",
      "epoch 13, batch 560, train_loss 0.700\n",
      "epoch 13, batch 570, train_loss 0.704\n",
      "epoch 13, batch 580, train_loss 0.697\n",
      "epoch 13, batch 590, train_loss 0.705\n",
      "epoch 13, batch 600, train_loss 0.692\n",
      "epoch 13, batch 610, train_loss 0.695\n",
      "epoch 13, batch 620, train_loss 0.699\n",
      "epoch 13, batch 630, train_loss 0.709\n",
      "epoch 13, batch 640, train_loss 0.700\n",
      "epoch 13, batch 650, train_loss 0.710\n",
      "epoch 13, batch 660, train_loss 0.704\n",
      "epoch 13, batch 670, train_loss 0.694\n",
      "epoch 13, batch 680, train_loss 0.694\n",
      "epoch 13, batch 690, train_loss 0.691\n",
      "epoch 13, batch 700, train_loss 0.699\n",
      "epoch 13, batch 710, train_loss 0.702\n",
      "epoch 13, batch 720, train_loss 0.702\n",
      "epoch 13, batch 730, train_loss 0.707\n",
      "epoch 13, batch 740, train_loss 0.695\n",
      "epoch 13, batch 750, train_loss 0.703\n",
      "epoch 13, batch 760, train_loss 0.699\n",
      "epoch 13, batch 770, train_loss 0.688\n",
      "epoch 13, batch 780, train_loss 0.705\n",
      "epoch 13, batch 790, train_loss 0.697\n",
      "epoch 13, batch 800, train_loss 0.695\n",
      "epoch 13, batch 810, train_loss 0.700\n",
      "epoch 13, batch 820, train_loss 0.696\n",
      "epoch 13, batch 830, train_loss 0.703\n",
      "epoch 13, batch 840, train_loss 0.706\n",
      "epoch 13, batch 850, train_loss 0.703\n",
      "epoch 13, batch 860, train_loss 0.706\n",
      "epoch 13, batch 870, train_loss 0.702\n",
      "epoch 13, batch 880, train_loss 0.700\n",
      "epoch 13, batch 890, train_loss 0.704\n",
      "epoch 13, batch 900, train_loss 0.705\n",
      "epoch 13, batch 910, train_loss 0.698\n",
      "epoch 13, batch 920, train_loss 0.694\n",
      "epoch 13, batch 930, train_loss 0.688\n",
      "epoch 13, batch 940, train_loss 0.699\n",
      "epoch 13, batch 950, train_loss 0.698\n",
      "epoch 13, batch 960, train_loss 0.689\n",
      "epoch 13, batch 970, train_loss 0.693\n",
      "epoch 13, batch 980, train_loss 0.686\n",
      "epoch 13, batch 990, train_loss 0.700\n",
      "epoch 13, batch 1000, train_loss 0.701\n",
      "epoch 13, batch 1010, train_loss 0.703\n",
      "epoch 13, batch 1020, train_loss 0.692\n",
      "epoch 13, batch 1030, train_loss 0.695\n",
      "epoch 13, batch 1040, train_loss 0.705\n",
      "epoch 13, batch 1050, train_loss 0.702\n",
      "epoch 13, batch 1060, train_loss 0.697\n",
      "epoch 13, batch 1070, train_loss 0.704\n",
      "epoch 13, batch 1080, train_loss 0.699\n",
      "epoch 13, batch 1090, train_loss 0.710\n",
      "epoch 13, batch 1100, train_loss 0.695\n",
      "epoch 13, batch 1110, train_loss 0.697\n",
      "epoch 13, batch 1120, train_loss 0.696\n",
      "epoch 13, batch 1130, train_loss 0.695\n",
      "epoch 13, batch 1140, train_loss 0.696\n",
      "epoch 13, batch 1150, train_loss 0.706\n",
      "epoch 13, batch 1160, train_loss 0.712\n",
      "epoch 13, batch 1170, train_loss 0.703\n",
      "epoch 13, batch 1180, train_loss 0.690\n",
      "epoch 13, batch 1190, train_loss 0.703\n",
      "epoch 13, train_loss 0.698, valid_loss 0.722, train_accuracy  69.93%, valid_accuracy  68.63%\n",
      "model saved with highest valid accuracy:  68.63%\n",
      "epoch 14, batch 0, train_loss 0.691\n",
      "epoch 14, batch 10, train_loss 0.696\n",
      "epoch 14, batch 20, train_loss 0.701\n",
      "epoch 14, batch 30, train_loss 0.697\n",
      "epoch 14, batch 40, train_loss 0.697\n",
      "epoch 14, batch 50, train_loss 0.695\n",
      "epoch 14, batch 60, train_loss 0.701\n",
      "epoch 14, batch 70, train_loss 0.700\n",
      "epoch 14, batch 80, train_loss 0.702\n",
      "epoch 14, batch 90, train_loss 0.706\n",
      "epoch 14, batch 100, train_loss 0.703\n",
      "epoch 14, batch 110, train_loss 0.687\n",
      "epoch 14, batch 120, train_loss 0.682\n",
      "epoch 14, batch 130, train_loss 0.694\n",
      "epoch 14, batch 140, train_loss 0.705\n",
      "epoch 14, batch 150, train_loss 0.700\n",
      "epoch 14, batch 160, train_loss 0.699\n",
      "epoch 14, batch 170, train_loss 0.695\n",
      "epoch 14, batch 180, train_loss 0.697\n",
      "epoch 14, batch 190, train_loss 0.690\n",
      "epoch 14, batch 200, train_loss 0.696\n",
      "epoch 14, batch 210, train_loss 0.696\n",
      "epoch 14, batch 220, train_loss 0.701\n",
      "epoch 14, batch 230, train_loss 0.708\n",
      "epoch 14, batch 240, train_loss 0.695\n",
      "epoch 14, batch 250, train_loss 0.692\n",
      "epoch 14, batch 260, train_loss 0.703\n",
      "epoch 14, batch 270, train_loss 0.699\n",
      "epoch 14, batch 280, train_loss 0.706\n",
      "epoch 14, batch 290, train_loss 0.695\n",
      "epoch 14, batch 300, train_loss 0.715\n",
      "epoch 14, batch 310, train_loss 0.697\n",
      "epoch 14, batch 320, train_loss 0.696\n",
      "epoch 14, batch 330, train_loss 0.698\n",
      "epoch 14, batch 340, train_loss 0.711\n",
      "epoch 14, batch 350, train_loss 0.700\n",
      "epoch 14, batch 360, train_loss 0.699\n",
      "epoch 14, batch 370, train_loss 0.693\n",
      "epoch 14, batch 380, train_loss 0.709\n",
      "epoch 14, batch 390, train_loss 0.708\n",
      "epoch 14, batch 400, train_loss 0.697\n",
      "epoch 14, batch 410, train_loss 0.708\n",
      "epoch 14, batch 420, train_loss 0.695\n",
      "epoch 14, batch 430, train_loss 0.686\n",
      "epoch 14, batch 440, train_loss 0.709\n",
      "epoch 14, batch 450, train_loss 0.704\n",
      "epoch 14, batch 460, train_loss 0.700\n",
      "epoch 14, batch 470, train_loss 0.707\n",
      "epoch 14, batch 480, train_loss 0.699\n",
      "epoch 14, batch 490, train_loss 0.693\n",
      "epoch 14, batch 500, train_loss 0.699\n",
      "epoch 14, batch 510, train_loss 0.698\n",
      "epoch 14, batch 520, train_loss 0.701\n",
      "epoch 14, batch 530, train_loss 0.703\n",
      "epoch 14, batch 540, train_loss 0.696\n",
      "epoch 14, batch 550, train_loss 0.715\n",
      "epoch 14, batch 560, train_loss 0.707\n",
      "epoch 14, batch 570, train_loss 0.697\n",
      "epoch 14, batch 580, train_loss 0.690\n",
      "epoch 14, batch 590, train_loss 0.704\n",
      "epoch 14, batch 600, train_loss 0.698\n",
      "epoch 14, batch 610, train_loss 0.702\n",
      "epoch 14, batch 620, train_loss 0.693\n",
      "epoch 14, batch 630, train_loss 0.701\n",
      "epoch 14, batch 640, train_loss 0.704\n",
      "epoch 14, batch 650, train_loss 0.706\n",
      "epoch 14, batch 660, train_loss 0.713\n",
      "epoch 14, batch 670, train_loss 0.704\n",
      "epoch 14, batch 680, train_loss 0.690\n",
      "epoch 14, batch 690, train_loss 0.699\n",
      "epoch 14, batch 700, train_loss 0.693\n",
      "epoch 14, batch 710, train_loss 0.702\n",
      "epoch 14, batch 720, train_loss 0.696\n",
      "epoch 14, batch 730, train_loss 0.694\n",
      "epoch 14, batch 740, train_loss 0.705\n",
      "epoch 14, batch 750, train_loss 0.706\n",
      "epoch 14, batch 760, train_loss 0.699\n",
      "epoch 14, batch 770, train_loss 0.692\n",
      "epoch 14, batch 780, train_loss 0.704\n",
      "epoch 14, batch 790, train_loss 0.703\n",
      "epoch 14, batch 800, train_loss 0.698\n",
      "epoch 14, batch 810, train_loss 0.697\n",
      "epoch 14, batch 820, train_loss 0.700\n",
      "epoch 14, batch 830, train_loss 0.704\n",
      "epoch 14, batch 840, train_loss 0.682\n",
      "epoch 14, batch 850, train_loss 0.702\n",
      "epoch 14, batch 860, train_loss 0.701\n",
      "epoch 14, batch 870, train_loss 0.700\n",
      "epoch 14, batch 880, train_loss 0.699\n",
      "epoch 14, batch 890, train_loss 0.702\n",
      "epoch 14, batch 900, train_loss 0.701\n",
      "epoch 14, batch 910, train_loss 0.705\n",
      "epoch 14, batch 920, train_loss 0.708\n",
      "epoch 14, batch 930, train_loss 0.701\n",
      "epoch 14, batch 940, train_loss 0.699\n",
      "epoch 14, batch 950, train_loss 0.705\n",
      "epoch 14, batch 960, train_loss 0.704\n",
      "epoch 14, batch 970, train_loss 0.701\n",
      "epoch 14, batch 980, train_loss 0.706\n",
      "epoch 14, batch 990, train_loss 0.690\n",
      "epoch 14, batch 1000, train_loss 0.696\n",
      "epoch 14, batch 1010, train_loss 0.698\n",
      "epoch 14, batch 1020, train_loss 0.689\n",
      "epoch 14, batch 1030, train_loss 0.699\n",
      "epoch 14, batch 1040, train_loss 0.697\n",
      "epoch 14, batch 1050, train_loss 0.700\n",
      "epoch 14, batch 1060, train_loss 0.703\n",
      "epoch 14, batch 1070, train_loss 0.692\n",
      "epoch 14, batch 1080, train_loss 0.691\n",
      "epoch 14, batch 1090, train_loss 0.698\n",
      "epoch 14, batch 1100, train_loss 0.686\n",
      "epoch 14, batch 1110, train_loss 0.704\n",
      "epoch 14, batch 1120, train_loss 0.696\n",
      "epoch 14, batch 1130, train_loss 0.701\n",
      "epoch 14, batch 1140, train_loss 0.690\n",
      "epoch 14, batch 1150, train_loss 0.706\n",
      "epoch 14, batch 1160, train_loss 0.696\n",
      "epoch 14, batch 1170, train_loss 0.697\n",
      "epoch 14, batch 1180, train_loss 0.698\n",
      "epoch 14, batch 1190, train_loss 0.714\n",
      "epoch 14, train_loss 0.697, valid_loss 0.721, train_accuracy  69.97%, valid_accuracy  68.60%\n",
      "epoch 15, batch 0, train_loss 0.695\n",
      "epoch 15, batch 10, train_loss 0.705\n",
      "epoch 15, batch 20, train_loss 0.691\n",
      "epoch 15, batch 30, train_loss 0.694\n",
      "epoch 15, batch 40, train_loss 0.709\n",
      "epoch 15, batch 50, train_loss 0.696\n",
      "epoch 15, batch 60, train_loss 0.689\n",
      "epoch 15, batch 70, train_loss 0.701\n",
      "epoch 15, batch 80, train_loss 0.698\n",
      "epoch 15, batch 90, train_loss 0.695\n",
      "epoch 15, batch 100, train_loss 0.707\n",
      "epoch 15, batch 110, train_loss 0.700\n",
      "epoch 15, batch 120, train_loss 0.697\n",
      "epoch 15, batch 130, train_loss 0.705\n",
      "epoch 15, batch 140, train_loss 0.697\n",
      "epoch 15, batch 150, train_loss 0.704\n",
      "epoch 15, batch 160, train_loss 0.702\n",
      "epoch 15, batch 170, train_loss 0.694\n",
      "epoch 15, batch 180, train_loss 0.699\n",
      "epoch 15, batch 190, train_loss 0.691\n",
      "epoch 15, batch 200, train_loss 0.711\n",
      "epoch 15, batch 210, train_loss 0.700\n",
      "epoch 15, batch 220, train_loss 0.694\n",
      "epoch 15, batch 230, train_loss 0.695\n",
      "epoch 15, batch 240, train_loss 0.697\n",
      "epoch 15, batch 250, train_loss 0.701\n",
      "epoch 15, batch 260, train_loss 0.704\n",
      "epoch 15, batch 270, train_loss 0.694\n",
      "epoch 15, batch 280, train_loss 0.703\n",
      "epoch 15, batch 290, train_loss 0.696\n",
      "epoch 15, batch 300, train_loss 0.700\n",
      "epoch 15, batch 310, train_loss 0.693\n",
      "epoch 15, batch 320, train_loss 0.706\n",
      "epoch 15, batch 330, train_loss 0.689\n",
      "epoch 15, batch 340, train_loss 0.695\n",
      "epoch 15, batch 350, train_loss 0.696\n",
      "epoch 15, batch 360, train_loss 0.702\n",
      "epoch 15, batch 370, train_loss 0.694\n",
      "epoch 15, batch 380, train_loss 0.696\n",
      "epoch 15, batch 390, train_loss 0.690\n",
      "epoch 15, batch 400, train_loss 0.697\n",
      "epoch 15, batch 410, train_loss 0.699\n",
      "epoch 15, batch 420, train_loss 0.700\n",
      "epoch 15, batch 430, train_loss 0.695\n",
      "epoch 15, batch 440, train_loss 0.702\n",
      "epoch 15, batch 450, train_loss 0.697\n",
      "epoch 15, batch 460, train_loss 0.698\n",
      "epoch 15, batch 470, train_loss 0.703\n",
      "epoch 15, batch 480, train_loss 0.703\n",
      "epoch 15, batch 490, train_loss 0.702\n",
      "epoch 15, batch 500, train_loss 0.700\n",
      "epoch 15, batch 510, train_loss 0.698\n",
      "epoch 15, batch 520, train_loss 0.686\n",
      "epoch 15, batch 530, train_loss 0.692\n",
      "epoch 15, batch 540, train_loss 0.696\n",
      "epoch 15, batch 550, train_loss 0.700\n",
      "epoch 15, batch 560, train_loss 0.697\n",
      "epoch 15, batch 570, train_loss 0.706\n",
      "epoch 15, batch 580, train_loss 0.703\n",
      "epoch 15, batch 590, train_loss 0.702\n",
      "epoch 15, batch 600, train_loss 0.701\n",
      "epoch 15, batch 610, train_loss 0.702\n",
      "epoch 15, batch 620, train_loss 0.698\n",
      "epoch 15, batch 630, train_loss 0.705\n",
      "epoch 15, batch 640, train_loss 0.691\n",
      "epoch 15, batch 650, train_loss 0.708\n",
      "epoch 15, batch 660, train_loss 0.697\n",
      "epoch 15, batch 670, train_loss 0.689\n",
      "epoch 15, batch 680, train_loss 0.693\n",
      "epoch 15, batch 690, train_loss 0.693\n",
      "epoch 15, batch 700, train_loss 0.711\n",
      "epoch 15, batch 710, train_loss 0.707\n",
      "epoch 15, batch 720, train_loss 0.693\n",
      "epoch 15, batch 730, train_loss 0.701\n",
      "epoch 15, batch 740, train_loss 0.698\n",
      "epoch 15, batch 750, train_loss 0.705\n",
      "epoch 15, batch 760, train_loss 0.699\n",
      "epoch 15, batch 770, train_loss 0.704\n",
      "epoch 15, batch 780, train_loss 0.691\n",
      "epoch 15, batch 790, train_loss 0.709\n",
      "epoch 15, batch 800, train_loss 0.710\n",
      "epoch 15, batch 810, train_loss 0.711\n",
      "epoch 15, batch 820, train_loss 0.708\n",
      "epoch 15, batch 830, train_loss 0.691\n",
      "epoch 15, batch 840, train_loss 0.714\n",
      "epoch 15, batch 850, train_loss 0.700\n",
      "epoch 15, batch 860, train_loss 0.695\n",
      "epoch 15, batch 870, train_loss 0.703\n",
      "epoch 15, batch 880, train_loss 0.694\n",
      "epoch 15, batch 890, train_loss 0.711\n",
      "epoch 15, batch 900, train_loss 0.694\n",
      "epoch 15, batch 910, train_loss 0.688\n",
      "epoch 15, batch 920, train_loss 0.703\n",
      "epoch 15, batch 930, train_loss 0.698\n",
      "epoch 15, batch 940, train_loss 0.694\n",
      "epoch 15, batch 950, train_loss 0.698\n",
      "epoch 15, batch 960, train_loss 0.697\n",
      "epoch 15, batch 970, train_loss 0.705\n",
      "epoch 15, batch 980, train_loss 0.713\n",
      "epoch 15, batch 990, train_loss 0.703\n",
      "epoch 15, batch 1000, train_loss 0.691\n",
      "epoch 15, batch 1010, train_loss 0.692\n",
      "epoch 15, batch 1020, train_loss 0.715\n",
      "epoch 15, batch 1030, train_loss 0.699\n",
      "epoch 15, batch 1040, train_loss 0.703\n",
      "epoch 15, batch 1050, train_loss 0.699\n",
      "epoch 15, batch 1060, train_loss 0.699\n",
      "epoch 15, batch 1070, train_loss 0.700\n",
      "epoch 15, batch 1080, train_loss 0.696\n",
      "epoch 15, batch 1090, train_loss 0.693\n",
      "epoch 15, batch 1100, train_loss 0.702\n",
      "epoch 15, batch 1110, train_loss 0.702\n",
      "epoch 15, batch 1120, train_loss 0.696\n",
      "epoch 15, batch 1130, train_loss 0.699\n",
      "epoch 15, batch 1140, train_loss 0.696\n",
      "epoch 15, batch 1150, train_loss 0.701\n",
      "epoch 15, batch 1160, train_loss 0.693\n",
      "epoch 15, batch 1170, train_loss 0.700\n",
      "epoch 15, batch 1180, train_loss 0.703\n",
      "epoch 15, batch 1190, train_loss 0.697\n",
      "epoch 15, train_loss 0.697, valid_loss 0.722, train_accuracy  69.98%, valid_accuracy  68.58%\n",
      "epoch 16, batch 0, train_loss 0.702\n",
      "epoch 16, batch 10, train_loss 0.694\n",
      "epoch 16, batch 20, train_loss 0.692\n",
      "epoch 16, batch 30, train_loss 0.697\n",
      "epoch 16, batch 40, train_loss 0.707\n",
      "epoch 16, batch 50, train_loss 0.695\n",
      "epoch 16, batch 60, train_loss 0.691\n",
      "epoch 16, batch 70, train_loss 0.689\n",
      "epoch 16, batch 80, train_loss 0.708\n",
      "epoch 16, batch 90, train_loss 0.703\n",
      "epoch 16, batch 100, train_loss 0.696\n",
      "epoch 16, batch 110, train_loss 0.698\n",
      "epoch 16, batch 120, train_loss 0.699\n",
      "epoch 16, batch 130, train_loss 0.695\n",
      "epoch 16, batch 140, train_loss 0.690\n",
      "epoch 16, batch 150, train_loss 0.694\n",
      "epoch 16, batch 160, train_loss 0.696\n",
      "epoch 16, batch 170, train_loss 0.699\n",
      "epoch 16, batch 180, train_loss 0.691\n",
      "epoch 16, batch 190, train_loss 0.690\n",
      "epoch 16, batch 200, train_loss 0.694\n",
      "epoch 16, batch 210, train_loss 0.691\n",
      "epoch 16, batch 220, train_loss 0.710\n",
      "epoch 16, batch 230, train_loss 0.689\n",
      "epoch 16, batch 240, train_loss 0.699\n",
      "epoch 16, batch 250, train_loss 0.707\n",
      "epoch 16, batch 260, train_loss 0.695\n",
      "epoch 16, batch 270, train_loss 0.683\n",
      "epoch 16, batch 280, train_loss 0.699\n",
      "epoch 16, batch 290, train_loss 0.693\n",
      "epoch 16, batch 300, train_loss 0.696\n",
      "epoch 16, batch 310, train_loss 0.706\n",
      "epoch 16, batch 320, train_loss 0.704\n",
      "epoch 16, batch 330, train_loss 0.697\n",
      "epoch 16, batch 340, train_loss 0.697\n",
      "epoch 16, batch 350, train_loss 0.676\n",
      "epoch 16, batch 360, train_loss 0.696\n",
      "epoch 16, batch 370, train_loss 0.695\n",
      "epoch 16, batch 380, train_loss 0.698\n",
      "epoch 16, batch 390, train_loss 0.702\n",
      "epoch 16, batch 400, train_loss 0.696\n",
      "epoch 16, batch 410, train_loss 0.692\n",
      "epoch 16, batch 420, train_loss 0.693\n",
      "epoch 16, batch 430, train_loss 0.689\n",
      "epoch 16, batch 440, train_loss 0.702\n",
      "epoch 16, batch 450, train_loss 0.692\n",
      "epoch 16, batch 460, train_loss 0.705\n",
      "epoch 16, batch 470, train_loss 0.703\n",
      "epoch 16, batch 480, train_loss 0.692\n",
      "epoch 16, batch 490, train_loss 0.699\n",
      "epoch 16, batch 500, train_loss 0.699\n",
      "epoch 16, batch 510, train_loss 0.699\n",
      "epoch 16, batch 520, train_loss 0.702\n",
      "epoch 16, batch 530, train_loss 0.703\n",
      "epoch 16, batch 540, train_loss 0.698\n",
      "epoch 16, batch 550, train_loss 0.691\n",
      "epoch 16, batch 560, train_loss 0.694\n",
      "epoch 16, batch 570, train_loss 0.689\n",
      "epoch 16, batch 580, train_loss 0.704\n",
      "epoch 16, batch 590, train_loss 0.702\n",
      "epoch 16, batch 600, train_loss 0.702\n",
      "epoch 16, batch 610, train_loss 0.693\n",
      "epoch 16, batch 620, train_loss 0.696\n",
      "epoch 16, batch 630, train_loss 0.702\n",
      "epoch 16, batch 640, train_loss 0.690\n",
      "epoch 16, batch 650, train_loss 0.695\n",
      "epoch 16, batch 660, train_loss 0.710\n",
      "epoch 16, batch 670, train_loss 0.704\n",
      "epoch 16, batch 680, train_loss 0.692\n",
      "epoch 16, batch 690, train_loss 0.693\n",
      "epoch 16, batch 700, train_loss 0.693\n",
      "epoch 16, batch 710, train_loss 0.711\n",
      "epoch 16, batch 720, train_loss 0.699\n",
      "epoch 16, batch 730, train_loss 0.700\n",
      "epoch 16, batch 740, train_loss 0.703\n",
      "epoch 16, batch 750, train_loss 0.698\n",
      "epoch 16, batch 760, train_loss 0.698\n",
      "epoch 16, batch 770, train_loss 0.703\n",
      "epoch 16, batch 780, train_loss 0.696\n",
      "epoch 16, batch 790, train_loss 0.702\n",
      "epoch 16, batch 800, train_loss 0.699\n",
      "epoch 16, batch 810, train_loss 0.689\n",
      "epoch 16, batch 820, train_loss 0.694\n",
      "epoch 16, batch 830, train_loss 0.694\n",
      "epoch 16, batch 840, train_loss 0.708\n",
      "epoch 16, batch 850, train_loss 0.699\n",
      "epoch 16, batch 860, train_loss 0.690\n",
      "epoch 16, batch 870, train_loss 0.690\n",
      "epoch 16, batch 880, train_loss 0.692\n",
      "epoch 16, batch 890, train_loss 0.703\n",
      "epoch 16, batch 900, train_loss 0.701\n",
      "epoch 16, batch 910, train_loss 0.697\n",
      "epoch 16, batch 920, train_loss 0.707\n",
      "epoch 16, batch 930, train_loss 0.687\n",
      "epoch 16, batch 940, train_loss 0.697\n",
      "epoch 16, batch 950, train_loss 0.702\n",
      "epoch 16, batch 960, train_loss 0.696\n",
      "epoch 16, batch 970, train_loss 0.707\n",
      "epoch 16, batch 980, train_loss 0.698\n",
      "epoch 16, batch 990, train_loss 0.694\n",
      "epoch 16, batch 1000, train_loss 0.703\n",
      "epoch 16, batch 1010, train_loss 0.682\n",
      "epoch 16, batch 1020, train_loss 0.707\n",
      "epoch 16, batch 1030, train_loss 0.697\n",
      "epoch 16, batch 1040, train_loss 0.701\n",
      "epoch 16, batch 1050, train_loss 0.698\n",
      "epoch 16, batch 1060, train_loss 0.696\n",
      "epoch 16, batch 1070, train_loss 0.688\n",
      "epoch 16, batch 1080, train_loss 0.698\n",
      "epoch 16, batch 1090, train_loss 0.700\n",
      "epoch 16, batch 1100, train_loss 0.694\n",
      "epoch 16, batch 1110, train_loss 0.703\n",
      "epoch 16, batch 1120, train_loss 0.695\n",
      "epoch 16, batch 1130, train_loss 0.699\n",
      "epoch 16, batch 1140, train_loss 0.692\n",
      "epoch 16, batch 1150, train_loss 0.694\n",
      "epoch 16, batch 1160, train_loss 0.701\n",
      "epoch 16, batch 1170, train_loss 0.695\n",
      "epoch 16, batch 1180, train_loss 0.701\n",
      "epoch 16, batch 1190, train_loss 0.697\n",
      "epoch 16, train_loss 0.697, valid_loss 0.722, train_accuracy  70.00%, valid_accuracy  68.57%\n",
      "epoch 17, batch 0, train_loss 0.697\n",
      "epoch 17, batch 10, train_loss 0.691\n",
      "epoch 17, batch 20, train_loss 0.698\n",
      "epoch 17, batch 30, train_loss 0.693\n",
      "epoch 17, batch 40, train_loss 0.701\n",
      "epoch 17, batch 50, train_loss 0.695\n",
      "epoch 17, batch 60, train_loss 0.695\n",
      "epoch 17, batch 70, train_loss 0.694\n",
      "epoch 17, batch 80, train_loss 0.698\n",
      "epoch 17, batch 90, train_loss 0.696\n",
      "epoch 17, batch 100, train_loss 0.701\n",
      "epoch 17, batch 110, train_loss 0.696\n",
      "epoch 17, batch 120, train_loss 0.695\n",
      "epoch 17, batch 130, train_loss 0.698\n",
      "epoch 17, batch 140, train_loss 0.697\n",
      "epoch 17, batch 150, train_loss 0.697\n",
      "epoch 17, batch 160, train_loss 0.699\n",
      "epoch 17, batch 170, train_loss 0.705\n",
      "epoch 17, batch 180, train_loss 0.701\n",
      "epoch 17, batch 190, train_loss 0.697\n",
      "epoch 17, batch 200, train_loss 0.697\n",
      "epoch 17, batch 210, train_loss 0.699\n",
      "epoch 17, batch 220, train_loss 0.700\n",
      "epoch 17, batch 230, train_loss 0.695\n",
      "epoch 17, batch 240, train_loss 0.702\n",
      "epoch 17, batch 250, train_loss 0.701\n",
      "epoch 17, batch 260, train_loss 0.699\n",
      "epoch 17, batch 270, train_loss 0.694\n",
      "epoch 17, batch 280, train_loss 0.697\n",
      "epoch 17, batch 290, train_loss 0.698\n",
      "epoch 17, batch 300, train_loss 0.704\n",
      "epoch 17, batch 310, train_loss 0.707\n",
      "epoch 17, batch 320, train_loss 0.687\n",
      "epoch 17, batch 330, train_loss 0.697\n",
      "epoch 17, batch 340, train_loss 0.696\n",
      "epoch 17, batch 350, train_loss 0.698\n",
      "epoch 17, batch 360, train_loss 0.696\n",
      "epoch 17, batch 370, train_loss 0.693\n",
      "epoch 17, batch 380, train_loss 0.696\n",
      "epoch 17, batch 390, train_loss 0.705\n",
      "epoch 17, batch 400, train_loss 0.693\n",
      "epoch 17, batch 410, train_loss 0.700\n",
      "epoch 17, batch 420, train_loss 0.694\n",
      "epoch 17, batch 430, train_loss 0.697\n",
      "epoch 17, batch 440, train_loss 0.705\n",
      "epoch 17, batch 450, train_loss 0.695\n",
      "epoch 17, batch 460, train_loss 0.707\n",
      "epoch 17, batch 470, train_loss 0.692\n",
      "epoch 17, batch 480, train_loss 0.704\n",
      "epoch 17, batch 490, train_loss 0.702\n",
      "epoch 17, batch 500, train_loss 0.693\n",
      "epoch 17, batch 510, train_loss 0.693\n",
      "epoch 17, batch 520, train_loss 0.697\n",
      "epoch 17, batch 530, train_loss 0.696\n",
      "epoch 17, batch 540, train_loss 0.691\n",
      "epoch 17, batch 550, train_loss 0.697\n",
      "epoch 17, batch 560, train_loss 0.696\n",
      "epoch 17, batch 570, train_loss 0.698\n",
      "epoch 17, batch 580, train_loss 0.697\n",
      "epoch 17, batch 590, train_loss 0.692\n",
      "epoch 17, batch 600, train_loss 0.690\n",
      "epoch 17, batch 610, train_loss 0.694\n",
      "epoch 17, batch 620, train_loss 0.695\n",
      "epoch 17, batch 630, train_loss 0.692\n",
      "epoch 17, batch 640, train_loss 0.695\n",
      "epoch 17, batch 650, train_loss 0.697\n",
      "epoch 17, batch 660, train_loss 0.692\n",
      "epoch 17, batch 670, train_loss 0.692\n",
      "epoch 17, batch 680, train_loss 0.708\n",
      "epoch 17, batch 690, train_loss 0.706\n",
      "epoch 17, batch 700, train_loss 0.708\n",
      "epoch 17, batch 710, train_loss 0.696\n",
      "epoch 17, batch 720, train_loss 0.702\n",
      "epoch 17, batch 730, train_loss 0.696\n",
      "epoch 17, batch 740, train_loss 0.704\n",
      "epoch 17, batch 750, train_loss 0.696\n",
      "epoch 17, batch 760, train_loss 0.691\n",
      "epoch 17, batch 770, train_loss 0.696\n",
      "epoch 17, batch 780, train_loss 0.699\n",
      "epoch 17, batch 790, train_loss 0.700\n",
      "epoch 17, batch 800, train_loss 0.700\n",
      "epoch 17, batch 810, train_loss 0.702\n",
      "epoch 17, batch 820, train_loss 0.695\n",
      "epoch 17, batch 830, train_loss 0.701\n",
      "epoch 17, batch 840, train_loss 0.696\n",
      "epoch 17, batch 850, train_loss 0.697\n",
      "epoch 17, batch 860, train_loss 0.695\n",
      "epoch 17, batch 870, train_loss 0.699\n",
      "epoch 17, batch 880, train_loss 0.703\n",
      "epoch 17, batch 890, train_loss 0.698\n",
      "epoch 17, batch 900, train_loss 0.688\n",
      "epoch 17, batch 910, train_loss 0.697\n",
      "epoch 17, batch 920, train_loss 0.700\n",
      "epoch 17, batch 930, train_loss 0.698\n",
      "epoch 17, batch 940, train_loss 0.690\n",
      "epoch 17, batch 950, train_loss 0.688\n",
      "epoch 17, batch 960, train_loss 0.693\n",
      "epoch 17, batch 970, train_loss 0.707\n",
      "epoch 17, batch 980, train_loss 0.702\n",
      "epoch 17, batch 990, train_loss 0.697\n",
      "epoch 17, batch 1000, train_loss 0.704\n",
      "epoch 17, batch 1010, train_loss 0.695\n",
      "epoch 17, batch 1020, train_loss 0.683\n",
      "epoch 17, batch 1030, train_loss 0.700\n",
      "epoch 17, batch 1040, train_loss 0.705\n",
      "epoch 17, batch 1050, train_loss 0.698\n",
      "epoch 17, batch 1060, train_loss 0.698\n",
      "epoch 17, batch 1070, train_loss 0.698\n",
      "epoch 17, batch 1080, train_loss 0.695\n",
      "epoch 17, batch 1090, train_loss 0.702\n",
      "epoch 17, batch 1100, train_loss 0.696\n",
      "epoch 17, batch 1110, train_loss 0.697\n",
      "epoch 17, batch 1120, train_loss 0.705\n",
      "epoch 17, batch 1130, train_loss 0.695\n",
      "epoch 17, batch 1140, train_loss 0.701\n",
      "epoch 17, batch 1150, train_loss 0.686\n",
      "epoch 17, batch 1160, train_loss 0.703\n",
      "epoch 17, batch 1170, train_loss 0.698\n",
      "epoch 17, batch 1180, train_loss 0.694\n",
      "epoch 17, batch 1190, train_loss 0.701\n",
      "epoch 17, train_loss 0.696, valid_loss 0.722, train_accuracy  70.01%, valid_accuracy  68.59%\n",
      "epoch 18, batch 0, train_loss 0.697\n",
      "epoch 18, batch 10, train_loss 0.693\n",
      "epoch 18, batch 20, train_loss 0.691\n",
      "epoch 18, batch 30, train_loss 0.706\n",
      "epoch 18, batch 40, train_loss 0.702\n",
      "epoch 18, batch 50, train_loss 0.699\n",
      "epoch 18, batch 60, train_loss 0.704\n",
      "epoch 18, batch 70, train_loss 0.700\n",
      "epoch 18, batch 80, train_loss 0.700\n",
      "epoch 18, batch 90, train_loss 0.701\n",
      "epoch 18, batch 100, train_loss 0.698\n",
      "epoch 18, batch 110, train_loss 0.699\n",
      "epoch 18, batch 120, train_loss 0.701\n",
      "epoch 18, batch 130, train_loss 0.700\n",
      "epoch 18, batch 140, train_loss 0.699\n",
      "epoch 18, batch 150, train_loss 0.694\n",
      "epoch 18, batch 160, train_loss 0.708\n",
      "epoch 18, batch 170, train_loss 0.685\n",
      "epoch 18, batch 180, train_loss 0.700\n",
      "epoch 18, batch 190, train_loss 0.692\n",
      "epoch 18, batch 200, train_loss 0.693\n",
      "epoch 18, batch 210, train_loss 0.701\n",
      "epoch 18, batch 220, train_loss 0.705\n",
      "epoch 18, batch 230, train_loss 0.701\n",
      "epoch 18, batch 240, train_loss 0.697\n",
      "epoch 18, batch 250, train_loss 0.699\n",
      "epoch 18, batch 260, train_loss 0.706\n",
      "epoch 18, batch 270, train_loss 0.693\n",
      "epoch 18, batch 280, train_loss 0.693\n",
      "epoch 18, batch 290, train_loss 0.690\n",
      "epoch 18, batch 300, train_loss 0.708\n",
      "epoch 18, batch 310, train_loss 0.697\n",
      "epoch 18, batch 320, train_loss 0.706\n",
      "epoch 18, batch 330, train_loss 0.706\n",
      "epoch 18, batch 340, train_loss 0.709\n",
      "epoch 18, batch 350, train_loss 0.700\n",
      "epoch 18, batch 360, train_loss 0.702\n",
      "epoch 18, batch 370, train_loss 0.698\n",
      "epoch 18, batch 380, train_loss 0.693\n",
      "epoch 18, batch 390, train_loss 0.696\n",
      "epoch 18, batch 400, train_loss 0.694\n",
      "epoch 18, batch 410, train_loss 0.692\n",
      "epoch 18, batch 420, train_loss 0.691\n",
      "epoch 18, batch 430, train_loss 0.693\n",
      "epoch 18, batch 440, train_loss 0.697\n",
      "epoch 18, batch 450, train_loss 0.699\n",
      "epoch 18, batch 460, train_loss 0.696\n",
      "epoch 18, batch 470, train_loss 0.688\n",
      "epoch 18, batch 480, train_loss 0.694\n",
      "epoch 18, batch 490, train_loss 0.696\n",
      "epoch 18, batch 500, train_loss 0.703\n",
      "epoch 18, batch 510, train_loss 0.707\n",
      "epoch 18, batch 520, train_loss 0.704\n",
      "epoch 18, batch 530, train_loss 0.705\n",
      "epoch 18, batch 540, train_loss 0.696\n",
      "epoch 18, batch 550, train_loss 0.696\n",
      "epoch 18, batch 560, train_loss 0.692\n",
      "epoch 18, batch 570, train_loss 0.703\n",
      "epoch 18, batch 580, train_loss 0.699\n",
      "epoch 18, batch 590, train_loss 0.701\n",
      "epoch 18, batch 600, train_loss 0.709\n",
      "epoch 18, batch 610, train_loss 0.690\n",
      "epoch 18, batch 620, train_loss 0.691\n",
      "epoch 18, batch 630, train_loss 0.686\n",
      "epoch 18, batch 640, train_loss 0.697\n",
      "epoch 18, batch 650, train_loss 0.703\n",
      "epoch 18, batch 660, train_loss 0.693\n",
      "epoch 18, batch 670, train_loss 0.694\n",
      "epoch 18, batch 680, train_loss 0.704\n",
      "epoch 18, batch 690, train_loss 0.697\n",
      "epoch 18, batch 700, train_loss 0.691\n",
      "epoch 18, batch 710, train_loss 0.703\n",
      "epoch 18, batch 720, train_loss 0.694\n",
      "epoch 18, batch 730, train_loss 0.700\n",
      "epoch 18, batch 740, train_loss 0.694\n",
      "epoch 18, batch 750, train_loss 0.709\n",
      "epoch 18, batch 760, train_loss 0.698\n",
      "epoch 18, batch 770, train_loss 0.705\n",
      "epoch 18, batch 780, train_loss 0.689\n",
      "epoch 18, batch 790, train_loss 0.689\n",
      "epoch 18, batch 800, train_loss 0.687\n",
      "epoch 18, batch 810, train_loss 0.696\n",
      "epoch 18, batch 820, train_loss 0.704\n",
      "epoch 18, batch 830, train_loss 0.698\n",
      "epoch 18, batch 840, train_loss 0.701\n",
      "epoch 18, batch 850, train_loss 0.703\n",
      "epoch 18, batch 860, train_loss 0.693\n",
      "epoch 18, batch 870, train_loss 0.686\n",
      "epoch 18, batch 880, train_loss 0.697\n",
      "epoch 18, batch 890, train_loss 0.695\n",
      "epoch 18, batch 900, train_loss 0.695\n",
      "epoch 18, batch 910, train_loss 0.693\n",
      "epoch 18, batch 920, train_loss 0.699\n",
      "epoch 18, batch 930, train_loss 0.699\n",
      "epoch 18, batch 940, train_loss 0.700\n",
      "epoch 18, batch 950, train_loss 0.699\n",
      "epoch 18, batch 960, train_loss 0.693\n",
      "epoch 18, batch 970, train_loss 0.697\n",
      "epoch 18, batch 980, train_loss 0.688\n",
      "epoch 18, batch 990, train_loss 0.697\n",
      "epoch 18, batch 1000, train_loss 0.698\n",
      "epoch 18, batch 1010, train_loss 0.698\n",
      "epoch 18, batch 1020, train_loss 0.690\n",
      "epoch 18, batch 1030, train_loss 0.689\n",
      "epoch 18, batch 1040, train_loss 0.706\n",
      "epoch 18, batch 1050, train_loss 0.697\n",
      "epoch 18, batch 1060, train_loss 0.685\n",
      "epoch 18, batch 1070, train_loss 0.703\n",
      "epoch 18, batch 1080, train_loss 0.699\n",
      "epoch 18, batch 1090, train_loss 0.698\n",
      "epoch 18, batch 1100, train_loss 0.697\n",
      "epoch 18, batch 1110, train_loss 0.696\n",
      "epoch 18, batch 1120, train_loss 0.691\n",
      "epoch 18, batch 1130, train_loss 0.703\n",
      "epoch 18, batch 1140, train_loss 0.694\n",
      "epoch 18, batch 1150, train_loss 0.692\n",
      "epoch 18, batch 1160, train_loss 0.701\n",
      "epoch 18, batch 1170, train_loss 0.700\n",
      "epoch 18, batch 1180, train_loss 0.692\n",
      "epoch 18, batch 1190, train_loss 0.690\n",
      "epoch 18, train_loss 0.696, valid_loss 0.722, train_accuracy  70.04%, valid_accuracy  68.56%\n",
      "epoch 19, batch 0, train_loss 0.702\n",
      "epoch 19, batch 10, train_loss 0.696\n",
      "epoch 19, batch 20, train_loss 0.691\n",
      "epoch 19, batch 30, train_loss 0.691\n",
      "epoch 19, batch 40, train_loss 0.689\n",
      "epoch 19, batch 50, train_loss 0.690\n",
      "epoch 19, batch 60, train_loss 0.690\n",
      "epoch 19, batch 70, train_loss 0.694\n",
      "epoch 19, batch 80, train_loss 0.707\n",
      "epoch 19, batch 90, train_loss 0.705\n",
      "epoch 19, batch 100, train_loss 0.699\n",
      "epoch 19, batch 110, train_loss 0.692\n",
      "epoch 19, batch 120, train_loss 0.695\n",
      "epoch 19, batch 130, train_loss 0.692\n",
      "epoch 19, batch 140, train_loss 0.701\n",
      "epoch 19, batch 150, train_loss 0.686\n",
      "epoch 19, batch 160, train_loss 0.695\n",
      "epoch 19, batch 170, train_loss 0.689\n",
      "epoch 19, batch 180, train_loss 0.702\n",
      "epoch 19, batch 190, train_loss 0.698\n",
      "epoch 19, batch 200, train_loss 0.694\n",
      "epoch 19, batch 210, train_loss 0.697\n",
      "epoch 19, batch 220, train_loss 0.704\n",
      "epoch 19, batch 230, train_loss 0.696\n",
      "epoch 19, batch 240, train_loss 0.684\n",
      "epoch 19, batch 250, train_loss 0.692\n",
      "epoch 19, batch 260, train_loss 0.693\n",
      "epoch 19, batch 270, train_loss 0.708\n",
      "epoch 19, batch 280, train_loss 0.702\n",
      "epoch 19, batch 290, train_loss 0.697\n",
      "epoch 19, batch 300, train_loss 0.700\n",
      "epoch 19, batch 310, train_loss 0.688\n",
      "epoch 19, batch 320, train_loss 0.698\n",
      "epoch 19, batch 330, train_loss 0.700\n",
      "epoch 19, batch 340, train_loss 0.695\n",
      "epoch 19, batch 350, train_loss 0.697\n",
      "epoch 19, batch 360, train_loss 0.702\n",
      "epoch 19, batch 370, train_loss 0.705\n",
      "epoch 19, batch 380, train_loss 0.702\n",
      "epoch 19, batch 390, train_loss 0.702\n",
      "epoch 19, batch 400, train_loss 0.697\n",
      "epoch 19, batch 410, train_loss 0.705\n",
      "epoch 19, batch 420, train_loss 0.706\n",
      "epoch 19, batch 430, train_loss 0.695\n",
      "epoch 19, batch 440, train_loss 0.708\n",
      "epoch 19, batch 450, train_loss 0.697\n",
      "epoch 19, batch 460, train_loss 0.698\n",
      "epoch 19, batch 470, train_loss 0.693\n",
      "epoch 19, batch 480, train_loss 0.695\n",
      "epoch 19, batch 490, train_loss 0.690\n",
      "epoch 19, batch 500, train_loss 0.696\n",
      "epoch 19, batch 510, train_loss 0.695\n",
      "epoch 19, batch 520, train_loss 0.701\n",
      "epoch 19, batch 530, train_loss 0.690\n",
      "epoch 19, batch 540, train_loss 0.710\n",
      "epoch 19, batch 550, train_loss 0.696\n",
      "epoch 19, batch 560, train_loss 0.694\n",
      "epoch 19, batch 570, train_loss 0.704\n",
      "epoch 19, batch 580, train_loss 0.699\n",
      "epoch 19, batch 590, train_loss 0.693\n",
      "epoch 19, batch 600, train_loss 0.694\n",
      "epoch 19, batch 610, train_loss 0.698\n",
      "epoch 19, batch 620, train_loss 0.704\n",
      "epoch 19, batch 630, train_loss 0.694\n",
      "epoch 19, batch 640, train_loss 0.697\n",
      "epoch 19, batch 650, train_loss 0.704\n",
      "epoch 19, batch 660, train_loss 0.695\n",
      "epoch 19, batch 670, train_loss 0.692\n",
      "epoch 19, batch 680, train_loss 0.705\n",
      "epoch 19, batch 690, train_loss 0.698\n",
      "epoch 19, batch 700, train_loss 0.699\n",
      "epoch 19, batch 710, train_loss 0.704\n",
      "epoch 19, batch 720, train_loss 0.700\n",
      "epoch 19, batch 730, train_loss 0.693\n",
      "epoch 19, batch 740, train_loss 0.695\n",
      "epoch 19, batch 750, train_loss 0.695\n",
      "epoch 19, batch 760, train_loss 0.696\n",
      "epoch 19, batch 770, train_loss 0.687\n",
      "epoch 19, batch 780, train_loss 0.691\n",
      "epoch 19, batch 790, train_loss 0.697\n",
      "epoch 19, batch 800, train_loss 0.686\n",
      "epoch 19, batch 810, train_loss 0.702\n",
      "epoch 19, batch 820, train_loss 0.692\n",
      "epoch 19, batch 830, train_loss 0.695\n",
      "epoch 19, batch 840, train_loss 0.689\n",
      "epoch 19, batch 850, train_loss 0.704\n",
      "epoch 19, batch 860, train_loss 0.703\n",
      "epoch 19, batch 870, train_loss 0.692\n",
      "epoch 19, batch 880, train_loss 0.709\n",
      "epoch 19, batch 890, train_loss 0.699\n",
      "epoch 19, batch 900, train_loss 0.699\n",
      "epoch 19, batch 910, train_loss 0.692\n",
      "epoch 19, batch 920, train_loss 0.697\n",
      "epoch 19, batch 930, train_loss 0.696\n",
      "epoch 19, batch 940, train_loss 0.702\n",
      "epoch 19, batch 950, train_loss 0.695\n",
      "epoch 19, batch 960, train_loss 0.690\n",
      "epoch 19, batch 970, train_loss 0.703\n",
      "epoch 19, batch 980, train_loss 0.693\n",
      "epoch 19, batch 990, train_loss 0.696\n",
      "epoch 19, batch 1000, train_loss 0.695\n",
      "epoch 19, batch 1010, train_loss 0.681\n",
      "epoch 19, batch 1020, train_loss 0.707\n",
      "epoch 19, batch 1030, train_loss 0.700\n",
      "epoch 19, batch 1040, train_loss 0.694\n",
      "epoch 19, batch 1050, train_loss 0.702\n",
      "epoch 19, batch 1060, train_loss 0.707\n",
      "epoch 19, batch 1070, train_loss 0.695\n",
      "epoch 19, batch 1080, train_loss 0.693\n",
      "epoch 19, batch 1090, train_loss 0.698\n",
      "epoch 19, batch 1100, train_loss 0.690\n",
      "epoch 19, batch 1110, train_loss 0.687\n",
      "epoch 19, batch 1120, train_loss 0.691\n",
      "epoch 19, batch 1130, train_loss 0.693\n",
      "epoch 19, batch 1140, train_loss 0.692\n",
      "epoch 19, batch 1150, train_loss 0.698\n",
      "epoch 19, batch 1160, train_loss 0.701\n",
      "epoch 19, batch 1170, train_loss 0.699\n",
      "epoch 19, batch 1180, train_loss 0.693\n",
      "epoch 19, batch 1190, train_loss 0.702\n",
      "epoch 19, train_loss 0.696, valid_loss 0.722, train_accuracy  70.04%, valid_accuracy  68.59%\n",
      "epoch 20, batch 0, train_loss 0.685\n",
      "epoch 20, batch 10, train_loss 0.699\n",
      "epoch 20, batch 20, train_loss 0.691\n",
      "epoch 20, batch 30, train_loss 0.693\n",
      "epoch 20, batch 40, train_loss 0.701\n",
      "epoch 20, batch 50, train_loss 0.689\n",
      "epoch 20, batch 60, train_loss 0.693\n",
      "epoch 20, batch 70, train_loss 0.699\n",
      "epoch 20, batch 80, train_loss 0.711\n",
      "epoch 20, batch 90, train_loss 0.696\n",
      "epoch 20, batch 100, train_loss 0.697\n",
      "epoch 20, batch 110, train_loss 0.691\n",
      "epoch 20, batch 120, train_loss 0.690\n",
      "epoch 20, batch 130, train_loss 0.696\n",
      "epoch 20, batch 140, train_loss 0.689\n",
      "epoch 20, batch 150, train_loss 0.692\n",
      "epoch 20, batch 160, train_loss 0.691\n",
      "epoch 20, batch 170, train_loss 0.705\n",
      "epoch 20, batch 180, train_loss 0.693\n",
      "epoch 20, batch 190, train_loss 0.696\n",
      "epoch 20, batch 200, train_loss 0.691\n",
      "epoch 20, batch 210, train_loss 0.695\n",
      "epoch 20, batch 220, train_loss 0.698\n",
      "epoch 20, batch 230, train_loss 0.706\n",
      "epoch 20, batch 240, train_loss 0.694\n",
      "epoch 20, batch 250, train_loss 0.694\n",
      "epoch 20, batch 260, train_loss 0.700\n",
      "epoch 20, batch 270, train_loss 0.693\n",
      "epoch 20, batch 280, train_loss 0.700\n",
      "epoch 20, batch 290, train_loss 0.690\n",
      "epoch 20, batch 300, train_loss 0.691\n",
      "epoch 20, batch 310, train_loss 0.701\n",
      "epoch 20, batch 320, train_loss 0.700\n",
      "epoch 20, batch 330, train_loss 0.696\n",
      "epoch 20, batch 340, train_loss 0.699\n",
      "epoch 20, batch 350, train_loss 0.700\n",
      "epoch 20, batch 360, train_loss 0.692\n",
      "epoch 20, batch 370, train_loss 0.692\n",
      "epoch 20, batch 380, train_loss 0.696\n",
      "epoch 20, batch 390, train_loss 0.700\n",
      "epoch 20, batch 400, train_loss 0.700\n",
      "epoch 20, batch 410, train_loss 0.704\n",
      "epoch 20, batch 420, train_loss 0.702\n",
      "epoch 20, batch 430, train_loss 0.700\n",
      "epoch 20, batch 440, train_loss 0.693\n",
      "epoch 20, batch 450, train_loss 0.708\n",
      "epoch 20, batch 460, train_loss 0.703\n",
      "epoch 20, batch 470, train_loss 0.702\n",
      "epoch 20, batch 480, train_loss 0.694\n",
      "epoch 20, batch 490, train_loss 0.704\n",
      "epoch 20, batch 500, train_loss 0.698\n",
      "epoch 20, batch 510, train_loss 0.690\n",
      "epoch 20, batch 520, train_loss 0.705\n",
      "epoch 20, batch 530, train_loss 0.697\n",
      "epoch 20, batch 540, train_loss 0.700\n",
      "epoch 20, batch 550, train_loss 0.685\n",
      "epoch 20, batch 560, train_loss 0.702\n",
      "epoch 20, batch 570, train_loss 0.701\n",
      "epoch 20, batch 580, train_loss 0.697\n",
      "epoch 20, batch 590, train_loss 0.693\n",
      "epoch 20, batch 600, train_loss 0.694\n",
      "epoch 20, batch 610, train_loss 0.705\n",
      "epoch 20, batch 620, train_loss 0.691\n",
      "epoch 20, batch 630, train_loss 0.705\n",
      "epoch 20, batch 640, train_loss 0.700\n",
      "epoch 20, batch 650, train_loss 0.689\n",
      "epoch 20, batch 660, train_loss 0.697\n",
      "epoch 20, batch 670, train_loss 0.694\n",
      "epoch 20, batch 680, train_loss 0.697\n",
      "epoch 20, batch 690, train_loss 0.691\n",
      "epoch 20, batch 700, train_loss 0.705\n",
      "epoch 20, batch 710, train_loss 0.688\n",
      "epoch 20, batch 720, train_loss 0.687\n",
      "epoch 20, batch 730, train_loss 0.697\n",
      "epoch 20, batch 740, train_loss 0.698\n",
      "epoch 20, batch 750, train_loss 0.693\n",
      "epoch 20, batch 760, train_loss 0.707\n",
      "epoch 20, batch 770, train_loss 0.690\n",
      "epoch 20, batch 780, train_loss 0.701\n",
      "epoch 20, batch 790, train_loss 0.697\n",
      "epoch 20, batch 800, train_loss 0.685\n",
      "epoch 20, batch 810, train_loss 0.698\n",
      "epoch 20, batch 820, train_loss 0.702\n",
      "epoch 20, batch 830, train_loss 0.697\n",
      "epoch 20, batch 840, train_loss 0.701\n",
      "epoch 20, batch 850, train_loss 0.697\n",
      "epoch 20, batch 860, train_loss 0.691\n",
      "epoch 20, batch 870, train_loss 0.684\n",
      "epoch 20, batch 880, train_loss 0.702\n",
      "epoch 20, batch 890, train_loss 0.702\n",
      "epoch 20, batch 900, train_loss 0.701\n",
      "epoch 20, batch 910, train_loss 0.704\n",
      "epoch 20, batch 920, train_loss 0.695\n",
      "epoch 20, batch 930, train_loss 0.693\n",
      "epoch 20, batch 940, train_loss 0.692\n",
      "epoch 20, batch 950, train_loss 0.696\n",
      "epoch 20, batch 960, train_loss 0.700\n",
      "epoch 20, batch 970, train_loss 0.703\n",
      "epoch 20, batch 980, train_loss 0.695\n",
      "epoch 20, batch 990, train_loss 0.691\n",
      "epoch 20, batch 1000, train_loss 0.701\n",
      "epoch 20, batch 1010, train_loss 0.694\n",
      "epoch 20, batch 1020, train_loss 0.699\n",
      "epoch 20, batch 1030, train_loss 0.698\n",
      "epoch 20, batch 1040, train_loss 0.688\n",
      "epoch 20, batch 1050, train_loss 0.688\n",
      "epoch 20, batch 1060, train_loss 0.700\n",
      "epoch 20, batch 1070, train_loss 0.694\n",
      "epoch 20, batch 1080, train_loss 0.700\n",
      "epoch 20, batch 1090, train_loss 0.693\n",
      "epoch 20, batch 1100, train_loss 0.696\n",
      "epoch 20, batch 1110, train_loss 0.701\n",
      "epoch 20, batch 1120, train_loss 0.695\n",
      "epoch 20, batch 1130, train_loss 0.699\n",
      "epoch 20, batch 1140, train_loss 0.696\n",
      "epoch 20, batch 1150, train_loss 0.695\n",
      "epoch 20, batch 1160, train_loss 0.700\n",
      "epoch 20, batch 1170, train_loss 0.693\n",
      "epoch 20, batch 1180, train_loss 0.693\n",
      "epoch 20, batch 1190, train_loss 0.709\n",
      "epoch 20, train_loss 0.695, valid_loss 0.723, train_accuracy  70.08%, valid_accuracy  68.56%\n",
      "epoch 21, batch 0, train_loss 0.701\n",
      "epoch 21, batch 10, train_loss 0.695\n",
      "epoch 21, batch 20, train_loss 0.690\n",
      "epoch 21, batch 30, train_loss 0.697\n",
      "epoch 21, batch 40, train_loss 0.697\n",
      "epoch 21, batch 50, train_loss 0.691\n",
      "epoch 21, batch 60, train_loss 0.700\n",
      "epoch 21, batch 70, train_loss 0.691\n",
      "epoch 21, batch 80, train_loss 0.698\n",
      "epoch 21, batch 90, train_loss 0.690\n",
      "epoch 21, batch 100, train_loss 0.699\n",
      "epoch 21, batch 110, train_loss 0.695\n",
      "epoch 21, batch 120, train_loss 0.698\n",
      "epoch 21, batch 130, train_loss 0.687\n",
      "epoch 21, batch 140, train_loss 0.699\n",
      "epoch 21, batch 150, train_loss 0.696\n",
      "epoch 21, batch 160, train_loss 0.700\n",
      "epoch 21, batch 170, train_loss 0.695\n",
      "epoch 21, batch 180, train_loss 0.694\n",
      "epoch 21, batch 190, train_loss 0.708\n",
      "epoch 21, batch 200, train_loss 0.703\n",
      "epoch 21, batch 210, train_loss 0.700\n",
      "epoch 21, batch 220, train_loss 0.702\n",
      "epoch 21, batch 230, train_loss 0.707\n",
      "epoch 21, batch 240, train_loss 0.692\n",
      "epoch 21, batch 250, train_loss 0.700\n",
      "epoch 21, batch 260, train_loss 0.705\n",
      "epoch 21, batch 270, train_loss 0.701\n",
      "epoch 21, batch 280, train_loss 0.703\n",
      "epoch 21, batch 290, train_loss 0.694\n",
      "epoch 21, batch 300, train_loss 0.703\n",
      "epoch 21, batch 310, train_loss 0.700\n",
      "epoch 21, batch 320, train_loss 0.702\n",
      "epoch 21, batch 330, train_loss 0.697\n",
      "epoch 21, batch 340, train_loss 0.696\n",
      "epoch 21, batch 350, train_loss 0.695\n",
      "epoch 21, batch 360, train_loss 0.699\n",
      "epoch 21, batch 370, train_loss 0.692\n",
      "epoch 21, batch 380, train_loss 0.691\n",
      "epoch 21, batch 390, train_loss 0.705\n",
      "epoch 21, batch 400, train_loss 0.700\n",
      "epoch 21, batch 410, train_loss 0.690\n",
      "epoch 21, batch 420, train_loss 0.691\n",
      "epoch 21, batch 430, train_loss 0.694\n",
      "epoch 21, batch 440, train_loss 0.704\n",
      "epoch 21, batch 450, train_loss 0.693\n",
      "epoch 21, batch 460, train_loss 0.696\n",
      "epoch 21, batch 470, train_loss 0.702\n",
      "epoch 21, batch 480, train_loss 0.690\n",
      "epoch 21, batch 490, train_loss 0.684\n",
      "epoch 21, batch 500, train_loss 0.694\n",
      "epoch 21, batch 510, train_loss 0.693\n",
      "epoch 21, batch 520, train_loss 0.690\n",
      "epoch 21, batch 530, train_loss 0.701\n",
      "epoch 21, batch 540, train_loss 0.706\n",
      "epoch 21, batch 550, train_loss 0.699\n",
      "epoch 21, batch 560, train_loss 0.697\n",
      "epoch 21, batch 570, train_loss 0.695\n",
      "epoch 21, batch 580, train_loss 0.698\n",
      "epoch 21, batch 590, train_loss 0.700\n",
      "epoch 21, batch 600, train_loss 0.693\n",
      "epoch 21, batch 610, train_loss 0.700\n",
      "epoch 21, batch 620, train_loss 0.693\n",
      "epoch 21, batch 630, train_loss 0.699\n",
      "epoch 21, batch 640, train_loss 0.704\n",
      "epoch 21, batch 650, train_loss 0.701\n",
      "epoch 21, batch 660, train_loss 0.694\n",
      "epoch 21, batch 670, train_loss 0.689\n",
      "epoch 21, batch 680, train_loss 0.697\n",
      "epoch 21, batch 690, train_loss 0.701\n",
      "epoch 21, batch 700, train_loss 0.688\n",
      "epoch 21, batch 710, train_loss 0.707\n",
      "epoch 21, batch 720, train_loss 0.696\n",
      "epoch 21, batch 730, train_loss 0.698\n",
      "epoch 21, batch 740, train_loss 0.689\n",
      "epoch 21, batch 750, train_loss 0.696\n",
      "epoch 21, batch 760, train_loss 0.695\n",
      "epoch 21, batch 770, train_loss 0.694\n",
      "epoch 21, batch 780, train_loss 0.697\n",
      "epoch 21, batch 790, train_loss 0.697\n",
      "epoch 21, batch 800, train_loss 0.697\n",
      "epoch 21, batch 810, train_loss 0.693\n",
      "epoch 21, batch 820, train_loss 0.691\n",
      "epoch 21, batch 830, train_loss 0.703\n",
      "epoch 21, batch 840, train_loss 0.689\n",
      "epoch 21, batch 850, train_loss 0.685\n",
      "epoch 21, batch 860, train_loss 0.704\n",
      "epoch 21, batch 870, train_loss 0.696\n",
      "epoch 21, batch 880, train_loss 0.699\n",
      "epoch 21, batch 890, train_loss 0.695\n",
      "epoch 21, batch 900, train_loss 0.694\n",
      "epoch 21, batch 910, train_loss 0.699\n",
      "epoch 21, batch 920, train_loss 0.696\n",
      "epoch 21, batch 930, train_loss 0.695\n",
      "epoch 21, batch 940, train_loss 0.698\n",
      "epoch 21, batch 950, train_loss 0.694\n",
      "epoch 21, batch 960, train_loss 0.698\n",
      "epoch 21, batch 970, train_loss 0.702\n",
      "epoch 21, batch 980, train_loss 0.697\n",
      "epoch 21, batch 990, train_loss 0.700\n",
      "epoch 21, batch 1000, train_loss 0.699\n",
      "epoch 21, batch 1010, train_loss 0.691\n",
      "epoch 21, batch 1020, train_loss 0.686\n",
      "epoch 21, batch 1030, train_loss 0.697\n",
      "epoch 21, batch 1040, train_loss 0.706\n",
      "epoch 21, batch 1050, train_loss 0.691\n",
      "epoch 21, batch 1060, train_loss 0.682\n",
      "epoch 21, batch 1070, train_loss 0.688\n",
      "epoch 21, batch 1080, train_loss 0.701\n",
      "epoch 21, batch 1090, train_loss 0.693\n",
      "epoch 21, batch 1100, train_loss 0.694\n",
      "epoch 21, batch 1110, train_loss 0.694\n",
      "epoch 21, batch 1120, train_loss 0.697\n",
      "epoch 21, batch 1130, train_loss 0.692\n",
      "epoch 21, batch 1140, train_loss 0.695\n",
      "epoch 21, batch 1150, train_loss 0.697\n",
      "epoch 21, batch 1160, train_loss 0.702\n",
      "epoch 21, batch 1170, train_loss 0.704\n",
      "epoch 21, batch 1180, train_loss 0.695\n",
      "epoch 21, batch 1190, train_loss 0.697\n",
      "epoch 21, train_loss 0.695, valid_loss 0.722, train_accuracy  70.10%, valid_accuracy  68.60%\n",
      "epoch 22, batch 0, train_loss 0.696\n",
      "epoch 22, batch 10, train_loss 0.695\n",
      "epoch 22, batch 20, train_loss 0.696\n",
      "epoch 22, batch 30, train_loss 0.696\n",
      "epoch 22, batch 40, train_loss 0.695\n",
      "epoch 22, batch 50, train_loss 0.697\n",
      "epoch 22, batch 60, train_loss 0.700\n",
      "epoch 22, batch 70, train_loss 0.697\n",
      "epoch 22, batch 80, train_loss 0.696\n",
      "epoch 22, batch 90, train_loss 0.701\n",
      "epoch 22, batch 100, train_loss 0.699\n",
      "epoch 22, batch 110, train_loss 0.696\n",
      "epoch 22, batch 120, train_loss 0.692\n",
      "epoch 22, batch 130, train_loss 0.691\n",
      "epoch 22, batch 140, train_loss 0.695\n",
      "epoch 22, batch 150, train_loss 0.703\n",
      "epoch 22, batch 160, train_loss 0.696\n",
      "epoch 22, batch 170, train_loss 0.696\n",
      "epoch 22, batch 180, train_loss 0.700\n",
      "epoch 22, batch 190, train_loss 0.696\n",
      "epoch 22, batch 200, train_loss 0.704\n",
      "epoch 22, batch 210, train_loss 0.703\n",
      "epoch 22, batch 220, train_loss 0.693\n",
      "epoch 22, batch 230, train_loss 0.695\n",
      "epoch 22, batch 240, train_loss 0.694\n",
      "epoch 22, batch 250, train_loss 0.701\n",
      "epoch 22, batch 260, train_loss 0.689\n",
      "epoch 22, batch 270, train_loss 0.694\n",
      "epoch 22, batch 280, train_loss 0.693\n",
      "epoch 22, batch 290, train_loss 0.700\n",
      "epoch 22, batch 300, train_loss 0.692\n",
      "epoch 22, batch 310, train_loss 0.698\n",
      "epoch 22, batch 320, train_loss 0.701\n",
      "epoch 22, batch 330, train_loss 0.691\n",
      "epoch 22, batch 340, train_loss 0.693\n",
      "epoch 22, batch 350, train_loss 0.701\n",
      "epoch 22, batch 360, train_loss 0.688\n",
      "epoch 22, batch 370, train_loss 0.705\n",
      "epoch 22, batch 380, train_loss 0.700\n",
      "epoch 22, batch 390, train_loss 0.700\n",
      "epoch 22, batch 400, train_loss 0.710\n",
      "epoch 22, batch 410, train_loss 0.701\n",
      "epoch 22, batch 420, train_loss 0.687\n",
      "epoch 22, batch 430, train_loss 0.697\n",
      "epoch 22, batch 440, train_loss 0.689\n",
      "epoch 22, batch 450, train_loss 0.700\n",
      "epoch 22, batch 460, train_loss 0.685\n",
      "epoch 22, batch 470, train_loss 0.702\n",
      "epoch 22, batch 480, train_loss 0.698\n",
      "epoch 22, batch 490, train_loss 0.702\n",
      "epoch 22, batch 500, train_loss 0.699\n",
      "epoch 22, batch 510, train_loss 0.686\n",
      "epoch 22, batch 520, train_loss 0.693\n",
      "epoch 22, batch 530, train_loss 0.705\n",
      "epoch 22, batch 540, train_loss 0.696\n",
      "epoch 22, batch 550, train_loss 0.694\n",
      "epoch 22, batch 560, train_loss 0.688\n",
      "epoch 22, batch 570, train_loss 0.689\n",
      "epoch 22, batch 580, train_loss 0.696\n",
      "epoch 22, batch 590, train_loss 0.697\n",
      "epoch 22, batch 600, train_loss 0.695\n",
      "epoch 22, batch 610, train_loss 0.698\n",
      "epoch 22, batch 620, train_loss 0.699\n",
      "epoch 22, batch 630, train_loss 0.693\n",
      "epoch 22, batch 640, train_loss 0.684\n",
      "epoch 22, batch 650, train_loss 0.696\n",
      "epoch 22, batch 660, train_loss 0.696\n",
      "epoch 22, batch 670, train_loss 0.694\n",
      "epoch 22, batch 680, train_loss 0.699\n",
      "epoch 22, batch 690, train_loss 0.682\n",
      "epoch 22, batch 700, train_loss 0.695\n",
      "epoch 22, batch 710, train_loss 0.687\n",
      "epoch 22, batch 720, train_loss 0.695\n",
      "epoch 22, batch 730, train_loss 0.694\n",
      "epoch 22, batch 740, train_loss 0.691\n",
      "epoch 22, batch 750, train_loss 0.687\n",
      "epoch 22, batch 760, train_loss 0.698\n",
      "epoch 22, batch 770, train_loss 0.701\n",
      "epoch 22, batch 780, train_loss 0.701\n",
      "epoch 22, batch 790, train_loss 0.692\n",
      "epoch 22, batch 800, train_loss 0.704\n",
      "epoch 22, batch 810, train_loss 0.699\n",
      "epoch 22, batch 820, train_loss 0.690\n",
      "epoch 22, batch 830, train_loss 0.686\n",
      "epoch 22, batch 840, train_loss 0.694\n",
      "epoch 22, batch 850, train_loss 0.687\n",
      "epoch 22, batch 860, train_loss 0.699\n",
      "epoch 22, batch 870, train_loss 0.690\n",
      "epoch 22, batch 880, train_loss 0.697\n",
      "epoch 22, batch 890, train_loss 0.694\n",
      "epoch 22, batch 900, train_loss 0.706\n",
      "epoch 22, batch 910, train_loss 0.699\n",
      "epoch 22, batch 920, train_loss 0.696\n",
      "epoch 22, batch 930, train_loss 0.698\n",
      "epoch 22, batch 940, train_loss 0.696\n",
      "epoch 22, batch 950, train_loss 0.678\n",
      "epoch 22, batch 960, train_loss 0.693\n",
      "epoch 22, batch 970, train_loss 0.683\n",
      "epoch 22, batch 980, train_loss 0.701\n",
      "epoch 22, batch 990, train_loss 0.698\n",
      "epoch 22, batch 1000, train_loss 0.693\n",
      "epoch 22, batch 1010, train_loss 0.697\n",
      "epoch 22, batch 1020, train_loss 0.699\n",
      "epoch 22, batch 1030, train_loss 0.699\n",
      "epoch 22, batch 1040, train_loss 0.693\n",
      "epoch 22, batch 1050, train_loss 0.695\n",
      "epoch 22, batch 1060, train_loss 0.695\n",
      "epoch 22, batch 1070, train_loss 0.693\n",
      "epoch 22, batch 1080, train_loss 0.701\n",
      "epoch 22, batch 1090, train_loss 0.690\n",
      "epoch 22, batch 1100, train_loss 0.697\n",
      "epoch 22, batch 1110, train_loss 0.702\n",
      "epoch 22, batch 1120, train_loss 0.695\n",
      "epoch 22, batch 1130, train_loss 0.689\n",
      "epoch 22, batch 1140, train_loss 0.696\n",
      "epoch 22, batch 1150, train_loss 0.694\n",
      "epoch 22, batch 1160, train_loss 0.705\n",
      "epoch 22, batch 1170, train_loss 0.695\n",
      "epoch 22, batch 1180, train_loss 0.692\n",
      "epoch 22, batch 1190, train_loss 0.694\n",
      "epoch 22, train_loss 0.695, valid_loss 0.723, train_accuracy  70.09%, valid_accuracy  68.56%\n",
      "epoch 23, batch 0, train_loss 0.694\n",
      "epoch 23, batch 10, train_loss 0.697\n",
      "epoch 23, batch 20, train_loss 0.689\n",
      "epoch 23, batch 30, train_loss 0.691\n",
      "epoch 23, batch 40, train_loss 0.698\n",
      "epoch 23, batch 50, train_loss 0.699\n",
      "epoch 23, batch 60, train_loss 0.694\n",
      "epoch 23, batch 70, train_loss 0.686\n",
      "epoch 23, batch 80, train_loss 0.694\n",
      "epoch 23, batch 90, train_loss 0.692\n",
      "epoch 23, batch 100, train_loss 0.692\n",
      "epoch 23, batch 110, train_loss 0.704\n",
      "epoch 23, batch 120, train_loss 0.701\n",
      "epoch 23, batch 130, train_loss 0.693\n",
      "epoch 23, batch 140, train_loss 0.691\n",
      "epoch 23, batch 150, train_loss 0.691\n",
      "epoch 23, batch 160, train_loss 0.691\n",
      "epoch 23, batch 170, train_loss 0.697\n",
      "epoch 23, batch 180, train_loss 0.694\n",
      "epoch 23, batch 190, train_loss 0.696\n",
      "epoch 23, batch 200, train_loss 0.694\n",
      "epoch 23, batch 210, train_loss 0.697\n",
      "epoch 23, batch 220, train_loss 0.693\n",
      "epoch 23, batch 230, train_loss 0.694\n",
      "epoch 23, batch 240, train_loss 0.698\n",
      "epoch 23, batch 250, train_loss 0.692\n",
      "epoch 23, batch 260, train_loss 0.692\n",
      "epoch 23, batch 270, train_loss 0.698\n",
      "epoch 23, batch 280, train_loss 0.687\n",
      "epoch 23, batch 290, train_loss 0.694\n",
      "epoch 23, batch 300, train_loss 0.702\n",
      "epoch 23, batch 310, train_loss 0.691\n",
      "epoch 23, batch 320, train_loss 0.702\n",
      "epoch 23, batch 330, train_loss 0.688\n",
      "epoch 23, batch 340, train_loss 0.692\n",
      "epoch 23, batch 350, train_loss 0.698\n",
      "epoch 23, batch 360, train_loss 0.694\n",
      "epoch 23, batch 370, train_loss 0.690\n",
      "epoch 23, batch 380, train_loss 0.696\n",
      "epoch 23, batch 390, train_loss 0.698\n",
      "epoch 23, batch 400, train_loss 0.693\n",
      "epoch 23, batch 410, train_loss 0.687\n",
      "epoch 23, batch 420, train_loss 0.697\n",
      "epoch 23, batch 430, train_loss 0.700\n",
      "epoch 23, batch 440, train_loss 0.697\n",
      "epoch 23, batch 450, train_loss 0.701\n",
      "epoch 23, batch 460, train_loss 0.696\n",
      "epoch 23, batch 470, train_loss 0.691\n",
      "epoch 23, batch 480, train_loss 0.694\n",
      "epoch 23, batch 490, train_loss 0.700\n",
      "epoch 23, batch 500, train_loss 0.702\n",
      "epoch 23, batch 510, train_loss 0.701\n",
      "epoch 23, batch 520, train_loss 0.696\n",
      "epoch 23, batch 530, train_loss 0.705\n",
      "epoch 23, batch 540, train_loss 0.685\n",
      "epoch 23, batch 550, train_loss 0.701\n",
      "epoch 23, batch 560, train_loss 0.693\n",
      "epoch 23, batch 570, train_loss 0.707\n",
      "epoch 23, batch 580, train_loss 0.695\n",
      "epoch 23, batch 590, train_loss 0.694\n",
      "epoch 23, batch 600, train_loss 0.697\n",
      "epoch 23, batch 610, train_loss 0.696\n",
      "epoch 23, batch 620, train_loss 0.694\n",
      "epoch 23, batch 630, train_loss 0.696\n",
      "epoch 23, batch 640, train_loss 0.701\n",
      "epoch 23, batch 650, train_loss 0.690\n",
      "epoch 23, batch 660, train_loss 0.690\n",
      "epoch 23, batch 670, train_loss 0.696\n",
      "epoch 23, batch 680, train_loss 0.699\n",
      "epoch 23, batch 690, train_loss 0.702\n",
      "epoch 23, batch 700, train_loss 0.700\n",
      "epoch 23, batch 710, train_loss 0.697\n",
      "epoch 23, batch 720, train_loss 0.693\n",
      "epoch 23, batch 730, train_loss 0.683\n",
      "epoch 23, batch 740, train_loss 0.697\n",
      "epoch 23, batch 750, train_loss 0.686\n",
      "epoch 23, batch 760, train_loss 0.696\n",
      "epoch 23, batch 770, train_loss 0.694\n",
      "epoch 23, batch 780, train_loss 0.689\n",
      "epoch 23, batch 790, train_loss 0.699\n",
      "epoch 23, batch 800, train_loss 0.695\n",
      "epoch 23, batch 810, train_loss 0.700\n",
      "epoch 23, batch 820, train_loss 0.702\n",
      "epoch 23, batch 830, train_loss 0.690\n",
      "epoch 23, batch 840, train_loss 0.702\n",
      "epoch 23, batch 850, train_loss 0.697\n",
      "epoch 23, batch 860, train_loss 0.687\n",
      "epoch 23, batch 870, train_loss 0.696\n",
      "epoch 23, batch 880, train_loss 0.693\n",
      "epoch 23, batch 890, train_loss 0.690\n",
      "epoch 23, batch 900, train_loss 0.697\n",
      "epoch 23, batch 910, train_loss 0.700\n",
      "epoch 23, batch 920, train_loss 0.706\n",
      "epoch 23, batch 930, train_loss 0.700\n",
      "epoch 23, batch 940, train_loss 0.696\n",
      "epoch 23, batch 950, train_loss 0.696\n",
      "epoch 23, batch 960, train_loss 0.701\n",
      "epoch 23, batch 970, train_loss 0.696\n",
      "epoch 23, batch 980, train_loss 0.703\n",
      "epoch 23, batch 990, train_loss 0.687\n",
      "epoch 23, batch 1000, train_loss 0.697\n",
      "epoch 23, batch 1010, train_loss 0.683\n",
      "epoch 23, batch 1020, train_loss 0.702\n",
      "epoch 23, batch 1030, train_loss 0.694\n",
      "epoch 23, batch 1040, train_loss 0.686\n",
      "epoch 23, batch 1050, train_loss 0.691\n",
      "epoch 23, batch 1060, train_loss 0.698\n",
      "epoch 23, batch 1070, train_loss 0.697\n",
      "epoch 23, batch 1080, train_loss 0.695\n",
      "epoch 23, batch 1090, train_loss 0.693\n",
      "epoch 23, batch 1100, train_loss 0.690\n",
      "epoch 23, batch 1110, train_loss 0.698\n",
      "epoch 23, batch 1120, train_loss 0.700\n",
      "epoch 23, batch 1130, train_loss 0.695\n",
      "epoch 23, batch 1140, train_loss 0.681\n",
      "epoch 23, batch 1150, train_loss 0.692\n",
      "epoch 23, batch 1160, train_loss 0.697\n",
      "epoch 23, batch 1170, train_loss 0.696\n",
      "epoch 23, batch 1180, train_loss 0.695\n",
      "epoch 23, batch 1190, train_loss 0.707\n",
      "epoch 23, train_loss 0.695, valid_loss 0.723, train_accuracy  70.08%, valid_accuracy  68.53%\n",
      "epoch 24, batch 0, train_loss 0.697\n",
      "epoch 24, batch 10, train_loss 0.703\n",
      "epoch 24, batch 20, train_loss 0.689\n",
      "epoch 24, batch 30, train_loss 0.694\n",
      "epoch 24, batch 40, train_loss 0.686\n",
      "epoch 24, batch 50, train_loss 0.692\n",
      "epoch 24, batch 60, train_loss 0.699\n",
      "epoch 24, batch 70, train_loss 0.690\n",
      "epoch 24, batch 80, train_loss 0.695\n",
      "epoch 24, batch 90, train_loss 0.694\n",
      "epoch 24, batch 100, train_loss 0.702\n",
      "epoch 24, batch 110, train_loss 0.693\n",
      "epoch 24, batch 120, train_loss 0.692\n",
      "epoch 24, batch 130, train_loss 0.693\n",
      "epoch 24, batch 140, train_loss 0.690\n",
      "epoch 24, batch 150, train_loss 0.698\n",
      "epoch 24, batch 160, train_loss 0.688\n",
      "epoch 24, batch 170, train_loss 0.687\n",
      "epoch 24, batch 180, train_loss 0.698\n",
      "epoch 24, batch 190, train_loss 0.703\n",
      "epoch 24, batch 200, train_loss 0.692\n",
      "epoch 24, batch 210, train_loss 0.698\n",
      "epoch 24, batch 220, train_loss 0.699\n",
      "epoch 24, batch 230, train_loss 0.684\n",
      "epoch 24, batch 240, train_loss 0.697\n",
      "epoch 24, batch 250, train_loss 0.696\n",
      "epoch 24, batch 260, train_loss 0.698\n",
      "epoch 24, batch 270, train_loss 0.695\n",
      "epoch 24, batch 280, train_loss 0.700\n",
      "epoch 24, batch 290, train_loss 0.697\n",
      "epoch 24, batch 300, train_loss 0.704\n",
      "epoch 24, batch 310, train_loss 0.699\n",
      "epoch 24, batch 320, train_loss 0.692\n",
      "epoch 24, batch 330, train_loss 0.698\n",
      "epoch 24, batch 340, train_loss 0.701\n",
      "epoch 24, batch 350, train_loss 0.696\n",
      "epoch 24, batch 360, train_loss 0.705\n",
      "epoch 24, batch 370, train_loss 0.702\n",
      "epoch 24, batch 380, train_loss 0.695\n",
      "epoch 24, batch 390, train_loss 0.698\n",
      "epoch 24, batch 400, train_loss 0.691\n",
      "epoch 24, batch 410, train_loss 0.695\n",
      "epoch 24, batch 420, train_loss 0.689\n",
      "epoch 24, batch 430, train_loss 0.697\n",
      "epoch 24, batch 440, train_loss 0.696\n",
      "epoch 24, batch 450, train_loss 0.698\n",
      "epoch 24, batch 460, train_loss 0.696\n",
      "epoch 24, batch 470, train_loss 0.700\n",
      "epoch 24, batch 480, train_loss 0.698\n",
      "epoch 24, batch 490, train_loss 0.700\n",
      "epoch 24, batch 500, train_loss 0.690\n",
      "epoch 24, batch 510, train_loss 0.695\n",
      "epoch 24, batch 520, train_loss 0.689\n",
      "epoch 24, batch 530, train_loss 0.693\n",
      "epoch 24, batch 540, train_loss 0.692\n",
      "epoch 24, batch 550, train_loss 0.702\n",
      "epoch 24, batch 560, train_loss 0.701\n",
      "epoch 24, batch 570, train_loss 0.694\n",
      "epoch 24, batch 580, train_loss 0.686\n",
      "epoch 24, batch 590, train_loss 0.682\n",
      "epoch 24, batch 600, train_loss 0.698\n",
      "epoch 24, batch 610, train_loss 0.695\n",
      "epoch 24, batch 620, train_loss 0.694\n",
      "epoch 24, batch 630, train_loss 0.691\n",
      "epoch 24, batch 640, train_loss 0.694\n",
      "epoch 24, batch 650, train_loss 0.701\n",
      "epoch 24, batch 660, train_loss 0.695\n",
      "epoch 24, batch 670, train_loss 0.695\n",
      "epoch 24, batch 680, train_loss 0.697\n",
      "epoch 24, batch 690, train_loss 0.702\n",
      "epoch 24, batch 700, train_loss 0.690\n",
      "epoch 24, batch 710, train_loss 0.693\n",
      "epoch 24, batch 720, train_loss 0.691\n",
      "epoch 24, batch 730, train_loss 0.698\n",
      "epoch 24, batch 740, train_loss 0.701\n",
      "epoch 24, batch 750, train_loss 0.702\n",
      "epoch 24, batch 760, train_loss 0.693\n",
      "epoch 24, batch 770, train_loss 0.694\n",
      "epoch 24, batch 780, train_loss 0.700\n",
      "epoch 24, batch 790, train_loss 0.698\n",
      "epoch 24, batch 800, train_loss 0.695\n",
      "epoch 24, batch 810, train_loss 0.700\n",
      "epoch 24, batch 820, train_loss 0.711\n",
      "epoch 24, batch 830, train_loss 0.691\n",
      "epoch 24, batch 840, train_loss 0.692\n",
      "epoch 24, batch 850, train_loss 0.695\n",
      "epoch 24, batch 860, train_loss 0.704\n",
      "epoch 24, batch 870, train_loss 0.691\n",
      "epoch 24, batch 880, train_loss 0.701\n",
      "epoch 24, batch 890, train_loss 0.697\n",
      "epoch 24, batch 900, train_loss 0.705\n",
      "epoch 24, batch 910, train_loss 0.696\n",
      "epoch 24, batch 920, train_loss 0.695\n",
      "epoch 24, batch 930, train_loss 0.697\n",
      "epoch 24, batch 940, train_loss 0.691\n",
      "epoch 24, batch 950, train_loss 0.696\n",
      "epoch 24, batch 960, train_loss 0.697\n",
      "epoch 24, batch 970, train_loss 0.698\n",
      "epoch 24, batch 980, train_loss 0.684\n",
      "epoch 24, batch 990, train_loss 0.695\n",
      "epoch 24, batch 1000, train_loss 0.701\n",
      "epoch 24, batch 1010, train_loss 0.701\n",
      "epoch 24, batch 1020, train_loss 0.692\n",
      "epoch 24, batch 1030, train_loss 0.693\n",
      "epoch 24, batch 1040, train_loss 0.698\n",
      "epoch 24, batch 1050, train_loss 0.693\n",
      "epoch 24, batch 1060, train_loss 0.703\n",
      "epoch 24, batch 1070, train_loss 0.695\n",
      "epoch 24, batch 1080, train_loss 0.707\n",
      "epoch 24, batch 1090, train_loss 0.690\n",
      "epoch 24, batch 1100, train_loss 0.689\n",
      "epoch 24, batch 1110, train_loss 0.701\n",
      "epoch 24, batch 1120, train_loss 0.682\n",
      "epoch 24, batch 1130, train_loss 0.693\n",
      "epoch 24, batch 1140, train_loss 0.692\n",
      "epoch 24, batch 1150, train_loss 0.687\n",
      "epoch 24, batch 1160, train_loss 0.694\n",
      "epoch 24, batch 1170, train_loss 0.694\n",
      "epoch 24, batch 1180, train_loss 0.698\n",
      "epoch 24, batch 1190, train_loss 0.695\n",
      "epoch 24, train_loss 0.694, valid_loss 0.722, train_accuracy  70.13%, valid_accuracy  68.58%\n",
      "epoch 25, batch 0, train_loss 0.675\n",
      "epoch 25, batch 10, train_loss 0.693\n",
      "epoch 25, batch 20, train_loss 0.701\n",
      "epoch 25, batch 30, train_loss 0.688\n",
      "epoch 25, batch 40, train_loss 0.696\n",
      "epoch 25, batch 50, train_loss 0.694\n",
      "epoch 25, batch 60, train_loss 0.690\n",
      "epoch 25, batch 70, train_loss 0.698\n",
      "epoch 25, batch 80, train_loss 0.705\n",
      "epoch 25, batch 90, train_loss 0.697\n",
      "epoch 25, batch 100, train_loss 0.689\n",
      "epoch 25, batch 110, train_loss 0.691\n",
      "epoch 25, batch 120, train_loss 0.682\n",
      "epoch 25, batch 130, train_loss 0.690\n",
      "epoch 25, batch 140, train_loss 0.695\n",
      "epoch 25, batch 150, train_loss 0.687\n",
      "epoch 25, batch 160, train_loss 0.695\n",
      "epoch 25, batch 170, train_loss 0.694\n",
      "epoch 25, batch 180, train_loss 0.696\n",
      "epoch 25, batch 190, train_loss 0.694\n",
      "epoch 25, batch 200, train_loss 0.688\n",
      "epoch 25, batch 210, train_loss 0.700\n",
      "epoch 25, batch 220, train_loss 0.695\n",
      "epoch 25, batch 230, train_loss 0.693\n",
      "epoch 25, batch 240, train_loss 0.689\n",
      "epoch 25, batch 250, train_loss 0.691\n",
      "epoch 25, batch 260, train_loss 0.699\n",
      "epoch 25, batch 270, train_loss 0.690\n",
      "epoch 25, batch 280, train_loss 0.685\n",
      "epoch 25, batch 290, train_loss 0.692\n",
      "epoch 25, batch 300, train_loss 0.686\n",
      "epoch 25, batch 310, train_loss 0.694\n",
      "epoch 25, batch 320, train_loss 0.700\n",
      "epoch 25, batch 330, train_loss 0.696\n",
      "epoch 25, batch 340, train_loss 0.701\n",
      "epoch 25, batch 350, train_loss 0.693\n",
      "epoch 25, batch 360, train_loss 0.689\n",
      "epoch 25, batch 370, train_loss 0.701\n",
      "epoch 25, batch 380, train_loss 0.694\n",
      "epoch 25, batch 390, train_loss 0.697\n",
      "epoch 25, batch 400, train_loss 0.705\n",
      "epoch 25, batch 410, train_loss 0.692\n",
      "epoch 25, batch 420, train_loss 0.699\n",
      "epoch 25, batch 430, train_loss 0.695\n",
      "epoch 25, batch 440, train_loss 0.695\n",
      "epoch 25, batch 450, train_loss 0.700\n",
      "epoch 25, batch 460, train_loss 0.704\n",
      "epoch 25, batch 470, train_loss 0.701\n",
      "epoch 25, batch 480, train_loss 0.690\n",
      "epoch 25, batch 490, train_loss 0.697\n",
      "epoch 25, batch 500, train_loss 0.695\n",
      "epoch 25, batch 510, train_loss 0.694\n",
      "epoch 25, batch 520, train_loss 0.707\n",
      "epoch 25, batch 530, train_loss 0.701\n",
      "epoch 25, batch 540, train_loss 0.702\n",
      "epoch 25, batch 550, train_loss 0.696\n",
      "epoch 25, batch 560, train_loss 0.693\n",
      "epoch 25, batch 570, train_loss 0.691\n",
      "epoch 25, batch 580, train_loss 0.687\n",
      "epoch 25, batch 590, train_loss 0.698\n",
      "epoch 25, batch 600, train_loss 0.696\n",
      "epoch 25, batch 610, train_loss 0.701\n",
      "epoch 25, batch 620, train_loss 0.695\n",
      "epoch 25, batch 630, train_loss 0.693\n",
      "epoch 25, batch 640, train_loss 0.706\n",
      "epoch 25, batch 650, train_loss 0.700\n",
      "epoch 25, batch 660, train_loss 0.700\n",
      "epoch 25, batch 670, train_loss 0.703\n",
      "epoch 25, batch 680, train_loss 0.703\n",
      "epoch 25, batch 690, train_loss 0.700\n",
      "epoch 25, batch 700, train_loss 0.701\n",
      "epoch 25, batch 710, train_loss 0.706\n",
      "epoch 25, batch 720, train_loss 0.701\n",
      "epoch 25, batch 730, train_loss 0.700\n",
      "epoch 25, batch 740, train_loss 0.688\n",
      "epoch 25, batch 750, train_loss 0.698\n",
      "epoch 25, batch 760, train_loss 0.700\n",
      "epoch 25, batch 770, train_loss 0.689\n",
      "epoch 25, batch 780, train_loss 0.695\n",
      "epoch 25, batch 790, train_loss 0.700\n",
      "epoch 25, batch 800, train_loss 0.697\n",
      "epoch 25, batch 810, train_loss 0.694\n",
      "epoch 25, batch 820, train_loss 0.693\n",
      "epoch 25, batch 830, train_loss 0.692\n",
      "epoch 25, batch 840, train_loss 0.698\n",
      "epoch 25, batch 850, train_loss 0.691\n",
      "epoch 25, batch 860, train_loss 0.687\n",
      "epoch 25, batch 870, train_loss 0.700\n",
      "epoch 25, batch 880, train_loss 0.696\n",
      "epoch 25, batch 890, train_loss 0.696\n",
      "epoch 25, batch 900, train_loss 0.701\n",
      "epoch 25, batch 910, train_loss 0.694\n",
      "epoch 25, batch 920, train_loss 0.704\n",
      "epoch 25, batch 930, train_loss 0.708\n",
      "epoch 25, batch 940, train_loss 0.694\n",
      "epoch 25, batch 950, train_loss 0.698\n",
      "epoch 25, batch 960, train_loss 0.692\n",
      "epoch 25, batch 970, train_loss 0.695\n",
      "epoch 25, batch 980, train_loss 0.690\n",
      "epoch 25, batch 990, train_loss 0.694\n",
      "epoch 25, batch 1000, train_loss 0.690\n",
      "epoch 25, batch 1010, train_loss 0.689\n",
      "epoch 25, batch 1020, train_loss 0.689\n",
      "epoch 25, batch 1030, train_loss 0.688\n",
      "epoch 25, batch 1040, train_loss 0.688\n",
      "epoch 25, batch 1050, train_loss 0.699\n",
      "epoch 25, batch 1060, train_loss 0.697\n",
      "epoch 25, batch 1070, train_loss 0.690\n",
      "epoch 25, batch 1080, train_loss 0.694\n",
      "epoch 25, batch 1090, train_loss 0.691\n",
      "epoch 25, batch 1100, train_loss 0.689\n",
      "epoch 25, batch 1110, train_loss 0.696\n",
      "epoch 25, batch 1120, train_loss 0.690\n",
      "epoch 25, batch 1130, train_loss 0.697\n",
      "epoch 25, batch 1140, train_loss 0.698\n",
      "epoch 25, batch 1150, train_loss 0.701\n",
      "epoch 25, batch 1160, train_loss 0.688\n",
      "epoch 25, batch 1170, train_loss 0.691\n",
      "epoch 25, batch 1180, train_loss 0.701\n",
      "epoch 25, batch 1190, train_loss 0.700\n",
      "epoch 25, train_loss 0.694, valid_loss 0.722, train_accuracy  70.16%, valid_accuracy  68.59%\n",
      "epoch 26, batch 0, train_loss 0.702\n",
      "epoch 26, batch 10, train_loss 0.696\n",
      "epoch 26, batch 20, train_loss 0.691\n",
      "epoch 26, batch 30, train_loss 0.692\n",
      "epoch 26, batch 40, train_loss 0.695\n",
      "epoch 26, batch 50, train_loss 0.694\n",
      "epoch 26, batch 60, train_loss 0.699\n",
      "epoch 26, batch 70, train_loss 0.692\n",
      "epoch 26, batch 80, train_loss 0.702\n",
      "epoch 26, batch 90, train_loss 0.698\n",
      "epoch 26, batch 100, train_loss 0.687\n",
      "epoch 26, batch 110, train_loss 0.693\n",
      "epoch 26, batch 120, train_loss 0.696\n",
      "epoch 26, batch 130, train_loss 0.696\n",
      "epoch 26, batch 140, train_loss 0.706\n",
      "epoch 26, batch 150, train_loss 0.694\n",
      "epoch 26, batch 160, train_loss 0.690\n",
      "epoch 26, batch 170, train_loss 0.696\n",
      "epoch 26, batch 180, train_loss 0.686\n",
      "epoch 26, batch 190, train_loss 0.698\n",
      "epoch 26, batch 200, train_loss 0.689\n",
      "epoch 26, batch 210, train_loss 0.696\n",
      "epoch 26, batch 220, train_loss 0.695\n",
      "epoch 26, batch 230, train_loss 0.691\n",
      "epoch 26, batch 240, train_loss 0.692\n",
      "epoch 26, batch 250, train_loss 0.701\n",
      "epoch 26, batch 260, train_loss 0.701\n",
      "epoch 26, batch 270, train_loss 0.693\n",
      "epoch 26, batch 280, train_loss 0.700\n",
      "epoch 26, batch 290, train_loss 0.694\n",
      "epoch 26, batch 300, train_loss 0.700\n",
      "epoch 26, batch 310, train_loss 0.691\n",
      "epoch 26, batch 320, train_loss 0.699\n",
      "epoch 26, batch 330, train_loss 0.691\n",
      "epoch 26, batch 340, train_loss 0.698\n",
      "epoch 26, batch 350, train_loss 0.696\n",
      "epoch 26, batch 360, train_loss 0.695\n",
      "epoch 26, batch 370, train_loss 0.692\n",
      "epoch 26, batch 380, train_loss 0.692\n",
      "epoch 26, batch 390, train_loss 0.700\n",
      "epoch 26, batch 400, train_loss 0.695\n",
      "epoch 26, batch 410, train_loss 0.708\n",
      "epoch 26, batch 420, train_loss 0.700\n",
      "epoch 26, batch 430, train_loss 0.694\n",
      "epoch 26, batch 440, train_loss 0.696\n",
      "epoch 26, batch 450, train_loss 0.698\n",
      "epoch 26, batch 460, train_loss 0.699\n",
      "epoch 26, batch 470, train_loss 0.703\n",
      "epoch 26, batch 480, train_loss 0.702\n",
      "epoch 26, batch 490, train_loss 0.692\n",
      "epoch 26, batch 500, train_loss 0.693\n",
      "epoch 26, batch 510, train_loss 0.694\n",
      "epoch 26, batch 520, train_loss 0.680\n",
      "epoch 26, batch 530, train_loss 0.688\n",
      "epoch 26, batch 540, train_loss 0.692\n",
      "epoch 26, batch 550, train_loss 0.698\n",
      "epoch 26, batch 560, train_loss 0.696\n",
      "epoch 26, batch 570, train_loss 0.693\n",
      "epoch 26, batch 580, train_loss 0.684\n",
      "epoch 26, batch 590, train_loss 0.702\n",
      "epoch 26, batch 600, train_loss 0.691\n",
      "epoch 26, batch 610, train_loss 0.694\n",
      "epoch 26, batch 620, train_loss 0.692\n",
      "epoch 26, batch 630, train_loss 0.697\n",
      "epoch 26, batch 640, train_loss 0.695\n",
      "epoch 26, batch 650, train_loss 0.689\n",
      "epoch 26, batch 660, train_loss 0.687\n",
      "epoch 26, batch 670, train_loss 0.698\n",
      "epoch 26, batch 680, train_loss 0.698\n",
      "epoch 26, batch 690, train_loss 0.697\n",
      "epoch 26, batch 700, train_loss 0.691\n",
      "epoch 26, batch 710, train_loss 0.697\n",
      "epoch 26, batch 720, train_loss 0.696\n",
      "epoch 26, batch 730, train_loss 0.692\n",
      "epoch 26, batch 740, train_loss 0.692\n",
      "epoch 26, batch 750, train_loss 0.700\n",
      "epoch 26, batch 760, train_loss 0.697\n",
      "epoch 26, batch 770, train_loss 0.694\n",
      "epoch 26, batch 780, train_loss 0.704\n",
      "epoch 26, batch 790, train_loss 0.689\n",
      "epoch 26, batch 800, train_loss 0.691\n",
      "epoch 26, batch 810, train_loss 0.698\n",
      "epoch 26, batch 820, train_loss 0.694\n",
      "epoch 26, batch 830, train_loss 0.689\n",
      "epoch 26, batch 840, train_loss 0.698\n",
      "epoch 26, batch 850, train_loss 0.700\n",
      "epoch 26, batch 860, train_loss 0.696\n",
      "epoch 26, batch 870, train_loss 0.701\n",
      "epoch 26, batch 880, train_loss 0.697\n",
      "epoch 26, batch 890, train_loss 0.692\n",
      "epoch 26, batch 900, train_loss 0.696\n",
      "epoch 26, batch 910, train_loss 0.695\n",
      "epoch 26, batch 920, train_loss 0.690\n",
      "epoch 26, batch 930, train_loss 0.695\n",
      "epoch 26, batch 940, train_loss 0.690\n",
      "epoch 26, batch 950, train_loss 0.697\n",
      "epoch 26, batch 960, train_loss 0.699\n",
      "epoch 26, batch 970, train_loss 0.704\n",
      "epoch 26, batch 980, train_loss 0.689\n",
      "epoch 26, batch 990, train_loss 0.700\n",
      "epoch 26, batch 1000, train_loss 0.710\n",
      "epoch 26, batch 1010, train_loss 0.686\n",
      "epoch 26, batch 1020, train_loss 0.699\n",
      "epoch 26, batch 1030, train_loss 0.689\n",
      "epoch 26, batch 1040, train_loss 0.690\n",
      "epoch 26, batch 1050, train_loss 0.695\n",
      "epoch 26, batch 1060, train_loss 0.689\n",
      "epoch 26, batch 1070, train_loss 0.688\n",
      "epoch 26, batch 1080, train_loss 0.699\n",
      "epoch 26, batch 1090, train_loss 0.698\n",
      "epoch 26, batch 1100, train_loss 0.702\n",
      "epoch 26, batch 1110, train_loss 0.696\n",
      "epoch 26, batch 1120, train_loss 0.689\n",
      "epoch 26, batch 1130, train_loss 0.689\n",
      "epoch 26, batch 1140, train_loss 0.695\n",
      "epoch 26, batch 1150, train_loss 0.697\n",
      "epoch 26, batch 1160, train_loss 0.690\n",
      "epoch 26, batch 1170, train_loss 0.699\n",
      "epoch 26, batch 1180, train_loss 0.691\n",
      "epoch 26, batch 1190, train_loss 0.692\n",
      "epoch 26, train_loss 0.694, valid_loss 0.722, train_accuracy  70.16%, valid_accuracy  68.60%\n",
      "epoch 27, batch 0, train_loss 0.698\n",
      "epoch 27, batch 10, train_loss 0.692\n",
      "epoch 27, batch 20, train_loss 0.698\n",
      "epoch 27, batch 30, train_loss 0.700\n",
      "epoch 27, batch 40, train_loss 0.695\n",
      "epoch 27, batch 50, train_loss 0.689\n",
      "epoch 27, batch 60, train_loss 0.687\n",
      "epoch 27, batch 70, train_loss 0.697\n",
      "epoch 27, batch 80, train_loss 0.699\n",
      "epoch 27, batch 90, train_loss 0.700\n",
      "epoch 27, batch 100, train_loss 0.694\n",
      "epoch 27, batch 110, train_loss 0.686\n",
      "epoch 27, batch 120, train_loss 0.688\n",
      "epoch 27, batch 130, train_loss 0.689\n",
      "epoch 27, batch 140, train_loss 0.689\n",
      "epoch 27, batch 150, train_loss 0.692\n",
      "epoch 27, batch 160, train_loss 0.694\n",
      "epoch 27, batch 170, train_loss 0.695\n",
      "epoch 27, batch 180, train_loss 0.695\n",
      "epoch 27, batch 190, train_loss 0.695\n",
      "epoch 27, batch 200, train_loss 0.693\n",
      "epoch 27, batch 210, train_loss 0.692\n",
      "epoch 27, batch 220, train_loss 0.695\n",
      "epoch 27, batch 230, train_loss 0.685\n",
      "epoch 27, batch 240, train_loss 0.686\n",
      "epoch 27, batch 250, train_loss 0.697\n",
      "epoch 27, batch 260, train_loss 0.683\n",
      "epoch 27, batch 270, train_loss 0.698\n",
      "epoch 27, batch 280, train_loss 0.695\n",
      "epoch 27, batch 290, train_loss 0.690\n",
      "epoch 27, batch 300, train_loss 0.690\n",
      "epoch 27, batch 310, train_loss 0.685\n",
      "epoch 27, batch 320, train_loss 0.696\n",
      "epoch 27, batch 330, train_loss 0.693\n",
      "epoch 27, batch 340, train_loss 0.700\n",
      "epoch 27, batch 350, train_loss 0.690\n",
      "epoch 27, batch 360, train_loss 0.706\n",
      "epoch 27, batch 370, train_loss 0.709\n",
      "epoch 27, batch 380, train_loss 0.691\n",
      "epoch 27, batch 390, train_loss 0.698\n",
      "epoch 27, batch 400, train_loss 0.694\n",
      "epoch 27, batch 410, train_loss 0.692\n",
      "epoch 27, batch 420, train_loss 0.692\n",
      "epoch 27, batch 430, train_loss 0.698\n",
      "epoch 27, batch 440, train_loss 0.696\n",
      "epoch 27, batch 450, train_loss 0.696\n",
      "epoch 27, batch 460, train_loss 0.697\n",
      "epoch 27, batch 470, train_loss 0.697\n",
      "epoch 27, batch 480, train_loss 0.682\n",
      "epoch 27, batch 490, train_loss 0.691\n",
      "epoch 27, batch 500, train_loss 0.693\n",
      "epoch 27, batch 510, train_loss 0.697\n",
      "epoch 27, batch 520, train_loss 0.702\n",
      "epoch 27, batch 530, train_loss 0.683\n",
      "epoch 27, batch 540, train_loss 0.699\n",
      "epoch 27, batch 550, train_loss 0.694\n",
      "epoch 27, batch 560, train_loss 0.694\n",
      "epoch 27, batch 570, train_loss 0.689\n",
      "epoch 27, batch 580, train_loss 0.699\n",
      "epoch 27, batch 590, train_loss 0.687\n",
      "epoch 27, batch 600, train_loss 0.699\n",
      "epoch 27, batch 610, train_loss 0.699\n",
      "epoch 27, batch 620, train_loss 0.701\n",
      "epoch 27, batch 630, train_loss 0.691\n",
      "epoch 27, batch 640, train_loss 0.696\n",
      "epoch 27, batch 650, train_loss 0.703\n",
      "epoch 27, batch 660, train_loss 0.694\n",
      "epoch 27, batch 670, train_loss 0.692\n",
      "epoch 27, batch 680, train_loss 0.692\n",
      "epoch 27, batch 690, train_loss 0.693\n",
      "epoch 27, batch 700, train_loss 0.695\n",
      "epoch 27, batch 710, train_loss 0.696\n",
      "epoch 27, batch 720, train_loss 0.699\n",
      "epoch 27, batch 730, train_loss 0.696\n",
      "epoch 27, batch 740, train_loss 0.687\n",
      "epoch 27, batch 750, train_loss 0.693\n",
      "epoch 27, batch 760, train_loss 0.690\n",
      "epoch 27, batch 770, train_loss 0.692\n",
      "epoch 27, batch 780, train_loss 0.688\n",
      "epoch 27, batch 790, train_loss 0.702\n",
      "epoch 27, batch 800, train_loss 0.688\n",
      "epoch 27, batch 810, train_loss 0.702\n",
      "epoch 27, batch 820, train_loss 0.687\n",
      "epoch 27, batch 830, train_loss 0.686\n",
      "epoch 27, batch 840, train_loss 0.693\n",
      "epoch 27, batch 850, train_loss 0.682\n",
      "epoch 27, batch 860, train_loss 0.708\n",
      "epoch 27, batch 870, train_loss 0.694\n",
      "epoch 27, batch 880, train_loss 0.690\n",
      "epoch 27, batch 890, train_loss 0.693\n",
      "epoch 27, batch 900, train_loss 0.694\n",
      "epoch 27, batch 910, train_loss 0.684\n",
      "epoch 27, batch 920, train_loss 0.696\n",
      "epoch 27, batch 930, train_loss 0.694\n",
      "epoch 27, batch 940, train_loss 0.689\n",
      "epoch 27, batch 950, train_loss 0.696\n",
      "epoch 27, batch 960, train_loss 0.707\n",
      "epoch 27, batch 970, train_loss 0.690\n",
      "epoch 27, batch 980, train_loss 0.707\n",
      "epoch 27, batch 990, train_loss 0.693\n",
      "epoch 27, batch 1000, train_loss 0.695\n",
      "epoch 27, batch 1010, train_loss 0.694\n",
      "epoch 27, batch 1020, train_loss 0.691\n",
      "epoch 27, batch 1030, train_loss 0.704\n",
      "epoch 27, batch 1040, train_loss 0.698\n",
      "epoch 27, batch 1050, train_loss 0.692\n",
      "epoch 27, batch 1060, train_loss 0.694\n",
      "epoch 27, batch 1070, train_loss 0.697\n",
      "epoch 27, batch 1080, train_loss 0.703\n",
      "epoch 27, batch 1090, train_loss 0.702\n",
      "epoch 27, batch 1100, train_loss 0.705\n",
      "epoch 27, batch 1110, train_loss 0.688\n",
      "epoch 27, batch 1120, train_loss 0.702\n",
      "epoch 27, batch 1130, train_loss 0.699\n",
      "epoch 27, batch 1140, train_loss 0.695\n",
      "epoch 27, batch 1150, train_loss 0.690\n",
      "epoch 27, batch 1160, train_loss 0.692\n",
      "epoch 27, batch 1170, train_loss 0.698\n",
      "epoch 27, batch 1180, train_loss 0.699\n",
      "epoch 27, batch 1190, train_loss 0.696\n",
      "epoch 27, train_loss 0.694, valid_loss 0.722, train_accuracy  70.18%, valid_accuracy  68.61%\n",
      "epoch 28, batch 0, train_loss 0.696\n",
      "epoch 28, batch 10, train_loss 0.690\n",
      "epoch 28, batch 20, train_loss 0.701\n",
      "epoch 28, batch 30, train_loss 0.686\n",
      "epoch 28, batch 40, train_loss 0.688\n",
      "epoch 28, batch 50, train_loss 0.695\n",
      "epoch 28, batch 60, train_loss 0.696\n",
      "epoch 28, batch 70, train_loss 0.689\n",
      "epoch 28, batch 80, train_loss 0.699\n",
      "epoch 28, batch 90, train_loss 0.684\n",
      "epoch 28, batch 100, train_loss 0.687\n",
      "epoch 28, batch 110, train_loss 0.687\n",
      "epoch 28, batch 120, train_loss 0.698\n",
      "epoch 28, batch 130, train_loss 0.693\n",
      "epoch 28, batch 140, train_loss 0.695\n",
      "epoch 28, batch 150, train_loss 0.702\n",
      "epoch 28, batch 160, train_loss 0.701\n",
      "epoch 28, batch 170, train_loss 0.694\n",
      "epoch 28, batch 180, train_loss 0.703\n",
      "epoch 28, batch 190, train_loss 0.689\n",
      "epoch 28, batch 200, train_loss 0.686\n",
      "epoch 28, batch 210, train_loss 0.701\n",
      "epoch 28, batch 220, train_loss 0.686\n",
      "epoch 28, batch 230, train_loss 0.699\n",
      "epoch 28, batch 240, train_loss 0.703\n",
      "epoch 28, batch 250, train_loss 0.691\n",
      "epoch 28, batch 260, train_loss 0.695\n",
      "epoch 28, batch 270, train_loss 0.691\n",
      "epoch 28, batch 280, train_loss 0.693\n",
      "epoch 28, batch 290, train_loss 0.691\n",
      "epoch 28, batch 300, train_loss 0.697\n",
      "epoch 28, batch 310, train_loss 0.699\n",
      "epoch 28, batch 320, train_loss 0.698\n",
      "epoch 28, batch 330, train_loss 0.699\n",
      "epoch 28, batch 340, train_loss 0.695\n",
      "epoch 28, batch 350, train_loss 0.691\n",
      "epoch 28, batch 360, train_loss 0.701\n",
      "epoch 28, batch 370, train_loss 0.690\n",
      "epoch 28, batch 380, train_loss 0.695\n",
      "epoch 28, batch 390, train_loss 0.689\n",
      "epoch 28, batch 400, train_loss 0.705\n",
      "epoch 28, batch 410, train_loss 0.697\n",
      "epoch 28, batch 420, train_loss 0.702\n",
      "epoch 28, batch 430, train_loss 0.696\n",
      "epoch 28, batch 440, train_loss 0.697\n",
      "epoch 28, batch 450, train_loss 0.695\n",
      "epoch 28, batch 460, train_loss 0.698\n",
      "epoch 28, batch 470, train_loss 0.695\n",
      "epoch 28, batch 480, train_loss 0.699\n",
      "epoch 28, batch 490, train_loss 0.694\n",
      "epoch 28, batch 500, train_loss 0.703\n",
      "epoch 28, batch 510, train_loss 0.692\n",
      "epoch 28, batch 520, train_loss 0.696\n",
      "epoch 28, batch 530, train_loss 0.695\n",
      "epoch 28, batch 540, train_loss 0.691\n",
      "epoch 28, batch 550, train_loss 0.692\n",
      "epoch 28, batch 560, train_loss 0.681\n",
      "epoch 28, batch 570, train_loss 0.690\n",
      "epoch 28, batch 580, train_loss 0.702\n",
      "epoch 28, batch 590, train_loss 0.698\n",
      "epoch 28, batch 600, train_loss 0.693\n",
      "epoch 28, batch 610, train_loss 0.701\n",
      "epoch 28, batch 620, train_loss 0.700\n",
      "epoch 28, batch 630, train_loss 0.699\n",
      "epoch 28, batch 640, train_loss 0.698\n",
      "epoch 28, batch 650, train_loss 0.687\n",
      "epoch 28, batch 660, train_loss 0.690\n",
      "epoch 28, batch 670, train_loss 0.690\n",
      "epoch 28, batch 680, train_loss 0.702\n",
      "epoch 28, batch 690, train_loss 0.692\n",
      "epoch 28, batch 700, train_loss 0.698\n",
      "epoch 28, batch 710, train_loss 0.692\n",
      "epoch 28, batch 720, train_loss 0.692\n",
      "epoch 28, batch 730, train_loss 0.691\n",
      "epoch 28, batch 740, train_loss 0.693\n",
      "epoch 28, batch 750, train_loss 0.692\n",
      "epoch 28, batch 760, train_loss 0.695\n",
      "epoch 28, batch 770, train_loss 0.692\n",
      "epoch 28, batch 780, train_loss 0.693\n",
      "epoch 28, batch 790, train_loss 0.693\n",
      "epoch 28, batch 800, train_loss 0.695\n",
      "epoch 28, batch 810, train_loss 0.700\n",
      "epoch 28, batch 820, train_loss 0.694\n",
      "epoch 28, batch 830, train_loss 0.701\n",
      "epoch 28, batch 840, train_loss 0.693\n",
      "epoch 28, batch 850, train_loss 0.683\n",
      "epoch 28, batch 860, train_loss 0.692\n",
      "epoch 28, batch 870, train_loss 0.686\n",
      "epoch 28, batch 880, train_loss 0.695\n",
      "epoch 28, batch 890, train_loss 0.688\n",
      "epoch 28, batch 900, train_loss 0.689\n",
      "epoch 28, batch 910, train_loss 0.694\n",
      "epoch 28, batch 920, train_loss 0.683\n",
      "epoch 28, batch 930, train_loss 0.694\n",
      "epoch 28, batch 940, train_loss 0.700\n",
      "epoch 28, batch 950, train_loss 0.697\n",
      "epoch 28, batch 960, train_loss 0.699\n",
      "epoch 28, batch 970, train_loss 0.700\n",
      "epoch 28, batch 980, train_loss 0.696\n",
      "epoch 28, batch 990, train_loss 0.697\n",
      "epoch 28, batch 1000, train_loss 0.695\n",
      "epoch 28, batch 1010, train_loss 0.690\n",
      "epoch 28, batch 1020, train_loss 0.690\n",
      "epoch 28, batch 1030, train_loss 0.696\n",
      "epoch 28, batch 1040, train_loss 0.696\n",
      "epoch 28, batch 1050, train_loss 0.694\n",
      "epoch 28, batch 1060, train_loss 0.690\n",
      "epoch 28, batch 1070, train_loss 0.694\n",
      "epoch 28, batch 1080, train_loss 0.705\n",
      "epoch 28, batch 1090, train_loss 0.690\n",
      "epoch 28, batch 1100, train_loss 0.696\n",
      "epoch 28, batch 1110, train_loss 0.697\n",
      "epoch 28, batch 1120, train_loss 0.689\n",
      "epoch 28, batch 1130, train_loss 0.695\n",
      "epoch 28, batch 1140, train_loss 0.689\n",
      "epoch 28, batch 1150, train_loss 0.700\n",
      "epoch 28, batch 1160, train_loss 0.694\n",
      "epoch 28, batch 1170, train_loss 0.685\n",
      "epoch 28, batch 1180, train_loss 0.697\n",
      "epoch 28, batch 1190, train_loss 0.688\n",
      "epoch 28, train_loss 0.694, valid_loss 0.723, train_accuracy  70.15%, valid_accuracy  68.53%\n",
      "epoch 29, batch 0, train_loss 0.693\n",
      "epoch 29, batch 10, train_loss 0.711\n",
      "epoch 29, batch 20, train_loss 0.698\n",
      "epoch 29, batch 30, train_loss 0.694\n",
      "epoch 29, batch 40, train_loss 0.686\n",
      "epoch 29, batch 50, train_loss 0.684\n",
      "epoch 29, batch 60, train_loss 0.706\n",
      "epoch 29, batch 70, train_loss 0.689\n",
      "epoch 29, batch 80, train_loss 0.703\n",
      "epoch 29, batch 90, train_loss 0.699\n",
      "epoch 29, batch 100, train_loss 0.697\n",
      "epoch 29, batch 110, train_loss 0.694\n",
      "epoch 29, batch 120, train_loss 0.686\n",
      "epoch 29, batch 130, train_loss 0.697\n",
      "epoch 29, batch 140, train_loss 0.693\n",
      "epoch 29, batch 150, train_loss 0.693\n",
      "epoch 29, batch 160, train_loss 0.691\n",
      "epoch 29, batch 170, train_loss 0.691\n",
      "epoch 29, batch 180, train_loss 0.692\n",
      "epoch 29, batch 190, train_loss 0.684\n",
      "epoch 29, batch 200, train_loss 0.694\n",
      "epoch 29, batch 210, train_loss 0.699\n",
      "epoch 29, batch 220, train_loss 0.698\n",
      "epoch 29, batch 230, train_loss 0.693\n",
      "epoch 29, batch 240, train_loss 0.690\n",
      "epoch 29, batch 250, train_loss 0.690\n",
      "epoch 29, batch 260, train_loss 0.695\n",
      "epoch 29, batch 270, train_loss 0.687\n",
      "epoch 29, batch 280, train_loss 0.686\n",
      "epoch 29, batch 290, train_loss 0.693\n",
      "epoch 29, batch 300, train_loss 0.693\n",
      "epoch 29, batch 310, train_loss 0.684\n",
      "epoch 29, batch 320, train_loss 0.694\n",
      "epoch 29, batch 330, train_loss 0.699\n",
      "epoch 29, batch 340, train_loss 0.691\n",
      "epoch 29, batch 350, train_loss 0.690\n",
      "epoch 29, batch 360, train_loss 0.696\n",
      "epoch 29, batch 370, train_loss 0.695\n",
      "epoch 29, batch 380, train_loss 0.690\n",
      "epoch 29, batch 390, train_loss 0.693\n",
      "epoch 29, batch 400, train_loss 0.697\n",
      "epoch 29, batch 410, train_loss 0.700\n",
      "epoch 29, batch 420, train_loss 0.688\n",
      "epoch 29, batch 430, train_loss 0.693\n",
      "epoch 29, batch 440, train_loss 0.691\n",
      "epoch 29, batch 450, train_loss 0.684\n",
      "epoch 29, batch 460, train_loss 0.693\n",
      "epoch 29, batch 470, train_loss 0.686\n",
      "epoch 29, batch 480, train_loss 0.692\n",
      "epoch 29, batch 490, train_loss 0.691\n",
      "epoch 29, batch 500, train_loss 0.705\n",
      "epoch 29, batch 510, train_loss 0.711\n",
      "epoch 29, batch 520, train_loss 0.693\n",
      "epoch 29, batch 530, train_loss 0.700\n",
      "epoch 29, batch 540, train_loss 0.700\n",
      "epoch 29, batch 550, train_loss 0.691\n",
      "epoch 29, batch 560, train_loss 0.690\n",
      "epoch 29, batch 570, train_loss 0.700\n",
      "epoch 29, batch 580, train_loss 0.699\n",
      "epoch 29, batch 590, train_loss 0.693\n",
      "epoch 29, batch 600, train_loss 0.695\n",
      "epoch 29, batch 610, train_loss 0.704\n",
      "epoch 29, batch 620, train_loss 0.697\n",
      "epoch 29, batch 630, train_loss 0.692\n",
      "epoch 29, batch 640, train_loss 0.687\n",
      "epoch 29, batch 650, train_loss 0.699\n",
      "epoch 29, batch 660, train_loss 0.699\n",
      "epoch 29, batch 670, train_loss 0.697\n",
      "epoch 29, batch 680, train_loss 0.702\n",
      "epoch 29, batch 690, train_loss 0.692\n",
      "epoch 29, batch 700, train_loss 0.687\n",
      "epoch 29, batch 710, train_loss 0.689\n",
      "epoch 29, batch 720, train_loss 0.699\n",
      "epoch 29, batch 730, train_loss 0.692\n",
      "epoch 29, batch 740, train_loss 0.694\n",
      "epoch 29, batch 750, train_loss 0.699\n",
      "epoch 29, batch 760, train_loss 0.687\n",
      "epoch 29, batch 770, train_loss 0.694\n",
      "epoch 29, batch 780, train_loss 0.696\n",
      "epoch 29, batch 790, train_loss 0.701\n",
      "epoch 29, batch 800, train_loss 0.687\n",
      "epoch 29, batch 810, train_loss 0.691\n",
      "epoch 29, batch 820, train_loss 0.687\n",
      "epoch 29, batch 830, train_loss 0.691\n",
      "epoch 29, batch 840, train_loss 0.697\n",
      "epoch 29, batch 850, train_loss 0.685\n",
      "epoch 29, batch 860, train_loss 0.696\n",
      "epoch 29, batch 870, train_loss 0.690\n",
      "epoch 29, batch 880, train_loss 0.689\n",
      "epoch 29, batch 890, train_loss 0.691\n",
      "epoch 29, batch 900, train_loss 0.688\n",
      "epoch 29, batch 910, train_loss 0.691\n",
      "epoch 29, batch 920, train_loss 0.698\n",
      "epoch 29, batch 930, train_loss 0.695\n",
      "epoch 29, batch 940, train_loss 0.696\n",
      "epoch 29, batch 950, train_loss 0.695\n",
      "epoch 29, batch 960, train_loss 0.694\n",
      "epoch 29, batch 970, train_loss 0.694\n",
      "epoch 29, batch 980, train_loss 0.693\n",
      "epoch 29, batch 990, train_loss 0.691\n",
      "epoch 29, batch 1000, train_loss 0.698\n",
      "epoch 29, batch 1010, train_loss 0.694\n",
      "epoch 29, batch 1020, train_loss 0.698\n",
      "epoch 29, batch 1030, train_loss 0.699\n",
      "epoch 29, batch 1040, train_loss 0.692\n",
      "epoch 29, batch 1050, train_loss 0.681\n",
      "epoch 29, batch 1060, train_loss 0.696\n",
      "epoch 29, batch 1070, train_loss 0.700\n",
      "epoch 29, batch 1080, train_loss 0.691\n",
      "epoch 29, batch 1090, train_loss 0.700\n",
      "epoch 29, batch 1100, train_loss 0.693\n",
      "epoch 29, batch 1110, train_loss 0.690\n",
      "epoch 29, batch 1120, train_loss 0.695\n",
      "epoch 29, batch 1130, train_loss 0.697\n",
      "epoch 29, batch 1140, train_loss 0.702\n",
      "epoch 29, batch 1150, train_loss 0.690\n",
      "epoch 29, batch 1160, train_loss 0.696\n",
      "epoch 29, batch 1170, train_loss 0.700\n",
      "epoch 29, batch 1180, train_loss 0.704\n",
      "epoch 29, batch 1190, train_loss 0.694\n",
      "epoch 29, train_loss 0.694, valid_loss 0.723, train_accuracy  70.17%, valid_accuracy  68.60%\n",
      "epoch 30, batch 0, train_loss 0.709\n",
      "epoch 30, batch 10, train_loss 0.690\n",
      "epoch 30, batch 20, train_loss 0.703\n",
      "epoch 30, batch 30, train_loss 0.700\n",
      "epoch 30, batch 40, train_loss 0.688\n",
      "epoch 30, batch 50, train_loss 0.683\n",
      "epoch 30, batch 60, train_loss 0.711\n",
      "epoch 30, batch 70, train_loss 0.694\n",
      "epoch 30, batch 80, train_loss 0.694\n",
      "epoch 30, batch 90, train_loss 0.697\n",
      "epoch 30, batch 100, train_loss 0.699\n",
      "epoch 30, batch 110, train_loss 0.692\n",
      "epoch 30, batch 120, train_loss 0.692\n",
      "epoch 30, batch 130, train_loss 0.686\n",
      "epoch 30, batch 140, train_loss 0.691\n",
      "epoch 30, batch 150, train_loss 0.683\n",
      "epoch 30, batch 160, train_loss 0.698\n",
      "epoch 30, batch 170, train_loss 0.695\n",
      "epoch 30, batch 180, train_loss 0.695\n",
      "epoch 30, batch 190, train_loss 0.687\n",
      "epoch 30, batch 200, train_loss 0.696\n",
      "epoch 30, batch 210, train_loss 0.702\n",
      "epoch 30, batch 220, train_loss 0.687\n",
      "epoch 30, batch 230, train_loss 0.699\n",
      "epoch 30, batch 240, train_loss 0.697\n",
      "epoch 30, batch 250, train_loss 0.698\n",
      "epoch 30, batch 260, train_loss 0.681\n",
      "epoch 30, batch 270, train_loss 0.697\n",
      "epoch 30, batch 280, train_loss 0.701\n",
      "epoch 30, batch 290, train_loss 0.695\n",
      "epoch 30, batch 300, train_loss 0.693\n",
      "epoch 30, batch 310, train_loss 0.698\n",
      "epoch 30, batch 320, train_loss 0.701\n",
      "epoch 30, batch 330, train_loss 0.697\n",
      "epoch 30, batch 340, train_loss 0.695\n",
      "epoch 30, batch 350, train_loss 0.699\n",
      "epoch 30, batch 360, train_loss 0.698\n",
      "epoch 30, batch 370, train_loss 0.682\n",
      "epoch 30, batch 380, train_loss 0.697\n",
      "epoch 30, batch 390, train_loss 0.700\n",
      "epoch 30, batch 400, train_loss 0.698\n",
      "epoch 30, batch 410, train_loss 0.686\n",
      "epoch 30, batch 420, train_loss 0.701\n",
      "epoch 30, batch 430, train_loss 0.690\n",
      "epoch 30, batch 440, train_loss 0.694\n",
      "epoch 30, batch 450, train_loss 0.676\n",
      "epoch 30, batch 460, train_loss 0.694\n",
      "epoch 30, batch 470, train_loss 0.691\n",
      "epoch 30, batch 480, train_loss 0.691\n",
      "epoch 30, batch 490, train_loss 0.702\n",
      "epoch 30, batch 500, train_loss 0.681\n",
      "epoch 30, batch 510, train_loss 0.700\n",
      "epoch 30, batch 520, train_loss 0.692\n",
      "epoch 30, batch 530, train_loss 0.691\n",
      "epoch 30, batch 540, train_loss 0.690\n",
      "epoch 30, batch 550, train_loss 0.697\n",
      "epoch 30, batch 560, train_loss 0.690\n",
      "epoch 30, batch 570, train_loss 0.695\n",
      "epoch 30, batch 580, train_loss 0.699\n",
      "epoch 30, batch 590, train_loss 0.687\n",
      "epoch 30, batch 600, train_loss 0.694\n",
      "epoch 30, batch 610, train_loss 0.694\n",
      "epoch 30, batch 620, train_loss 0.687\n",
      "epoch 30, batch 630, train_loss 0.704\n",
      "epoch 30, batch 640, train_loss 0.689\n",
      "epoch 30, batch 650, train_loss 0.696\n",
      "epoch 30, batch 660, train_loss 0.695\n",
      "epoch 30, batch 670, train_loss 0.701\n",
      "epoch 30, batch 680, train_loss 0.696\n",
      "epoch 30, batch 690, train_loss 0.699\n",
      "epoch 30, batch 700, train_loss 0.685\n",
      "epoch 30, batch 710, train_loss 0.693\n",
      "epoch 30, batch 720, train_loss 0.689\n",
      "epoch 30, batch 730, train_loss 0.696\n",
      "epoch 30, batch 740, train_loss 0.701\n",
      "epoch 30, batch 750, train_loss 0.689\n",
      "epoch 30, batch 760, train_loss 0.686\n",
      "epoch 30, batch 770, train_loss 0.695\n",
      "epoch 30, batch 780, train_loss 0.692\n",
      "epoch 30, batch 790, train_loss 0.692\n",
      "epoch 30, batch 800, train_loss 0.694\n",
      "epoch 30, batch 810, train_loss 0.690\n",
      "epoch 30, batch 820, train_loss 0.694\n",
      "epoch 30, batch 830, train_loss 0.695\n",
      "epoch 30, batch 840, train_loss 0.693\n",
      "epoch 30, batch 850, train_loss 0.698\n",
      "epoch 30, batch 860, train_loss 0.695\n",
      "epoch 30, batch 870, train_loss 0.690\n",
      "epoch 30, batch 880, train_loss 0.697\n",
      "epoch 30, batch 890, train_loss 0.696\n",
      "epoch 30, batch 900, train_loss 0.698\n",
      "epoch 30, batch 910, train_loss 0.696\n",
      "epoch 30, batch 920, train_loss 0.705\n",
      "epoch 30, batch 930, train_loss 0.700\n",
      "epoch 30, batch 940, train_loss 0.693\n",
      "epoch 30, batch 950, train_loss 0.693\n",
      "epoch 30, batch 960, train_loss 0.696\n",
      "epoch 30, batch 970, train_loss 0.694\n",
      "epoch 30, batch 980, train_loss 0.695\n",
      "epoch 30, batch 990, train_loss 0.694\n",
      "epoch 30, batch 1000, train_loss 0.685\n",
      "epoch 30, batch 1010, train_loss 0.691\n",
      "epoch 30, batch 1020, train_loss 0.701\n",
      "epoch 30, batch 1030, train_loss 0.704\n",
      "epoch 30, batch 1040, train_loss 0.696\n",
      "epoch 30, batch 1050, train_loss 0.694\n",
      "epoch 30, batch 1060, train_loss 0.683\n",
      "epoch 30, batch 1070, train_loss 0.701\n",
      "epoch 30, batch 1080, train_loss 0.701\n",
      "epoch 30, batch 1090, train_loss 0.682\n",
      "epoch 30, batch 1100, train_loss 0.699\n",
      "epoch 30, batch 1110, train_loss 0.701\n",
      "epoch 30, batch 1120, train_loss 0.688\n",
      "epoch 30, batch 1130, train_loss 0.690\n",
      "epoch 30, batch 1140, train_loss 0.706\n",
      "epoch 30, batch 1150, train_loss 0.696\n",
      "epoch 30, batch 1160, train_loss 0.694\n",
      "epoch 30, batch 1170, train_loss 0.693\n",
      "epoch 30, batch 1180, train_loss 0.690\n",
      "epoch 30, batch 1190, train_loss 0.692\n",
      "epoch 30, train_loss 0.693, valid_loss 0.724, train_accuracy  70.19%, valid_accuracy  68.55%\n",
      "epoch 31, batch 0, train_loss 0.695\n",
      "epoch 31, batch 10, train_loss 0.694\n",
      "epoch 31, batch 20, train_loss 0.690\n",
      "epoch 31, batch 30, train_loss 0.689\n",
      "epoch 31, batch 40, train_loss 0.694\n",
      "epoch 31, batch 50, train_loss 0.689\n",
      "epoch 31, batch 60, train_loss 0.695\n",
      "epoch 31, batch 70, train_loss 0.693\n",
      "epoch 31, batch 80, train_loss 0.694\n",
      "epoch 31, batch 90, train_loss 0.698\n",
      "epoch 31, batch 100, train_loss 0.701\n",
      "epoch 31, batch 110, train_loss 0.695\n",
      "epoch 31, batch 120, train_loss 0.700\n",
      "epoch 31, batch 130, train_loss 0.691\n",
      "epoch 31, batch 140, train_loss 0.702\n",
      "epoch 31, batch 150, train_loss 0.697\n",
      "epoch 31, batch 160, train_loss 0.698\n",
      "epoch 31, batch 170, train_loss 0.691\n",
      "epoch 31, batch 180, train_loss 0.705\n",
      "epoch 31, batch 190, train_loss 0.695\n",
      "epoch 31, batch 200, train_loss 0.695\n",
      "epoch 31, batch 210, train_loss 0.689\n",
      "epoch 31, batch 220, train_loss 0.692\n",
      "epoch 31, batch 230, train_loss 0.698\n",
      "epoch 31, batch 240, train_loss 0.693\n",
      "epoch 31, batch 250, train_loss 0.696\n",
      "epoch 31, batch 260, train_loss 0.692\n",
      "epoch 31, batch 270, train_loss 0.694\n",
      "epoch 31, batch 280, train_loss 0.699\n",
      "epoch 31, batch 290, train_loss 0.693\n",
      "epoch 31, batch 300, train_loss 0.691\n",
      "epoch 31, batch 310, train_loss 0.703\n",
      "epoch 31, batch 320, train_loss 0.686\n",
      "epoch 31, batch 330, train_loss 0.696\n",
      "epoch 31, batch 340, train_loss 0.685\n",
      "epoch 31, batch 350, train_loss 0.699\n",
      "epoch 31, batch 360, train_loss 0.684\n",
      "epoch 31, batch 370, train_loss 0.691\n",
      "epoch 31, batch 380, train_loss 0.687\n",
      "epoch 31, batch 390, train_loss 0.701\n",
      "epoch 31, batch 400, train_loss 0.698\n",
      "epoch 31, batch 410, train_loss 0.694\n",
      "epoch 31, batch 420, train_loss 0.695\n",
      "epoch 31, batch 430, train_loss 0.697\n",
      "epoch 31, batch 440, train_loss 0.693\n",
      "epoch 31, batch 450, train_loss 0.690\n",
      "epoch 31, batch 460, train_loss 0.699\n",
      "epoch 31, batch 470, train_loss 0.685\n",
      "epoch 31, batch 480, train_loss 0.692\n",
      "epoch 31, batch 490, train_loss 0.694\n",
      "epoch 31, batch 500, train_loss 0.688\n",
      "epoch 31, batch 510, train_loss 0.701\n",
      "epoch 31, batch 520, train_loss 0.685\n",
      "epoch 31, batch 530, train_loss 0.694\n",
      "epoch 31, batch 540, train_loss 0.700\n",
      "epoch 31, batch 550, train_loss 0.693\n",
      "epoch 31, batch 560, train_loss 0.694\n",
      "epoch 31, batch 570, train_loss 0.697\n",
      "epoch 31, batch 580, train_loss 0.695\n",
      "epoch 31, batch 590, train_loss 0.698\n",
      "epoch 31, batch 600, train_loss 0.684\n",
      "epoch 31, batch 610, train_loss 0.703\n",
      "epoch 31, batch 620, train_loss 0.696\n",
      "epoch 31, batch 630, train_loss 0.703\n",
      "epoch 31, batch 640, train_loss 0.696\n",
      "epoch 31, batch 650, train_loss 0.693\n",
      "epoch 31, batch 660, train_loss 0.703\n",
      "epoch 31, batch 670, train_loss 0.692\n",
      "epoch 31, batch 680, train_loss 0.701\n",
      "epoch 31, batch 690, train_loss 0.696\n",
      "epoch 31, batch 700, train_loss 0.693\n",
      "epoch 31, batch 710, train_loss 0.690\n",
      "epoch 31, batch 720, train_loss 0.692\n",
      "epoch 31, batch 730, train_loss 0.690\n",
      "epoch 31, batch 740, train_loss 0.699\n",
      "epoch 31, batch 750, train_loss 0.692\n",
      "epoch 31, batch 760, train_loss 0.696\n",
      "epoch 31, batch 770, train_loss 0.695\n",
      "epoch 31, batch 780, train_loss 0.696\n",
      "epoch 31, batch 790, train_loss 0.691\n",
      "epoch 31, batch 800, train_loss 0.700\n",
      "epoch 31, batch 810, train_loss 0.688\n",
      "epoch 31, batch 820, train_loss 0.686\n",
      "epoch 31, batch 830, train_loss 0.696\n",
      "epoch 31, batch 840, train_loss 0.687\n",
      "epoch 31, batch 850, train_loss 0.693\n",
      "epoch 31, batch 860, train_loss 0.705\n",
      "epoch 31, batch 870, train_loss 0.690\n",
      "epoch 31, batch 880, train_loss 0.696\n",
      "epoch 31, batch 890, train_loss 0.688\n",
      "epoch 31, batch 900, train_loss 0.695\n",
      "epoch 31, batch 910, train_loss 0.688\n",
      "epoch 31, batch 920, train_loss 0.690\n",
      "epoch 31, batch 930, train_loss 0.693\n",
      "epoch 31, batch 940, train_loss 0.688\n",
      "epoch 31, batch 950, train_loss 0.694\n",
      "epoch 31, batch 960, train_loss 0.700\n",
      "epoch 31, batch 970, train_loss 0.700\n",
      "epoch 31, batch 980, train_loss 0.693\n",
      "epoch 31, batch 990, train_loss 0.680\n",
      "epoch 31, batch 1000, train_loss 0.697\n",
      "epoch 31, batch 1010, train_loss 0.693\n",
      "epoch 31, batch 1020, train_loss 0.689\n",
      "epoch 31, batch 1030, train_loss 0.693\n",
      "epoch 31, batch 1040, train_loss 0.697\n",
      "epoch 31, batch 1050, train_loss 0.704\n",
      "epoch 31, batch 1060, train_loss 0.691\n",
      "epoch 31, batch 1070, train_loss 0.694\n",
      "epoch 31, batch 1080, train_loss 0.694\n",
      "epoch 31, batch 1090, train_loss 0.690\n",
      "epoch 31, batch 1100, train_loss 0.702\n",
      "epoch 31, batch 1110, train_loss 0.700\n",
      "epoch 31, batch 1120, train_loss 0.685\n",
      "epoch 31, batch 1130, train_loss 0.686\n",
      "epoch 31, batch 1140, train_loss 0.689\n",
      "epoch 31, batch 1150, train_loss 0.701\n",
      "epoch 31, batch 1160, train_loss 0.694\n",
      "epoch 31, batch 1170, train_loss 0.692\n",
      "epoch 31, batch 1180, train_loss 0.697\n",
      "epoch 31, batch 1190, train_loss 0.695\n",
      "epoch 31, train_loss 0.693, valid_loss 0.723, train_accuracy  70.19%, valid_accuracy  68.57%\n",
      "epoch 32, batch 0, train_loss 0.693\n",
      "epoch 32, batch 10, train_loss 0.688\n",
      "epoch 32, batch 20, train_loss 0.692\n",
      "epoch 32, batch 30, train_loss 0.688\n",
      "epoch 32, batch 40, train_loss 0.689\n",
      "epoch 32, batch 50, train_loss 0.688\n",
      "epoch 32, batch 60, train_loss 0.689\n",
      "epoch 32, batch 70, train_loss 0.695\n",
      "epoch 32, batch 80, train_loss 0.695\n",
      "epoch 32, batch 90, train_loss 0.691\n",
      "epoch 32, batch 100, train_loss 0.700\n",
      "epoch 32, batch 110, train_loss 0.695\n",
      "epoch 32, batch 120, train_loss 0.692\n",
      "epoch 32, batch 130, train_loss 0.690\n",
      "epoch 32, batch 140, train_loss 0.690\n",
      "epoch 32, batch 150, train_loss 0.696\n",
      "epoch 32, batch 160, train_loss 0.694\n",
      "epoch 32, batch 170, train_loss 0.692\n",
      "epoch 32, batch 180, train_loss 0.684\n",
      "epoch 32, batch 190, train_loss 0.701\n",
      "epoch 32, batch 200, train_loss 0.700\n",
      "epoch 32, batch 210, train_loss 0.696\n",
      "epoch 32, batch 220, train_loss 0.694\n",
      "epoch 32, batch 230, train_loss 0.697\n",
      "epoch 32, batch 240, train_loss 0.699\n",
      "epoch 32, batch 250, train_loss 0.699\n",
      "epoch 32, batch 260, train_loss 0.689\n",
      "epoch 32, batch 270, train_loss 0.687\n",
      "epoch 32, batch 280, train_loss 0.696\n",
      "epoch 32, batch 290, train_loss 0.700\n",
      "epoch 32, batch 300, train_loss 0.683\n",
      "epoch 32, batch 310, train_loss 0.701\n",
      "epoch 32, batch 320, train_loss 0.702\n",
      "epoch 32, batch 330, train_loss 0.703\n",
      "epoch 32, batch 340, train_loss 0.686\n",
      "epoch 32, batch 350, train_loss 0.700\n",
      "epoch 32, batch 360, train_loss 0.698\n",
      "epoch 32, batch 370, train_loss 0.696\n",
      "epoch 32, batch 380, train_loss 0.706\n",
      "epoch 32, batch 390, train_loss 0.689\n",
      "epoch 32, batch 400, train_loss 0.703\n",
      "epoch 32, batch 410, train_loss 0.682\n",
      "epoch 32, batch 420, train_loss 0.693\n",
      "epoch 32, batch 430, train_loss 0.695\n",
      "epoch 32, batch 440, train_loss 0.701\n",
      "epoch 32, batch 450, train_loss 0.697\n",
      "epoch 32, batch 460, train_loss 0.693\n",
      "epoch 32, batch 470, train_loss 0.697\n",
      "epoch 32, batch 480, train_loss 0.690\n",
      "epoch 32, batch 490, train_loss 0.695\n",
      "epoch 32, batch 500, train_loss 0.687\n",
      "epoch 32, batch 510, train_loss 0.697\n",
      "epoch 32, batch 520, train_loss 0.693\n",
      "epoch 32, batch 530, train_loss 0.691\n",
      "epoch 32, batch 540, train_loss 0.696\n",
      "epoch 32, batch 550, train_loss 0.699\n",
      "epoch 32, batch 560, train_loss 0.679\n",
      "epoch 32, batch 570, train_loss 0.692\n",
      "epoch 32, batch 580, train_loss 0.694\n",
      "epoch 32, batch 590, train_loss 0.696\n",
      "epoch 32, batch 600, train_loss 0.696\n",
      "epoch 32, batch 610, train_loss 0.692\n",
      "epoch 32, batch 620, train_loss 0.698\n",
      "epoch 32, batch 630, train_loss 0.691\n",
      "epoch 32, batch 640, train_loss 0.688\n",
      "epoch 32, batch 650, train_loss 0.696\n",
      "epoch 32, batch 660, train_loss 0.699\n",
      "epoch 32, batch 670, train_loss 0.700\n",
      "epoch 32, batch 680, train_loss 0.703\n",
      "epoch 32, batch 690, train_loss 0.686\n",
      "epoch 32, batch 700, train_loss 0.700\n",
      "epoch 32, batch 710, train_loss 0.695\n",
      "epoch 32, batch 720, train_loss 0.698\n",
      "epoch 32, batch 730, train_loss 0.691\n",
      "epoch 32, batch 740, train_loss 0.702\n",
      "epoch 32, batch 750, train_loss 0.691\n",
      "epoch 32, batch 760, train_loss 0.690\n",
      "epoch 32, batch 770, train_loss 0.690\n",
      "epoch 32, batch 780, train_loss 0.695\n",
      "epoch 32, batch 790, train_loss 0.692\n",
      "epoch 32, batch 800, train_loss 0.697\n",
      "epoch 32, batch 810, train_loss 0.698\n",
      "epoch 32, batch 820, train_loss 0.701\n",
      "epoch 32, batch 830, train_loss 0.694\n",
      "epoch 32, batch 840, train_loss 0.695\n",
      "epoch 32, batch 850, train_loss 0.695\n",
      "epoch 32, batch 860, train_loss 0.686\n",
      "epoch 32, batch 870, train_loss 0.685\n",
      "epoch 32, batch 880, train_loss 0.691\n",
      "epoch 32, batch 890, train_loss 0.694\n",
      "epoch 32, batch 900, train_loss 0.689\n",
      "epoch 32, batch 910, train_loss 0.692\n",
      "epoch 32, batch 920, train_loss 0.701\n",
      "epoch 32, batch 930, train_loss 0.697\n",
      "epoch 32, batch 940, train_loss 0.698\n",
      "epoch 32, batch 950, train_loss 0.694\n",
      "epoch 32, batch 960, train_loss 0.698\n",
      "epoch 32, batch 970, train_loss 0.691\n",
      "epoch 32, batch 980, train_loss 0.698\n",
      "epoch 32, batch 990, train_loss 0.708\n",
      "epoch 32, batch 1000, train_loss 0.686\n",
      "epoch 32, batch 1010, train_loss 0.699\n",
      "epoch 32, batch 1020, train_loss 0.700\n",
      "epoch 32, batch 1030, train_loss 0.682\n",
      "epoch 32, batch 1040, train_loss 0.694\n",
      "epoch 32, batch 1050, train_loss 0.694\n",
      "epoch 32, batch 1060, train_loss 0.685\n",
      "epoch 32, batch 1070, train_loss 0.692\n",
      "epoch 32, batch 1080, train_loss 0.689\n",
      "epoch 32, batch 1090, train_loss 0.699\n",
      "epoch 32, batch 1100, train_loss 0.690\n",
      "epoch 32, batch 1110, train_loss 0.688\n",
      "epoch 32, batch 1120, train_loss 0.695\n",
      "epoch 32, batch 1130, train_loss 0.694\n",
      "epoch 32, batch 1140, train_loss 0.700\n",
      "epoch 32, batch 1150, train_loss 0.704\n",
      "epoch 32, batch 1160, train_loss 0.703\n",
      "epoch 32, batch 1170, train_loss 0.698\n",
      "epoch 32, batch 1180, train_loss 0.688\n",
      "epoch 32, batch 1190, train_loss 0.697\n",
      "epoch 32, train_loss 0.693, valid_loss 0.723, train_accuracy  70.18%, valid_accuracy  68.57%\n",
      "epoch 33, batch 0, train_loss 0.691\n",
      "epoch 33, batch 10, train_loss 0.687\n",
      "epoch 33, batch 20, train_loss 0.695\n",
      "epoch 33, batch 30, train_loss 0.693\n",
      "epoch 33, batch 40, train_loss 0.686\n",
      "epoch 33, batch 50, train_loss 0.699\n",
      "epoch 33, batch 60, train_loss 0.693\n",
      "epoch 33, batch 70, train_loss 0.695\n",
      "epoch 33, batch 80, train_loss 0.695\n",
      "epoch 33, batch 90, train_loss 0.691\n",
      "epoch 33, batch 100, train_loss 0.693\n",
      "epoch 33, batch 110, train_loss 0.685\n",
      "epoch 33, batch 120, train_loss 0.697\n",
      "epoch 33, batch 130, train_loss 0.693\n",
      "epoch 33, batch 140, train_loss 0.688\n",
      "epoch 33, batch 150, train_loss 0.692\n",
      "epoch 33, batch 160, train_loss 0.698\n",
      "epoch 33, batch 170, train_loss 0.693\n",
      "epoch 33, batch 180, train_loss 0.687\n",
      "epoch 33, batch 190, train_loss 0.692\n",
      "epoch 33, batch 200, train_loss 0.689\n",
      "epoch 33, batch 210, train_loss 0.692\n",
      "epoch 33, batch 220, train_loss 0.687\n",
      "epoch 33, batch 230, train_loss 0.687\n",
      "epoch 33, batch 240, train_loss 0.696\n",
      "epoch 33, batch 250, train_loss 0.693\n",
      "epoch 33, batch 260, train_loss 0.694\n",
      "epoch 33, batch 270, train_loss 0.700\n",
      "epoch 33, batch 280, train_loss 0.700\n",
      "epoch 33, batch 290, train_loss 0.701\n",
      "epoch 33, batch 300, train_loss 0.695\n",
      "epoch 33, batch 310, train_loss 0.701\n",
      "epoch 33, batch 320, train_loss 0.698\n",
      "epoch 33, batch 330, train_loss 0.691\n",
      "epoch 33, batch 340, train_loss 0.693\n",
      "epoch 33, batch 350, train_loss 0.684\n",
      "epoch 33, batch 360, train_loss 0.693\n",
      "epoch 33, batch 370, train_loss 0.692\n",
      "epoch 33, batch 380, train_loss 0.689\n",
      "epoch 33, batch 390, train_loss 0.694\n",
      "epoch 33, batch 400, train_loss 0.695\n",
      "epoch 33, batch 410, train_loss 0.693\n",
      "epoch 33, batch 420, train_loss 0.692\n",
      "epoch 33, batch 430, train_loss 0.700\n",
      "epoch 33, batch 440, train_loss 0.697\n",
      "epoch 33, batch 450, train_loss 0.686\n",
      "epoch 33, batch 460, train_loss 0.693\n",
      "epoch 33, batch 470, train_loss 0.694\n",
      "epoch 33, batch 480, train_loss 0.696\n",
      "epoch 33, batch 490, train_loss 0.693\n",
      "epoch 33, batch 500, train_loss 0.695\n",
      "epoch 33, batch 510, train_loss 0.691\n",
      "epoch 33, batch 520, train_loss 0.691\n",
      "epoch 33, batch 530, train_loss 0.701\n",
      "epoch 33, batch 540, train_loss 0.691\n",
      "epoch 33, batch 550, train_loss 0.694\n",
      "epoch 33, batch 560, train_loss 0.689\n",
      "epoch 33, batch 570, train_loss 0.691\n",
      "epoch 33, batch 580, train_loss 0.698\n",
      "epoch 33, batch 590, train_loss 0.696\n",
      "epoch 33, batch 600, train_loss 0.704\n",
      "epoch 33, batch 610, train_loss 0.698\n",
      "epoch 33, batch 620, train_loss 0.689\n",
      "epoch 33, batch 630, train_loss 0.693\n",
      "epoch 33, batch 640, train_loss 0.688\n",
      "epoch 33, batch 650, train_loss 0.701\n",
      "epoch 33, batch 660, train_loss 0.698\n",
      "epoch 33, batch 670, train_loss 0.692\n",
      "epoch 33, batch 680, train_loss 0.697\n",
      "epoch 33, batch 690, train_loss 0.685\n",
      "epoch 33, batch 700, train_loss 0.688\n",
      "epoch 33, batch 710, train_loss 0.695\n",
      "epoch 33, batch 720, train_loss 0.687\n",
      "epoch 33, batch 730, train_loss 0.694\n",
      "epoch 33, batch 740, train_loss 0.699\n",
      "epoch 33, batch 750, train_loss 0.694\n",
      "epoch 33, batch 760, train_loss 0.690\n",
      "epoch 33, batch 770, train_loss 0.688\n",
      "epoch 33, batch 780, train_loss 0.690\n",
      "epoch 33, batch 790, train_loss 0.694\n",
      "epoch 33, batch 800, train_loss 0.693\n",
      "epoch 33, batch 810, train_loss 0.694\n",
      "epoch 33, batch 820, train_loss 0.704\n",
      "epoch 33, batch 830, train_loss 0.700\n",
      "epoch 33, batch 840, train_loss 0.689\n",
      "epoch 33, batch 850, train_loss 0.694\n",
      "epoch 33, batch 860, train_loss 0.703\n",
      "epoch 33, batch 870, train_loss 0.693\n",
      "epoch 33, batch 880, train_loss 0.695\n",
      "epoch 33, batch 890, train_loss 0.693\n",
      "epoch 33, batch 900, train_loss 0.695\n",
      "epoch 33, batch 910, train_loss 0.704\n",
      "epoch 33, batch 920, train_loss 0.684\n",
      "epoch 33, batch 930, train_loss 0.702\n",
      "epoch 33, batch 940, train_loss 0.692\n",
      "epoch 33, batch 950, train_loss 0.697\n",
      "epoch 33, batch 960, train_loss 0.693\n",
      "epoch 33, batch 970, train_loss 0.695\n",
      "epoch 33, batch 980, train_loss 0.689\n",
      "epoch 33, batch 990, train_loss 0.689\n",
      "epoch 33, batch 1000, train_loss 0.689\n",
      "epoch 33, batch 1010, train_loss 0.686\n",
      "epoch 33, batch 1020, train_loss 0.697\n",
      "epoch 33, batch 1030, train_loss 0.701\n",
      "epoch 33, batch 1040, train_loss 0.698\n",
      "epoch 33, batch 1050, train_loss 0.704\n",
      "epoch 33, batch 1060, train_loss 0.689\n",
      "epoch 33, batch 1070, train_loss 0.693\n",
      "epoch 33, batch 1080, train_loss 0.692\n",
      "epoch 33, batch 1090, train_loss 0.688\n",
      "epoch 33, batch 1100, train_loss 0.696\n",
      "epoch 33, batch 1110, train_loss 0.698\n",
      "epoch 33, batch 1120, train_loss 0.694\n",
      "epoch 33, batch 1130, train_loss 0.705\n",
      "epoch 33, batch 1140, train_loss 0.694\n",
      "epoch 33, batch 1150, train_loss 0.694\n",
      "epoch 33, batch 1160, train_loss 0.693\n",
      "epoch 33, batch 1170, train_loss 0.702\n",
      "epoch 33, batch 1180, train_loss 0.694\n",
      "epoch 33, batch 1190, train_loss 0.700\n",
      "epoch 33, train_loss 0.693, valid_loss 0.723, train_accuracy  70.23%, valid_accuracy  68.56%\n",
      "epoch 34, batch 0, train_loss 0.693\n",
      "epoch 34, batch 10, train_loss 0.689\n",
      "epoch 34, batch 20, train_loss 0.694\n",
      "epoch 34, batch 30, train_loss 0.698\n",
      "epoch 34, batch 40, train_loss 0.697\n",
      "epoch 34, batch 50, train_loss 0.697\n",
      "epoch 34, batch 60, train_loss 0.692\n",
      "epoch 34, batch 70, train_loss 0.690\n",
      "epoch 34, batch 80, train_loss 0.692\n",
      "epoch 34, batch 90, train_loss 0.696\n",
      "epoch 34, batch 100, train_loss 0.700\n",
      "epoch 34, batch 110, train_loss 0.699\n",
      "epoch 34, batch 120, train_loss 0.690\n",
      "epoch 34, batch 130, train_loss 0.680\n",
      "epoch 34, batch 140, train_loss 0.685\n",
      "epoch 34, batch 150, train_loss 0.686\n",
      "epoch 34, batch 160, train_loss 0.687\n",
      "epoch 34, batch 170, train_loss 0.686\n",
      "epoch 34, batch 180, train_loss 0.697\n",
      "epoch 34, batch 190, train_loss 0.700\n",
      "epoch 34, batch 200, train_loss 0.688\n",
      "epoch 34, batch 210, train_loss 0.694\n",
      "epoch 34, batch 220, train_loss 0.688\n",
      "epoch 34, batch 230, train_loss 0.689\n",
      "epoch 34, batch 240, train_loss 0.689\n",
      "epoch 34, batch 250, train_loss 0.690\n",
      "epoch 34, batch 260, train_loss 0.691\n",
      "epoch 34, batch 270, train_loss 0.689\n",
      "epoch 34, batch 280, train_loss 0.689\n",
      "epoch 34, batch 290, train_loss 0.690\n",
      "epoch 34, batch 300, train_loss 0.688\n",
      "epoch 34, batch 310, train_loss 0.687\n",
      "epoch 34, batch 320, train_loss 0.699\n",
      "epoch 34, batch 330, train_loss 0.688\n",
      "epoch 34, batch 340, train_loss 0.690\n",
      "epoch 34, batch 350, train_loss 0.697\n",
      "epoch 34, batch 360, train_loss 0.699\n",
      "epoch 34, batch 370, train_loss 0.697\n",
      "epoch 34, batch 380, train_loss 0.695\n",
      "epoch 34, batch 390, train_loss 0.685\n",
      "epoch 34, batch 400, train_loss 0.700\n",
      "epoch 34, batch 410, train_loss 0.693\n",
      "epoch 34, batch 420, train_loss 0.705\n",
      "epoch 34, batch 430, train_loss 0.685\n",
      "epoch 34, batch 440, train_loss 0.698\n",
      "epoch 34, batch 450, train_loss 0.696\n",
      "epoch 34, batch 460, train_loss 0.694\n",
      "epoch 34, batch 470, train_loss 0.683\n",
      "epoch 34, batch 480, train_loss 0.700\n",
      "epoch 34, batch 490, train_loss 0.693\n",
      "epoch 34, batch 500, train_loss 0.691\n",
      "epoch 34, batch 510, train_loss 0.700\n",
      "epoch 34, batch 520, train_loss 0.691\n",
      "epoch 34, batch 530, train_loss 0.705\n",
      "epoch 34, batch 540, train_loss 0.694\n",
      "epoch 34, batch 550, train_loss 0.697\n",
      "epoch 34, batch 560, train_loss 0.688\n",
      "epoch 34, batch 570, train_loss 0.700\n",
      "epoch 34, batch 580, train_loss 0.683\n",
      "epoch 34, batch 590, train_loss 0.685\n",
      "epoch 34, batch 600, train_loss 0.706\n",
      "epoch 34, batch 610, train_loss 0.699\n",
      "epoch 34, batch 620, train_loss 0.697\n",
      "epoch 34, batch 630, train_loss 0.697\n",
      "epoch 34, batch 640, train_loss 0.694\n",
      "epoch 34, batch 650, train_loss 0.692\n",
      "epoch 34, batch 660, train_loss 0.697\n",
      "epoch 34, batch 670, train_loss 0.700\n",
      "epoch 34, batch 680, train_loss 0.692\n",
      "epoch 34, batch 690, train_loss 0.699\n",
      "epoch 34, batch 700, train_loss 0.692\n",
      "epoch 34, batch 710, train_loss 0.688\n",
      "epoch 34, batch 720, train_loss 0.691\n",
      "epoch 34, batch 730, train_loss 0.694\n",
      "epoch 34, batch 740, train_loss 0.694\n",
      "epoch 34, batch 750, train_loss 0.688\n",
      "epoch 34, batch 760, train_loss 0.687\n",
      "epoch 34, batch 770, train_loss 0.700\n",
      "epoch 34, batch 780, train_loss 0.693\n",
      "epoch 34, batch 790, train_loss 0.688\n",
      "epoch 34, batch 800, train_loss 0.697\n",
      "epoch 34, batch 810, train_loss 0.683\n",
      "epoch 34, batch 820, train_loss 0.696\n",
      "epoch 34, batch 830, train_loss 0.690\n",
      "epoch 34, batch 840, train_loss 0.686\n",
      "epoch 34, batch 850, train_loss 0.688\n",
      "epoch 34, batch 860, train_loss 0.686\n",
      "epoch 34, batch 870, train_loss 0.694\n",
      "epoch 34, batch 880, train_loss 0.694\n",
      "epoch 34, batch 890, train_loss 0.689\n",
      "epoch 34, batch 900, train_loss 0.689\n",
      "epoch 34, batch 910, train_loss 0.688\n",
      "epoch 34, batch 920, train_loss 0.698\n",
      "epoch 34, batch 930, train_loss 0.687\n",
      "epoch 34, batch 940, train_loss 0.687\n",
      "epoch 34, batch 950, train_loss 0.690\n",
      "epoch 34, batch 960, train_loss 0.690\n",
      "epoch 34, batch 970, train_loss 0.696\n",
      "epoch 34, batch 980, train_loss 0.693\n",
      "epoch 34, batch 990, train_loss 0.703\n",
      "epoch 34, batch 1000, train_loss 0.687\n",
      "epoch 34, batch 1010, train_loss 0.710\n",
      "epoch 34, batch 1020, train_loss 0.702\n",
      "epoch 34, batch 1030, train_loss 0.691\n",
      "epoch 34, batch 1040, train_loss 0.701\n",
      "epoch 34, batch 1050, train_loss 0.686\n",
      "epoch 34, batch 1060, train_loss 0.702\n",
      "epoch 34, batch 1070, train_loss 0.698\n",
      "epoch 34, batch 1080, train_loss 0.686\n",
      "epoch 34, batch 1090, train_loss 0.691\n",
      "epoch 34, batch 1100, train_loss 0.685\n",
      "epoch 34, batch 1110, train_loss 0.693\n",
      "epoch 34, batch 1120, train_loss 0.699\n",
      "epoch 34, batch 1130, train_loss 0.699\n",
      "epoch 34, batch 1140, train_loss 0.700\n",
      "epoch 34, batch 1150, train_loss 0.699\n",
      "epoch 34, batch 1160, train_loss 0.682\n",
      "epoch 34, batch 1170, train_loss 0.699\n",
      "epoch 34, batch 1180, train_loss 0.682\n",
      "epoch 34, batch 1190, train_loss 0.698\n",
      "epoch 34, train_loss 0.693, valid_loss 0.722, train_accuracy  70.24%, valid_accuracy  68.62%\n",
      "epoch 35, batch 0, train_loss 0.698\n",
      "epoch 35, batch 10, train_loss 0.697\n",
      "epoch 35, batch 20, train_loss 0.682\n",
      "epoch 35, batch 30, train_loss 0.705\n",
      "epoch 35, batch 40, train_loss 0.697\n",
      "epoch 35, batch 50, train_loss 0.695\n",
      "epoch 35, batch 60, train_loss 0.680\n",
      "epoch 35, batch 70, train_loss 0.696\n",
      "epoch 35, batch 80, train_loss 0.697\n",
      "epoch 35, batch 90, train_loss 0.692\n",
      "epoch 35, batch 100, train_loss 0.688\n",
      "epoch 35, batch 110, train_loss 0.695\n",
      "epoch 35, batch 120, train_loss 0.698\n",
      "epoch 35, batch 130, train_loss 0.696\n",
      "epoch 35, batch 140, train_loss 0.699\n",
      "epoch 35, batch 150, train_loss 0.689\n",
      "epoch 35, batch 160, train_loss 0.696\n",
      "epoch 35, batch 170, train_loss 0.686\n",
      "epoch 35, batch 180, train_loss 0.694\n",
      "epoch 35, batch 190, train_loss 0.693\n",
      "epoch 35, batch 200, train_loss 0.694\n",
      "epoch 35, batch 210, train_loss 0.693\n",
      "epoch 35, batch 220, train_loss 0.692\n",
      "epoch 35, batch 230, train_loss 0.692\n",
      "epoch 35, batch 240, train_loss 0.697\n",
      "epoch 35, batch 250, train_loss 0.692\n",
      "epoch 35, batch 260, train_loss 0.694\n",
      "epoch 35, batch 270, train_loss 0.689\n",
      "epoch 35, batch 280, train_loss 0.697\n",
      "epoch 35, batch 290, train_loss 0.693\n",
      "epoch 35, batch 300, train_loss 0.702\n",
      "epoch 35, batch 310, train_loss 0.691\n",
      "epoch 35, batch 320, train_loss 0.695\n",
      "epoch 35, batch 330, train_loss 0.689\n",
      "epoch 35, batch 340, train_loss 0.690\n",
      "epoch 35, batch 350, train_loss 0.699\n",
      "epoch 35, batch 360, train_loss 0.694\n",
      "epoch 35, batch 370, train_loss 0.703\n",
      "epoch 35, batch 380, train_loss 0.686\n",
      "epoch 35, batch 390, train_loss 0.689\n",
      "epoch 35, batch 400, train_loss 0.688\n",
      "epoch 35, batch 410, train_loss 0.704\n",
      "epoch 35, batch 420, train_loss 0.688\n",
      "epoch 35, batch 430, train_loss 0.690\n",
      "epoch 35, batch 440, train_loss 0.700\n",
      "epoch 35, batch 450, train_loss 0.700\n",
      "epoch 35, batch 460, train_loss 0.696\n",
      "epoch 35, batch 470, train_loss 0.687\n",
      "epoch 35, batch 480, train_loss 0.692\n",
      "epoch 35, batch 490, train_loss 0.695\n",
      "epoch 35, batch 500, train_loss 0.696\n",
      "epoch 35, batch 510, train_loss 0.692\n",
      "epoch 35, batch 520, train_loss 0.690\n",
      "epoch 35, batch 530, train_loss 0.700\n",
      "epoch 35, batch 540, train_loss 0.688\n",
      "epoch 35, batch 550, train_loss 0.687\n",
      "epoch 35, batch 560, train_loss 0.687\n",
      "epoch 35, batch 570, train_loss 0.697\n",
      "epoch 35, batch 580, train_loss 0.698\n",
      "epoch 35, batch 590, train_loss 0.693\n",
      "epoch 35, batch 600, train_loss 0.692\n",
      "epoch 35, batch 610, train_loss 0.701\n",
      "epoch 35, batch 620, train_loss 0.679\n",
      "epoch 35, batch 630, train_loss 0.690\n",
      "epoch 35, batch 640, train_loss 0.690\n",
      "epoch 35, batch 650, train_loss 0.683\n",
      "epoch 35, batch 660, train_loss 0.689\n",
      "epoch 35, batch 670, train_loss 0.701\n",
      "epoch 35, batch 680, train_loss 0.696\n",
      "epoch 35, batch 690, train_loss 0.687\n",
      "epoch 35, batch 700, train_loss 0.691\n",
      "epoch 35, batch 710, train_loss 0.685\n",
      "epoch 35, batch 720, train_loss 0.688\n",
      "epoch 35, batch 730, train_loss 0.697\n",
      "epoch 35, batch 740, train_loss 0.694\n",
      "epoch 35, batch 750, train_loss 0.691\n",
      "epoch 35, batch 760, train_loss 0.687\n",
      "epoch 35, batch 770, train_loss 0.695\n",
      "epoch 35, batch 780, train_loss 0.684\n",
      "epoch 35, batch 790, train_loss 0.707\n",
      "epoch 35, batch 800, train_loss 0.707\n",
      "epoch 35, batch 810, train_loss 0.695\n",
      "epoch 35, batch 820, train_loss 0.703\n",
      "epoch 35, batch 830, train_loss 0.695\n",
      "epoch 35, batch 840, train_loss 0.696\n",
      "epoch 35, batch 850, train_loss 0.693\n",
      "epoch 35, batch 860, train_loss 0.692\n",
      "epoch 35, batch 870, train_loss 0.697\n",
      "epoch 35, batch 880, train_loss 0.697\n",
      "epoch 35, batch 890, train_loss 0.686\n",
      "epoch 35, batch 900, train_loss 0.690\n",
      "epoch 35, batch 910, train_loss 0.689\n",
      "epoch 35, batch 920, train_loss 0.702\n",
      "epoch 35, batch 930, train_loss 0.698\n",
      "epoch 35, batch 940, train_loss 0.687\n",
      "epoch 35, batch 950, train_loss 0.693\n",
      "epoch 35, batch 960, train_loss 0.686\n",
      "epoch 35, batch 970, train_loss 0.687\n",
      "epoch 35, batch 980, train_loss 0.694\n",
      "epoch 35, batch 990, train_loss 0.696\n",
      "epoch 35, batch 1000, train_loss 0.696\n",
      "epoch 35, batch 1010, train_loss 0.696\n",
      "epoch 35, batch 1020, train_loss 0.697\n",
      "epoch 35, batch 1030, train_loss 0.693\n",
      "epoch 35, batch 1040, train_loss 0.695\n",
      "epoch 35, batch 1050, train_loss 0.693\n",
      "epoch 35, batch 1060, train_loss 0.689\n",
      "epoch 35, batch 1070, train_loss 0.687\n",
      "epoch 35, batch 1080, train_loss 0.704\n",
      "epoch 35, batch 1090, train_loss 0.685\n",
      "epoch 35, batch 1100, train_loss 0.700\n",
      "epoch 35, batch 1110, train_loss 0.686\n",
      "epoch 35, batch 1120, train_loss 0.687\n",
      "epoch 35, batch 1130, train_loss 0.693\n",
      "epoch 35, batch 1140, train_loss 0.691\n",
      "epoch 35, batch 1150, train_loss 0.689\n",
      "epoch 35, batch 1160, train_loss 0.699\n",
      "epoch 35, batch 1170, train_loss 0.705\n",
      "epoch 35, batch 1180, train_loss 0.692\n",
      "epoch 35, batch 1190, train_loss 0.681\n",
      "epoch 35, train_loss 0.692, valid_loss 0.723, train_accuracy  70.25%, valid_accuracy  68.63%\n",
      "model saved with highest valid accuracy:  68.63%\n",
      "epoch 36, batch 0, train_loss 0.694\n",
      "epoch 36, batch 10, train_loss 0.698\n",
      "epoch 36, batch 20, train_loss 0.692\n",
      "epoch 36, batch 30, train_loss 0.694\n",
      "epoch 36, batch 40, train_loss 0.697\n",
      "epoch 36, batch 50, train_loss 0.692\n",
      "epoch 36, batch 60, train_loss 0.694\n",
      "epoch 36, batch 70, train_loss 0.690\n",
      "epoch 36, batch 80, train_loss 0.690\n",
      "epoch 36, batch 90, train_loss 0.697\n",
      "epoch 36, batch 100, train_loss 0.695\n",
      "epoch 36, batch 110, train_loss 0.704\n",
      "epoch 36, batch 120, train_loss 0.696\n",
      "epoch 36, batch 130, train_loss 0.691\n",
      "epoch 36, batch 140, train_loss 0.693\n",
      "epoch 36, batch 150, train_loss 0.697\n",
      "epoch 36, batch 160, train_loss 0.688\n",
      "epoch 36, batch 170, train_loss 0.690\n",
      "epoch 36, batch 180, train_loss 0.707\n",
      "epoch 36, batch 190, train_loss 0.686\n",
      "epoch 36, batch 200, train_loss 0.691\n",
      "epoch 36, batch 210, train_loss 0.696\n",
      "epoch 36, batch 220, train_loss 0.687\n",
      "epoch 36, batch 230, train_loss 0.697\n",
      "epoch 36, batch 240, train_loss 0.688\n",
      "epoch 36, batch 250, train_loss 0.683\n",
      "epoch 36, batch 260, train_loss 0.700\n",
      "epoch 36, batch 270, train_loss 0.694\n",
      "epoch 36, batch 280, train_loss 0.696\n",
      "epoch 36, batch 290, train_loss 0.691\n",
      "epoch 36, batch 300, train_loss 0.687\n",
      "epoch 36, batch 310, train_loss 0.691\n",
      "epoch 36, batch 320, train_loss 0.701\n",
      "epoch 36, batch 330, train_loss 0.695\n",
      "epoch 36, batch 340, train_loss 0.693\n",
      "epoch 36, batch 350, train_loss 0.696\n",
      "epoch 36, batch 360, train_loss 0.689\n",
      "epoch 36, batch 370, train_loss 0.698\n",
      "epoch 36, batch 380, train_loss 0.693\n",
      "epoch 36, batch 390, train_loss 0.698\n",
      "epoch 36, batch 400, train_loss 0.697\n",
      "epoch 36, batch 410, train_loss 0.696\n",
      "epoch 36, batch 420, train_loss 0.693\n",
      "epoch 36, batch 430, train_loss 0.692\n",
      "epoch 36, batch 440, train_loss 0.688\n",
      "epoch 36, batch 450, train_loss 0.689\n",
      "epoch 36, batch 460, train_loss 0.689\n",
      "epoch 36, batch 470, train_loss 0.696\n",
      "epoch 36, batch 480, train_loss 0.696\n",
      "epoch 36, batch 490, train_loss 0.683\n",
      "epoch 36, batch 500, train_loss 0.698\n",
      "epoch 36, batch 510, train_loss 0.694\n",
      "epoch 36, batch 520, train_loss 0.689\n",
      "epoch 36, batch 530, train_loss 0.697\n",
      "epoch 36, batch 540, train_loss 0.684\n",
      "epoch 36, batch 550, train_loss 0.697\n",
      "epoch 36, batch 560, train_loss 0.699\n",
      "epoch 36, batch 570, train_loss 0.695\n",
      "epoch 36, batch 580, train_loss 0.693\n",
      "epoch 36, batch 590, train_loss 0.689\n",
      "epoch 36, batch 600, train_loss 0.697\n",
      "epoch 36, batch 610, train_loss 0.698\n",
      "epoch 36, batch 620, train_loss 0.697\n",
      "epoch 36, batch 630, train_loss 0.695\n",
      "epoch 36, batch 640, train_loss 0.705\n",
      "epoch 36, batch 650, train_loss 0.701\n",
      "epoch 36, batch 660, train_loss 0.695\n",
      "epoch 36, batch 670, train_loss 0.697\n",
      "epoch 36, batch 680, train_loss 0.701\n",
      "epoch 36, batch 690, train_loss 0.693\n",
      "epoch 36, batch 700, train_loss 0.690\n",
      "epoch 36, batch 710, train_loss 0.699\n",
      "epoch 36, batch 720, train_loss 0.691\n",
      "epoch 36, batch 730, train_loss 0.695\n",
      "epoch 36, batch 740, train_loss 0.697\n",
      "epoch 36, batch 750, train_loss 0.701\n",
      "epoch 36, batch 760, train_loss 0.691\n",
      "epoch 36, batch 770, train_loss 0.694\n",
      "epoch 36, batch 780, train_loss 0.703\n",
      "epoch 36, batch 790, train_loss 0.697\n",
      "epoch 36, batch 800, train_loss 0.694\n",
      "epoch 36, batch 810, train_loss 0.709\n",
      "epoch 36, batch 820, train_loss 0.699\n",
      "epoch 36, batch 830, train_loss 0.692\n",
      "epoch 36, batch 840, train_loss 0.693\n",
      "epoch 36, batch 850, train_loss 0.700\n",
      "epoch 36, batch 860, train_loss 0.694\n",
      "epoch 36, batch 870, train_loss 0.691\n",
      "epoch 36, batch 880, train_loss 0.681\n",
      "epoch 36, batch 890, train_loss 0.699\n",
      "epoch 36, batch 900, train_loss 0.692\n",
      "epoch 36, batch 910, train_loss 0.699\n",
      "epoch 36, batch 920, train_loss 0.697\n",
      "epoch 36, batch 930, train_loss 0.684\n",
      "epoch 36, batch 940, train_loss 0.697\n",
      "epoch 36, batch 950, train_loss 0.683\n",
      "epoch 36, batch 960, train_loss 0.689\n",
      "epoch 36, batch 970, train_loss 0.693\n",
      "epoch 36, batch 980, train_loss 0.690\n",
      "epoch 36, batch 990, train_loss 0.699\n",
      "epoch 36, batch 1000, train_loss 0.695\n",
      "epoch 36, batch 1010, train_loss 0.684\n",
      "epoch 36, batch 1020, train_loss 0.690\n",
      "epoch 36, batch 1030, train_loss 0.693\n",
      "epoch 36, batch 1040, train_loss 0.695\n",
      "epoch 36, batch 1050, train_loss 0.694\n",
      "epoch 36, batch 1060, train_loss 0.696\n",
      "epoch 36, batch 1070, train_loss 0.690\n",
      "epoch 36, batch 1080, train_loss 0.691\n",
      "epoch 36, batch 1090, train_loss 0.708\n",
      "epoch 36, batch 1100, train_loss 0.701\n",
      "epoch 36, batch 1110, train_loss 0.689\n",
      "epoch 36, batch 1120, train_loss 0.691\n",
      "epoch 36, batch 1130, train_loss 0.685\n",
      "epoch 36, batch 1140, train_loss 0.700\n",
      "epoch 36, batch 1150, train_loss 0.690\n",
      "epoch 36, batch 1160, train_loss 0.688\n",
      "epoch 36, batch 1170, train_loss 0.696\n",
      "epoch 36, batch 1180, train_loss 0.694\n",
      "epoch 36, batch 1190, train_loss 0.688\n",
      "epoch 36, train_loss 0.692, valid_loss 0.723, train_accuracy  70.23%, valid_accuracy  68.57%\n",
      "epoch 37, batch 0, train_loss 0.689\n",
      "epoch 37, batch 10, train_loss 0.694\n",
      "epoch 37, batch 20, train_loss 0.700\n",
      "epoch 37, batch 30, train_loss 0.688\n",
      "epoch 37, batch 40, train_loss 0.705\n",
      "epoch 37, batch 50, train_loss 0.688\n",
      "epoch 37, batch 60, train_loss 0.699\n",
      "epoch 37, batch 70, train_loss 0.686\n",
      "epoch 37, batch 80, train_loss 0.689\n",
      "epoch 37, batch 90, train_loss 0.690\n",
      "epoch 37, batch 100, train_loss 0.694\n",
      "epoch 37, batch 110, train_loss 0.700\n",
      "epoch 37, batch 120, train_loss 0.698\n",
      "epoch 37, batch 130, train_loss 0.686\n",
      "epoch 37, batch 140, train_loss 0.700\n",
      "epoch 37, batch 150, train_loss 0.700\n",
      "epoch 37, batch 160, train_loss 0.697\n",
      "epoch 37, batch 170, train_loss 0.683\n",
      "epoch 37, batch 180, train_loss 0.693\n",
      "epoch 37, batch 190, train_loss 0.684\n",
      "epoch 37, batch 200, train_loss 0.693\n",
      "epoch 37, batch 210, train_loss 0.692\n",
      "epoch 37, batch 220, train_loss 0.688\n",
      "epoch 37, batch 230, train_loss 0.696\n",
      "epoch 37, batch 240, train_loss 0.689\n",
      "epoch 37, batch 250, train_loss 0.685\n",
      "epoch 37, batch 260, train_loss 0.696\n",
      "epoch 37, batch 270, train_loss 0.689\n",
      "epoch 37, batch 280, train_loss 0.684\n",
      "epoch 37, batch 290, train_loss 0.696\n",
      "epoch 37, batch 300, train_loss 0.695\n",
      "epoch 37, batch 310, train_loss 0.691\n",
      "epoch 37, batch 320, train_loss 0.701\n",
      "epoch 37, batch 330, train_loss 0.693\n",
      "epoch 37, batch 340, train_loss 0.697\n",
      "epoch 37, batch 350, train_loss 0.697\n",
      "epoch 37, batch 360, train_loss 0.685\n",
      "epoch 37, batch 370, train_loss 0.693\n",
      "epoch 37, batch 380, train_loss 0.698\n",
      "epoch 37, batch 390, train_loss 0.697\n",
      "epoch 37, batch 400, train_loss 0.688\n",
      "epoch 37, batch 410, train_loss 0.694\n",
      "epoch 37, batch 420, train_loss 0.699\n",
      "epoch 37, batch 430, train_loss 0.690\n",
      "epoch 37, batch 440, train_loss 0.695\n",
      "epoch 37, batch 450, train_loss 0.695\n",
      "epoch 37, batch 460, train_loss 0.688\n",
      "epoch 37, batch 470, train_loss 0.703\n",
      "epoch 37, batch 480, train_loss 0.697\n",
      "epoch 37, batch 490, train_loss 0.688\n",
      "epoch 37, batch 500, train_loss 0.698\n",
      "epoch 37, batch 510, train_loss 0.688\n",
      "epoch 37, batch 520, train_loss 0.690\n",
      "epoch 37, batch 530, train_loss 0.685\n",
      "epoch 37, batch 540, train_loss 0.697\n",
      "epoch 37, batch 550, train_loss 0.687\n",
      "epoch 37, batch 560, train_loss 0.689\n",
      "epoch 37, batch 570, train_loss 0.691\n",
      "epoch 37, batch 580, train_loss 0.690\n",
      "epoch 37, batch 590, train_loss 0.690\n",
      "epoch 37, batch 600, train_loss 0.684\n",
      "epoch 37, batch 610, train_loss 0.689\n",
      "epoch 37, batch 620, train_loss 0.691\n",
      "epoch 37, batch 630, train_loss 0.695\n",
      "epoch 37, batch 640, train_loss 0.689\n",
      "epoch 37, batch 650, train_loss 0.696\n",
      "epoch 37, batch 660, train_loss 0.692\n",
      "epoch 37, batch 670, train_loss 0.690\n",
      "epoch 37, batch 680, train_loss 0.696\n",
      "epoch 37, batch 690, train_loss 0.694\n",
      "epoch 37, batch 700, train_loss 0.697\n",
      "epoch 37, batch 710, train_loss 0.687\n",
      "epoch 37, batch 720, train_loss 0.697\n",
      "epoch 37, batch 730, train_loss 0.690\n",
      "epoch 37, batch 740, train_loss 0.691\n",
      "epoch 37, batch 750, train_loss 0.705\n",
      "epoch 37, batch 760, train_loss 0.692\n",
      "epoch 37, batch 770, train_loss 0.687\n",
      "epoch 37, batch 780, train_loss 0.698\n",
      "epoch 37, batch 790, train_loss 0.702\n",
      "epoch 37, batch 800, train_loss 0.688\n",
      "epoch 37, batch 810, train_loss 0.696\n",
      "epoch 37, batch 820, train_loss 0.688\n",
      "epoch 37, batch 830, train_loss 0.694\n",
      "epoch 37, batch 840, train_loss 0.684\n",
      "epoch 37, batch 850, train_loss 0.694\n",
      "epoch 37, batch 860, train_loss 0.691\n",
      "epoch 37, batch 870, train_loss 0.692\n",
      "epoch 37, batch 880, train_loss 0.701\n",
      "epoch 37, batch 890, train_loss 0.690\n",
      "epoch 37, batch 900, train_loss 0.690\n",
      "epoch 37, batch 910, train_loss 0.692\n",
      "epoch 37, batch 920, train_loss 0.684\n",
      "epoch 37, batch 930, train_loss 0.683\n",
      "epoch 37, batch 940, train_loss 0.691\n",
      "epoch 37, batch 950, train_loss 0.696\n",
      "epoch 37, batch 960, train_loss 0.700\n",
      "epoch 37, batch 970, train_loss 0.691\n",
      "epoch 37, batch 980, train_loss 0.687\n",
      "epoch 37, batch 990, train_loss 0.699\n",
      "epoch 37, batch 1000, train_loss 0.700\n",
      "epoch 37, batch 1010, train_loss 0.700\n",
      "epoch 37, batch 1020, train_loss 0.710\n",
      "epoch 37, batch 1030, train_loss 0.689\n",
      "epoch 37, batch 1040, train_loss 0.694\n",
      "epoch 37, batch 1050, train_loss 0.690\n",
      "epoch 37, batch 1060, train_loss 0.696\n",
      "epoch 37, batch 1070, train_loss 0.692\n",
      "epoch 37, batch 1080, train_loss 0.692\n",
      "epoch 37, batch 1090, train_loss 0.688\n",
      "epoch 37, batch 1100, train_loss 0.696\n",
      "epoch 37, batch 1110, train_loss 0.695\n",
      "epoch 37, batch 1120, train_loss 0.694\n",
      "epoch 37, batch 1130, train_loss 0.685\n",
      "epoch 37, batch 1140, train_loss 0.696\n",
      "epoch 37, batch 1150, train_loss 0.692\n",
      "epoch 37, batch 1160, train_loss 0.692\n",
      "epoch 37, batch 1170, train_loss 0.691\n",
      "epoch 37, batch 1180, train_loss 0.691\n",
      "epoch 37, batch 1190, train_loss 0.707\n",
      "epoch 37, train_loss 0.692, valid_loss 0.722, train_accuracy  70.26%, valid_accuracy  68.60%\n",
      "epoch 38, batch 0, train_loss 0.685\n",
      "epoch 38, batch 10, train_loss 0.682\n",
      "epoch 38, batch 20, train_loss 0.690\n",
      "epoch 38, batch 30, train_loss 0.693\n",
      "epoch 38, batch 40, train_loss 0.703\n",
      "epoch 38, batch 50, train_loss 0.688\n",
      "epoch 38, batch 60, train_loss 0.698\n",
      "epoch 38, batch 70, train_loss 0.689\n",
      "epoch 38, batch 80, train_loss 0.686\n",
      "epoch 38, batch 90, train_loss 0.693\n",
      "epoch 38, batch 100, train_loss 0.695\n",
      "epoch 38, batch 110, train_loss 0.694\n",
      "epoch 38, batch 120, train_loss 0.686\n",
      "epoch 38, batch 130, train_loss 0.684\n",
      "epoch 38, batch 140, train_loss 0.692\n",
      "epoch 38, batch 150, train_loss 0.693\n",
      "epoch 38, batch 160, train_loss 0.703\n",
      "epoch 38, batch 170, train_loss 0.693\n",
      "epoch 38, batch 180, train_loss 0.694\n",
      "epoch 38, batch 190, train_loss 0.686\n",
      "epoch 38, batch 200, train_loss 0.697\n",
      "epoch 38, batch 210, train_loss 0.693\n",
      "epoch 38, batch 220, train_loss 0.688\n",
      "epoch 38, batch 230, train_loss 0.697\n",
      "epoch 38, batch 240, train_loss 0.693\n",
      "epoch 38, batch 250, train_loss 0.697\n",
      "epoch 38, batch 260, train_loss 0.691\n",
      "epoch 38, batch 270, train_loss 0.686\n",
      "epoch 38, batch 280, train_loss 0.687\n",
      "epoch 38, batch 290, train_loss 0.697\n",
      "epoch 38, batch 300, train_loss 0.698\n",
      "epoch 38, batch 310, train_loss 0.698\n",
      "epoch 38, batch 320, train_loss 0.700\n",
      "epoch 38, batch 330, train_loss 0.695\n",
      "epoch 38, batch 340, train_loss 0.692\n",
      "epoch 38, batch 350, train_loss 0.700\n",
      "epoch 38, batch 360, train_loss 0.685\n",
      "epoch 38, batch 370, train_loss 0.692\n",
      "epoch 38, batch 380, train_loss 0.693\n",
      "epoch 38, batch 390, train_loss 0.700\n",
      "epoch 38, batch 400, train_loss 0.683\n",
      "epoch 38, batch 410, train_loss 0.697\n",
      "epoch 38, batch 420, train_loss 0.690\n",
      "epoch 38, batch 430, train_loss 0.696\n",
      "epoch 38, batch 440, train_loss 0.692\n",
      "epoch 38, batch 450, train_loss 0.697\n",
      "epoch 38, batch 460, train_loss 0.692\n",
      "epoch 38, batch 470, train_loss 0.692\n",
      "epoch 38, batch 480, train_loss 0.695\n",
      "epoch 38, batch 490, train_loss 0.706\n",
      "epoch 38, batch 500, train_loss 0.693\n",
      "epoch 38, batch 510, train_loss 0.685\n",
      "epoch 38, batch 520, train_loss 0.695\n",
      "epoch 38, batch 530, train_loss 0.694\n",
      "epoch 38, batch 540, train_loss 0.691\n",
      "epoch 38, batch 550, train_loss 0.703\n",
      "epoch 38, batch 560, train_loss 0.697\n",
      "epoch 38, batch 570, train_loss 0.690\n",
      "epoch 38, batch 580, train_loss 0.680\n",
      "epoch 38, batch 590, train_loss 0.702\n",
      "epoch 38, batch 600, train_loss 0.706\n",
      "epoch 38, batch 610, train_loss 0.684\n",
      "epoch 38, batch 620, train_loss 0.695\n",
      "epoch 38, batch 630, train_loss 0.693\n",
      "epoch 38, batch 640, train_loss 0.695\n",
      "epoch 38, batch 650, train_loss 0.681\n",
      "epoch 38, batch 660, train_loss 0.694\n",
      "epoch 38, batch 670, train_loss 0.694\n",
      "epoch 38, batch 680, train_loss 0.704\n",
      "epoch 38, batch 690, train_loss 0.692\n",
      "epoch 38, batch 700, train_loss 0.690\n",
      "epoch 38, batch 710, train_loss 0.696\n",
      "epoch 38, batch 720, train_loss 0.691\n",
      "epoch 38, batch 730, train_loss 0.701\n",
      "epoch 38, batch 740, train_loss 0.698\n",
      "epoch 38, batch 750, train_loss 0.697\n",
      "epoch 38, batch 760, train_loss 0.691\n",
      "epoch 38, batch 770, train_loss 0.690\n",
      "epoch 38, batch 780, train_loss 0.698\n",
      "epoch 38, batch 790, train_loss 0.689\n",
      "epoch 38, batch 800, train_loss 0.692\n",
      "epoch 38, batch 810, train_loss 0.695\n",
      "epoch 38, batch 820, train_loss 0.695\n",
      "epoch 38, batch 830, train_loss 0.689\n",
      "epoch 38, batch 840, train_loss 0.696\n",
      "epoch 38, batch 850, train_loss 0.691\n",
      "epoch 38, batch 860, train_loss 0.704\n",
      "epoch 38, batch 870, train_loss 0.692\n",
      "epoch 38, batch 880, train_loss 0.692\n",
      "epoch 38, batch 890, train_loss 0.685\n",
      "epoch 38, batch 900, train_loss 0.699\n",
      "epoch 38, batch 910, train_loss 0.700\n",
      "epoch 38, batch 920, train_loss 0.676\n",
      "epoch 38, batch 930, train_loss 0.686\n",
      "epoch 38, batch 940, train_loss 0.702\n",
      "epoch 38, batch 950, train_loss 0.704\n",
      "epoch 38, batch 960, train_loss 0.684\n",
      "epoch 38, batch 970, train_loss 0.705\n",
      "epoch 38, batch 980, train_loss 0.694\n",
      "epoch 38, batch 990, train_loss 0.696\n",
      "epoch 38, batch 1000, train_loss 0.694\n",
      "epoch 38, batch 1010, train_loss 0.689\n",
      "epoch 38, batch 1020, train_loss 0.702\n",
      "epoch 38, batch 1030, train_loss 0.689\n",
      "epoch 38, batch 1040, train_loss 0.699\n",
      "epoch 38, batch 1050, train_loss 0.695\n",
      "epoch 38, batch 1060, train_loss 0.691\n",
      "epoch 38, batch 1070, train_loss 0.697\n",
      "epoch 38, batch 1080, train_loss 0.694\n",
      "epoch 38, batch 1090, train_loss 0.694\n",
      "epoch 38, batch 1100, train_loss 0.689\n",
      "epoch 38, batch 1110, train_loss 0.692\n",
      "epoch 38, batch 1120, train_loss 0.697\n",
      "epoch 38, batch 1130, train_loss 0.692\n",
      "epoch 38, batch 1140, train_loss 0.688\n",
      "epoch 38, batch 1150, train_loss 0.695\n",
      "epoch 38, batch 1160, train_loss 0.700\n",
      "epoch 38, batch 1170, train_loss 0.686\n",
      "epoch 38, batch 1180, train_loss 0.689\n",
      "epoch 38, batch 1190, train_loss 0.699\n",
      "epoch 38, train_loss 0.692, valid_loss 0.723, train_accuracy  70.25%, valid_accuracy  68.59%\n",
      "epoch 39, batch 0, train_loss 0.702\n",
      "epoch 39, batch 10, train_loss 0.694\n",
      "epoch 39, batch 20, train_loss 0.690\n",
      "epoch 39, batch 30, train_loss 0.700\n",
      "epoch 39, batch 40, train_loss 0.703\n",
      "epoch 39, batch 50, train_loss 0.689\n",
      "epoch 39, batch 60, train_loss 0.707\n",
      "epoch 39, batch 70, train_loss 0.696\n",
      "epoch 39, batch 80, train_loss 0.689\n",
      "epoch 39, batch 90, train_loss 0.698\n",
      "epoch 39, batch 100, train_loss 0.681\n",
      "epoch 39, batch 110, train_loss 0.693\n",
      "epoch 39, batch 120, train_loss 0.689\n",
      "epoch 39, batch 130, train_loss 0.702\n",
      "epoch 39, batch 140, train_loss 0.691\n",
      "epoch 39, batch 150, train_loss 0.690\n",
      "epoch 39, batch 160, train_loss 0.694\n",
      "epoch 39, batch 170, train_loss 0.691\n",
      "epoch 39, batch 180, train_loss 0.692\n",
      "epoch 39, batch 190, train_loss 0.702\n",
      "epoch 39, batch 200, train_loss 0.702\n",
      "epoch 39, batch 210, train_loss 0.699\n",
      "epoch 39, batch 220, train_loss 0.689\n",
      "epoch 39, batch 230, train_loss 0.688\n",
      "epoch 39, batch 240, train_loss 0.689\n",
      "epoch 39, batch 250, train_loss 0.689\n",
      "epoch 39, batch 260, train_loss 0.698\n",
      "epoch 39, batch 270, train_loss 0.690\n",
      "epoch 39, batch 280, train_loss 0.689\n",
      "epoch 39, batch 290, train_loss 0.677\n",
      "epoch 39, batch 300, train_loss 0.693\n",
      "epoch 39, batch 310, train_loss 0.700\n",
      "epoch 39, batch 320, train_loss 0.690\n",
      "epoch 39, batch 330, train_loss 0.696\n",
      "epoch 39, batch 340, train_loss 0.694\n",
      "epoch 39, batch 350, train_loss 0.701\n",
      "epoch 39, batch 360, train_loss 0.702\n",
      "epoch 39, batch 370, train_loss 0.697\n",
      "epoch 39, batch 380, train_loss 0.699\n",
      "epoch 39, batch 390, train_loss 0.690\n",
      "epoch 39, batch 400, train_loss 0.700\n",
      "epoch 39, batch 410, train_loss 0.698\n",
      "epoch 39, batch 420, train_loss 0.694\n",
      "epoch 39, batch 430, train_loss 0.689\n",
      "epoch 39, batch 440, train_loss 0.689\n",
      "epoch 39, batch 450, train_loss 0.697\n",
      "epoch 39, batch 460, train_loss 0.692\n",
      "epoch 39, batch 470, train_loss 0.696\n",
      "epoch 39, batch 480, train_loss 0.697\n",
      "epoch 39, batch 490, train_loss 0.687\n",
      "epoch 39, batch 500, train_loss 0.691\n",
      "epoch 39, batch 510, train_loss 0.694\n",
      "epoch 39, batch 520, train_loss 0.706\n",
      "epoch 39, batch 530, train_loss 0.694\n",
      "epoch 39, batch 540, train_loss 0.690\n",
      "epoch 39, batch 550, train_loss 0.698\n",
      "epoch 39, batch 560, train_loss 0.687\n",
      "epoch 39, batch 570, train_loss 0.696\n",
      "epoch 39, batch 580, train_loss 0.685\n",
      "epoch 39, batch 590, train_loss 0.696\n",
      "epoch 39, batch 600, train_loss 0.696\n",
      "epoch 39, batch 610, train_loss 0.701\n",
      "epoch 39, batch 620, train_loss 0.693\n",
      "epoch 39, batch 630, train_loss 0.689\n",
      "epoch 39, batch 640, train_loss 0.693\n",
      "epoch 39, batch 650, train_loss 0.702\n",
      "epoch 39, batch 660, train_loss 0.679\n",
      "epoch 39, batch 670, train_loss 0.700\n",
      "epoch 39, batch 680, train_loss 0.694\n",
      "epoch 39, batch 690, train_loss 0.690\n",
      "epoch 39, batch 700, train_loss 0.699\n",
      "epoch 39, batch 710, train_loss 0.683\n",
      "epoch 39, batch 720, train_loss 0.681\n",
      "epoch 39, batch 730, train_loss 0.693\n",
      "epoch 39, batch 740, train_loss 0.697\n",
      "epoch 39, batch 750, train_loss 0.696\n",
      "epoch 39, batch 760, train_loss 0.690\n",
      "epoch 39, batch 770, train_loss 0.689\n",
      "epoch 39, batch 780, train_loss 0.682\n",
      "epoch 39, batch 790, train_loss 0.686\n",
      "epoch 39, batch 800, train_loss 0.694\n",
      "epoch 39, batch 810, train_loss 0.684\n",
      "epoch 39, batch 820, train_loss 0.694\n",
      "epoch 39, batch 830, train_loss 0.698\n",
      "epoch 39, batch 840, train_loss 0.687\n",
      "epoch 39, batch 850, train_loss 0.695\n",
      "epoch 39, batch 860, train_loss 0.689\n",
      "epoch 39, batch 870, train_loss 0.694\n",
      "epoch 39, batch 880, train_loss 0.687\n",
      "epoch 39, batch 890, train_loss 0.695\n",
      "epoch 39, batch 900, train_loss 0.693\n",
      "epoch 39, batch 910, train_loss 0.694\n",
      "epoch 39, batch 920, train_loss 0.686\n",
      "epoch 39, batch 930, train_loss 0.680\n",
      "epoch 39, batch 940, train_loss 0.698\n",
      "epoch 39, batch 950, train_loss 0.695\n",
      "epoch 39, batch 960, train_loss 0.687\n",
      "epoch 39, batch 970, train_loss 0.691\n",
      "epoch 39, batch 980, train_loss 0.691\n",
      "epoch 39, batch 990, train_loss 0.692\n",
      "epoch 39, batch 1000, train_loss 0.690\n",
      "epoch 39, batch 1010, train_loss 0.692\n",
      "epoch 39, batch 1020, train_loss 0.695\n",
      "epoch 39, batch 1030, train_loss 0.696\n",
      "epoch 39, batch 1040, train_loss 0.699\n",
      "epoch 39, batch 1050, train_loss 0.692\n",
      "epoch 39, batch 1060, train_loss 0.691\n",
      "epoch 39, batch 1070, train_loss 0.693\n",
      "epoch 39, batch 1080, train_loss 0.703\n",
      "epoch 39, batch 1090, train_loss 0.695\n",
      "epoch 39, batch 1100, train_loss 0.683\n",
      "epoch 39, batch 1110, train_loss 0.689\n",
      "epoch 39, batch 1120, train_loss 0.685\n",
      "epoch 39, batch 1130, train_loss 0.696\n",
      "epoch 39, batch 1140, train_loss 0.699\n",
      "epoch 39, batch 1150, train_loss 0.695\n",
      "epoch 39, batch 1160, train_loss 0.688\n",
      "epoch 39, batch 1170, train_loss 0.686\n",
      "epoch 39, batch 1180, train_loss 0.687\n",
      "epoch 39, batch 1190, train_loss 0.697\n",
      "epoch 39, train_loss 0.693, valid_loss 0.724, train_accuracy  70.21%, valid_accuracy  68.58%\n",
      "epoch 40, batch 0, train_loss 0.690\n",
      "epoch 40, batch 10, train_loss 0.691\n",
      "epoch 40, batch 20, train_loss 0.696\n",
      "epoch 40, batch 30, train_loss 0.687\n",
      "epoch 40, batch 40, train_loss 0.690\n",
      "epoch 40, batch 50, train_loss 0.709\n",
      "epoch 40, batch 60, train_loss 0.688\n",
      "epoch 40, batch 70, train_loss 0.690\n",
      "epoch 40, batch 80, train_loss 0.695\n",
      "epoch 40, batch 90, train_loss 0.692\n",
      "epoch 40, batch 100, train_loss 0.697\n",
      "epoch 40, batch 110, train_loss 0.686\n",
      "epoch 40, batch 120, train_loss 0.694\n",
      "epoch 40, batch 130, train_loss 0.694\n",
      "epoch 40, batch 140, train_loss 0.696\n",
      "epoch 40, batch 150, train_loss 0.687\n",
      "epoch 40, batch 160, train_loss 0.695\n",
      "epoch 40, batch 170, train_loss 0.692\n",
      "epoch 40, batch 180, train_loss 0.688\n",
      "epoch 40, batch 190, train_loss 0.693\n",
      "epoch 40, batch 200, train_loss 0.684\n",
      "epoch 40, batch 210, train_loss 0.689\n",
      "epoch 40, batch 220, train_loss 0.699\n",
      "epoch 40, batch 230, train_loss 0.688\n",
      "epoch 40, batch 240, train_loss 0.697\n",
      "epoch 40, batch 250, train_loss 0.687\n",
      "epoch 40, batch 260, train_loss 0.689\n",
      "epoch 40, batch 270, train_loss 0.706\n",
      "epoch 40, batch 280, train_loss 0.694\n",
      "epoch 40, batch 290, train_loss 0.692\n",
      "epoch 40, batch 300, train_loss 0.702\n",
      "epoch 40, batch 310, train_loss 0.697\n",
      "epoch 40, batch 320, train_loss 0.691\n",
      "epoch 40, batch 330, train_loss 0.697\n",
      "epoch 40, batch 340, train_loss 0.695\n",
      "epoch 40, batch 350, train_loss 0.696\n",
      "epoch 40, batch 360, train_loss 0.680\n",
      "epoch 40, batch 370, train_loss 0.686\n",
      "epoch 40, batch 380, train_loss 0.690\n",
      "epoch 40, batch 390, train_loss 0.700\n",
      "epoch 40, batch 400, train_loss 0.691\n",
      "epoch 40, batch 410, train_loss 0.686\n",
      "epoch 40, batch 420, train_loss 0.691\n",
      "epoch 40, batch 430, train_loss 0.698\n",
      "epoch 40, batch 440, train_loss 0.696\n",
      "epoch 40, batch 450, train_loss 0.679\n",
      "epoch 40, batch 460, train_loss 0.705\n",
      "epoch 40, batch 470, train_loss 0.692\n",
      "epoch 40, batch 480, train_loss 0.686\n",
      "epoch 40, batch 490, train_loss 0.697\n",
      "epoch 40, batch 500, train_loss 0.691\n",
      "epoch 40, batch 510, train_loss 0.691\n",
      "epoch 40, batch 520, train_loss 0.696\n",
      "epoch 40, batch 530, train_loss 0.692\n",
      "epoch 40, batch 540, train_loss 0.694\n",
      "epoch 40, batch 550, train_loss 0.688\n",
      "epoch 40, batch 560, train_loss 0.699\n",
      "epoch 40, batch 570, train_loss 0.700\n",
      "epoch 40, batch 580, train_loss 0.688\n",
      "epoch 40, batch 590, train_loss 0.691\n",
      "epoch 40, batch 600, train_loss 0.684\n",
      "epoch 40, batch 610, train_loss 0.689\n",
      "epoch 40, batch 620, train_loss 0.694\n",
      "epoch 40, batch 630, train_loss 0.691\n",
      "epoch 40, batch 640, train_loss 0.698\n",
      "epoch 40, batch 650, train_loss 0.682\n",
      "epoch 40, batch 660, train_loss 0.691\n",
      "epoch 40, batch 670, train_loss 0.703\n",
      "epoch 40, batch 680, train_loss 0.698\n",
      "epoch 40, batch 690, train_loss 0.682\n",
      "epoch 40, batch 700, train_loss 0.690\n",
      "epoch 40, batch 710, train_loss 0.685\n",
      "epoch 40, batch 720, train_loss 0.700\n",
      "epoch 40, batch 730, train_loss 0.698\n",
      "epoch 40, batch 740, train_loss 0.684\n",
      "epoch 40, batch 750, train_loss 0.693\n",
      "epoch 40, batch 760, train_loss 0.682\n",
      "epoch 40, batch 770, train_loss 0.699\n",
      "epoch 40, batch 780, train_loss 0.692\n",
      "epoch 40, batch 790, train_loss 0.696\n",
      "epoch 40, batch 800, train_loss 0.702\n",
      "epoch 40, batch 810, train_loss 0.683\n",
      "epoch 40, batch 820, train_loss 0.699\n",
      "epoch 40, batch 830, train_loss 0.687\n",
      "epoch 40, batch 840, train_loss 0.699\n",
      "epoch 40, batch 850, train_loss 0.695\n",
      "epoch 40, batch 860, train_loss 0.703\n",
      "epoch 40, batch 870, train_loss 0.700\n",
      "epoch 40, batch 880, train_loss 0.691\n",
      "epoch 40, batch 890, train_loss 0.688\n",
      "epoch 40, batch 900, train_loss 0.687\n",
      "epoch 40, batch 910, train_loss 0.688\n",
      "epoch 40, batch 920, train_loss 0.688\n",
      "epoch 40, batch 930, train_loss 0.696\n",
      "epoch 40, batch 940, train_loss 0.685\n",
      "epoch 40, batch 950, train_loss 0.691\n",
      "epoch 40, batch 960, train_loss 0.700\n",
      "epoch 40, batch 970, train_loss 0.694\n",
      "epoch 40, batch 980, train_loss 0.697\n",
      "epoch 40, batch 990, train_loss 0.694\n",
      "epoch 40, batch 1000, train_loss 0.681\n",
      "epoch 40, batch 1010, train_loss 0.697\n",
      "epoch 40, batch 1020, train_loss 0.697\n",
      "epoch 40, batch 1030, train_loss 0.687\n",
      "epoch 40, batch 1040, train_loss 0.699\n",
      "epoch 40, batch 1050, train_loss 0.696\n",
      "epoch 40, batch 1060, train_loss 0.691\n",
      "epoch 40, batch 1070, train_loss 0.695\n",
      "epoch 40, batch 1080, train_loss 0.700\n",
      "epoch 40, batch 1090, train_loss 0.687\n",
      "epoch 40, batch 1100, train_loss 0.690\n",
      "epoch 40, batch 1110, train_loss 0.698\n",
      "epoch 40, batch 1120, train_loss 0.692\n",
      "epoch 40, batch 1130, train_loss 0.689\n",
      "epoch 40, batch 1140, train_loss 0.687\n",
      "epoch 40, batch 1150, train_loss 0.693\n",
      "epoch 40, batch 1160, train_loss 0.697\n",
      "epoch 40, batch 1170, train_loss 0.693\n",
      "epoch 40, batch 1180, train_loss 0.698\n",
      "epoch 40, batch 1190, train_loss 0.694\n",
      "epoch 40, train_loss 0.691, valid_loss 0.723, train_accuracy  70.29%, valid_accuracy  68.62%\n",
      "epoch 41, batch 0, train_loss 0.698\n",
      "epoch 41, batch 10, train_loss 0.687\n",
      "epoch 41, batch 20, train_loss 0.686\n",
      "epoch 41, batch 30, train_loss 0.698\n",
      "epoch 41, batch 40, train_loss 0.693\n",
      "epoch 41, batch 50, train_loss 0.689\n",
      "epoch 41, batch 60, train_loss 0.690\n",
      "epoch 41, batch 70, train_loss 0.693\n",
      "epoch 41, batch 80, train_loss 0.685\n",
      "epoch 41, batch 90, train_loss 0.683\n",
      "epoch 41, batch 100, train_loss 0.694\n",
      "epoch 41, batch 110, train_loss 0.704\n",
      "epoch 41, batch 120, train_loss 0.690\n",
      "epoch 41, batch 130, train_loss 0.694\n",
      "epoch 41, batch 140, train_loss 0.696\n",
      "epoch 41, batch 150, train_loss 0.690\n",
      "epoch 41, batch 160, train_loss 0.693\n",
      "epoch 41, batch 170, train_loss 0.696\n",
      "epoch 41, batch 180, train_loss 0.696\n",
      "epoch 41, batch 190, train_loss 0.690\n",
      "epoch 41, batch 200, train_loss 0.694\n",
      "epoch 41, batch 210, train_loss 0.688\n",
      "epoch 41, batch 220, train_loss 0.698\n",
      "epoch 41, batch 230, train_loss 0.690\n",
      "epoch 41, batch 240, train_loss 0.698\n",
      "epoch 41, batch 250, train_loss 0.695\n",
      "epoch 41, batch 260, train_loss 0.693\n",
      "epoch 41, batch 270, train_loss 0.687\n",
      "epoch 41, batch 280, train_loss 0.697\n",
      "epoch 41, batch 290, train_loss 0.694\n",
      "epoch 41, batch 300, train_loss 0.688\n",
      "epoch 41, batch 310, train_loss 0.694\n",
      "epoch 41, batch 320, train_loss 0.704\n",
      "epoch 41, batch 330, train_loss 0.686\n",
      "epoch 41, batch 340, train_loss 0.688\n",
      "epoch 41, batch 350, train_loss 0.698\n",
      "epoch 41, batch 360, train_loss 0.690\n",
      "epoch 41, batch 370, train_loss 0.695\n",
      "epoch 41, batch 380, train_loss 0.684\n",
      "epoch 41, batch 390, train_loss 0.685\n",
      "epoch 41, batch 400, train_loss 0.688\n",
      "epoch 41, batch 410, train_loss 0.691\n",
      "epoch 41, batch 420, train_loss 0.693\n",
      "epoch 41, batch 430, train_loss 0.698\n",
      "epoch 41, batch 440, train_loss 0.682\n",
      "epoch 41, batch 450, train_loss 0.689\n",
      "epoch 41, batch 460, train_loss 0.696\n",
      "epoch 41, batch 470, train_loss 0.693\n",
      "epoch 41, batch 480, train_loss 0.685\n",
      "epoch 41, batch 490, train_loss 0.699\n",
      "epoch 41, batch 500, train_loss 0.695\n",
      "epoch 41, batch 510, train_loss 0.685\n",
      "epoch 41, batch 520, train_loss 0.694\n",
      "epoch 41, batch 530, train_loss 0.676\n",
      "epoch 41, batch 540, train_loss 0.701\n",
      "epoch 41, batch 550, train_loss 0.688\n",
      "epoch 41, batch 560, train_loss 0.692\n",
      "epoch 41, batch 570, train_loss 0.690\n",
      "epoch 41, batch 580, train_loss 0.690\n",
      "epoch 41, batch 590, train_loss 0.704\n",
      "epoch 41, batch 600, train_loss 0.678\n",
      "epoch 41, batch 610, train_loss 0.692\n",
      "epoch 41, batch 620, train_loss 0.702\n",
      "epoch 41, batch 630, train_loss 0.686\n",
      "epoch 41, batch 640, train_loss 0.692\n",
      "epoch 41, batch 650, train_loss 0.697\n",
      "epoch 41, batch 660, train_loss 0.699\n",
      "epoch 41, batch 670, train_loss 0.701\n",
      "epoch 41, batch 680, train_loss 0.684\n",
      "epoch 41, batch 690, train_loss 0.693\n",
      "epoch 41, batch 700, train_loss 0.691\n",
      "epoch 41, batch 710, train_loss 0.686\n",
      "epoch 41, batch 720, train_loss 0.695\n",
      "epoch 41, batch 730, train_loss 0.693\n",
      "epoch 41, batch 740, train_loss 0.709\n",
      "epoch 41, batch 750, train_loss 0.694\n",
      "epoch 41, batch 760, train_loss 0.687\n",
      "epoch 41, batch 770, train_loss 0.694\n",
      "epoch 41, batch 780, train_loss 0.684\n",
      "epoch 41, batch 790, train_loss 0.699\n",
      "epoch 41, batch 800, train_loss 0.694\n",
      "epoch 41, batch 810, train_loss 0.691\n",
      "epoch 41, batch 820, train_loss 0.699\n",
      "epoch 41, batch 830, train_loss 0.699\n",
      "epoch 41, batch 840, train_loss 0.688\n",
      "epoch 41, batch 850, train_loss 0.700\n",
      "epoch 41, batch 860, train_loss 0.688\n",
      "epoch 41, batch 870, train_loss 0.681\n",
      "epoch 41, batch 880, train_loss 0.687\n",
      "epoch 41, batch 890, train_loss 0.698\n",
      "epoch 41, batch 900, train_loss 0.691\n",
      "epoch 41, batch 910, train_loss 0.693\n",
      "epoch 41, batch 920, train_loss 0.686\n",
      "epoch 41, batch 930, train_loss 0.694\n",
      "epoch 41, batch 940, train_loss 0.706\n",
      "epoch 41, batch 950, train_loss 0.696\n",
      "epoch 41, batch 960, train_loss 0.704\n",
      "epoch 41, batch 970, train_loss 0.686\n",
      "epoch 41, batch 980, train_loss 0.694\n",
      "epoch 41, batch 990, train_loss 0.687\n",
      "epoch 41, batch 1000, train_loss 0.690\n",
      "epoch 41, batch 1010, train_loss 0.693\n",
      "epoch 41, batch 1020, train_loss 0.684\n",
      "epoch 41, batch 1030, train_loss 0.691\n",
      "epoch 41, batch 1040, train_loss 0.691\n",
      "epoch 41, batch 1050, train_loss 0.693\n",
      "epoch 41, batch 1060, train_loss 0.695\n",
      "epoch 41, batch 1070, train_loss 0.685\n",
      "epoch 41, batch 1080, train_loss 0.685\n",
      "epoch 41, batch 1090, train_loss 0.688\n",
      "epoch 41, batch 1100, train_loss 0.693\n",
      "epoch 41, batch 1110, train_loss 0.696\n",
      "epoch 41, batch 1120, train_loss 0.687\n",
      "epoch 41, batch 1130, train_loss 0.693\n",
      "epoch 41, batch 1140, train_loss 0.686\n",
      "epoch 41, batch 1150, train_loss 0.682\n",
      "epoch 41, batch 1160, train_loss 0.688\n",
      "epoch 41, batch 1170, train_loss 0.687\n",
      "epoch 41, batch 1180, train_loss 0.691\n",
      "epoch 41, batch 1190, train_loss 0.696\n",
      "epoch 41, train_loss 0.692, valid_loss 0.723, train_accuracy  70.26%, valid_accuracy  68.58%\n",
      "epoch 42, batch 0, train_loss 0.681\n",
      "epoch 42, batch 10, train_loss 0.685\n",
      "epoch 42, batch 20, train_loss 0.689\n",
      "epoch 42, batch 30, train_loss 0.692\n",
      "epoch 42, batch 40, train_loss 0.685\n",
      "epoch 42, batch 50, train_loss 0.684\n",
      "epoch 42, batch 60, train_loss 0.684\n",
      "epoch 42, batch 70, train_loss 0.695\n",
      "epoch 42, batch 80, train_loss 0.697\n",
      "epoch 42, batch 90, train_loss 0.688\n",
      "epoch 42, batch 100, train_loss 0.687\n",
      "epoch 42, batch 110, train_loss 0.694\n",
      "epoch 42, batch 120, train_loss 0.693\n",
      "epoch 42, batch 130, train_loss 0.693\n",
      "epoch 42, batch 140, train_loss 0.686\n",
      "epoch 42, batch 150, train_loss 0.691\n",
      "epoch 42, batch 160, train_loss 0.688\n",
      "epoch 42, batch 170, train_loss 0.687\n",
      "epoch 42, batch 180, train_loss 0.691\n",
      "epoch 42, batch 190, train_loss 0.691\n",
      "epoch 42, batch 200, train_loss 0.688\n",
      "epoch 42, batch 210, train_loss 0.695\n",
      "epoch 42, batch 220, train_loss 0.705\n",
      "epoch 42, batch 230, train_loss 0.690\n",
      "epoch 42, batch 240, train_loss 0.688\n",
      "epoch 42, batch 250, train_loss 0.691\n",
      "epoch 42, batch 260, train_loss 0.693\n",
      "epoch 42, batch 270, train_loss 0.695\n",
      "epoch 42, batch 280, train_loss 0.697\n",
      "epoch 42, batch 290, train_loss 0.687\n",
      "epoch 42, batch 300, train_loss 0.693\n",
      "epoch 42, batch 310, train_loss 0.687\n",
      "epoch 42, batch 320, train_loss 0.691\n",
      "epoch 42, batch 330, train_loss 0.692\n",
      "epoch 42, batch 340, train_loss 0.695\n",
      "epoch 42, batch 350, train_loss 0.693\n",
      "epoch 42, batch 360, train_loss 0.693\n",
      "epoch 42, batch 370, train_loss 0.686\n",
      "epoch 42, batch 380, train_loss 0.692\n",
      "epoch 42, batch 390, train_loss 0.682\n",
      "epoch 42, batch 400, train_loss 0.700\n",
      "epoch 42, batch 410, train_loss 0.691\n",
      "epoch 42, batch 420, train_loss 0.686\n",
      "epoch 42, batch 430, train_loss 0.687\n",
      "epoch 42, batch 440, train_loss 0.687\n",
      "epoch 42, batch 450, train_loss 0.690\n",
      "epoch 42, batch 460, train_loss 0.686\n",
      "epoch 42, batch 470, train_loss 0.696\n",
      "epoch 42, batch 480, train_loss 0.690\n",
      "epoch 42, batch 490, train_loss 0.692\n",
      "epoch 42, batch 500, train_loss 0.686\n",
      "epoch 42, batch 510, train_loss 0.686\n",
      "epoch 42, batch 520, train_loss 0.693\n",
      "epoch 42, batch 530, train_loss 0.703\n",
      "epoch 42, batch 540, train_loss 0.692\n",
      "epoch 42, batch 550, train_loss 0.691\n",
      "epoch 42, batch 560, train_loss 0.699\n",
      "epoch 42, batch 570, train_loss 0.689\n",
      "epoch 42, batch 580, train_loss 0.707\n",
      "epoch 42, batch 590, train_loss 0.687\n",
      "epoch 42, batch 600, train_loss 0.684\n",
      "epoch 42, batch 610, train_loss 0.704\n",
      "epoch 42, batch 620, train_loss 0.698\n",
      "epoch 42, batch 630, train_loss 0.685\n",
      "epoch 42, batch 640, train_loss 0.701\n",
      "epoch 42, batch 650, train_loss 0.695\n",
      "epoch 42, batch 660, train_loss 0.694\n",
      "epoch 42, batch 670, train_loss 0.698\n",
      "epoch 42, batch 680, train_loss 0.695\n",
      "epoch 42, batch 690, train_loss 0.702\n",
      "epoch 42, batch 700, train_loss 0.699\n",
      "epoch 42, batch 710, train_loss 0.697\n",
      "epoch 42, batch 720, train_loss 0.690\n",
      "epoch 42, batch 730, train_loss 0.690\n",
      "epoch 42, batch 740, train_loss 0.695\n",
      "epoch 42, batch 750, train_loss 0.696\n",
      "epoch 42, batch 760, train_loss 0.688\n",
      "epoch 42, batch 770, train_loss 0.690\n",
      "epoch 42, batch 780, train_loss 0.703\n",
      "epoch 42, batch 790, train_loss 0.687\n",
      "epoch 42, batch 800, train_loss 0.694\n",
      "epoch 42, batch 810, train_loss 0.696\n",
      "epoch 42, batch 820, train_loss 0.693\n",
      "epoch 42, batch 830, train_loss 0.692\n",
      "epoch 42, batch 840, train_loss 0.691\n",
      "epoch 42, batch 850, train_loss 0.680\n",
      "epoch 42, batch 860, train_loss 0.683\n",
      "epoch 42, batch 870, train_loss 0.686\n",
      "epoch 42, batch 880, train_loss 0.692\n",
      "epoch 42, batch 890, train_loss 0.691\n",
      "epoch 42, batch 900, train_loss 0.694\n",
      "epoch 42, batch 910, train_loss 0.684\n",
      "epoch 42, batch 920, train_loss 0.702\n",
      "epoch 42, batch 930, train_loss 0.687\n",
      "epoch 42, batch 940, train_loss 0.692\n",
      "epoch 42, batch 950, train_loss 0.693\n",
      "epoch 42, batch 960, train_loss 0.696\n",
      "epoch 42, batch 970, train_loss 0.699\n",
      "epoch 42, batch 980, train_loss 0.696\n",
      "epoch 42, batch 990, train_loss 0.703\n",
      "epoch 42, batch 1000, train_loss 0.695\n",
      "epoch 42, batch 1010, train_loss 0.699\n",
      "epoch 42, batch 1020, train_loss 0.691\n",
      "epoch 42, batch 1030, train_loss 0.690\n",
      "epoch 42, batch 1040, train_loss 0.693\n",
      "epoch 42, batch 1050, train_loss 0.696\n",
      "epoch 42, batch 1060, train_loss 0.693\n",
      "epoch 42, batch 1070, train_loss 0.686\n",
      "epoch 42, batch 1080, train_loss 0.689\n",
      "epoch 42, batch 1090, train_loss 0.685\n",
      "epoch 42, batch 1100, train_loss 0.701\n",
      "epoch 42, batch 1110, train_loss 0.684\n",
      "epoch 42, batch 1120, train_loss 0.691\n",
      "epoch 42, batch 1130, train_loss 0.688\n",
      "epoch 42, batch 1140, train_loss 0.696\n",
      "epoch 42, batch 1150, train_loss 0.685\n",
      "epoch 42, batch 1160, train_loss 0.704\n",
      "epoch 42, batch 1170, train_loss 0.698\n",
      "epoch 42, batch 1180, train_loss 0.695\n",
      "epoch 42, batch 1190, train_loss 0.682\n",
      "epoch 42, train_loss 0.691, valid_loss 0.723, train_accuracy  70.30%, valid_accuracy  68.55%\n",
      "epoch 43, batch 0, train_loss 0.700\n",
      "epoch 43, batch 10, train_loss 0.686\n",
      "epoch 43, batch 20, train_loss 0.691\n",
      "epoch 43, batch 30, train_loss 0.688\n",
      "epoch 43, batch 40, train_loss 0.695\n",
      "epoch 43, batch 50, train_loss 0.695\n",
      "epoch 43, batch 60, train_loss 0.692\n",
      "epoch 43, batch 70, train_loss 0.690\n",
      "epoch 43, batch 80, train_loss 0.693\n",
      "epoch 43, batch 90, train_loss 0.685\n",
      "epoch 43, batch 100, train_loss 0.689\n",
      "epoch 43, batch 110, train_loss 0.693\n",
      "epoch 43, batch 120, train_loss 0.689\n",
      "epoch 43, batch 130, train_loss 0.694\n",
      "epoch 43, batch 140, train_loss 0.690\n",
      "epoch 43, batch 150, train_loss 0.696\n",
      "epoch 43, batch 160, train_loss 0.697\n",
      "epoch 43, batch 170, train_loss 0.701\n",
      "epoch 43, batch 180, train_loss 0.700\n",
      "epoch 43, batch 190, train_loss 0.691\n",
      "epoch 43, batch 200, train_loss 0.683\n",
      "epoch 43, batch 210, train_loss 0.695\n",
      "epoch 43, batch 220, train_loss 0.690\n",
      "epoch 43, batch 230, train_loss 0.700\n",
      "epoch 43, batch 240, train_loss 0.700\n",
      "epoch 43, batch 250, train_loss 0.692\n",
      "epoch 43, batch 260, train_loss 0.687\n",
      "epoch 43, batch 270, train_loss 0.683\n",
      "epoch 43, batch 280, train_loss 0.696\n",
      "epoch 43, batch 290, train_loss 0.692\n",
      "epoch 43, batch 300, train_loss 0.692\n",
      "epoch 43, batch 310, train_loss 0.689\n",
      "epoch 43, batch 320, train_loss 0.703\n",
      "epoch 43, batch 330, train_loss 0.690\n",
      "epoch 43, batch 340, train_loss 0.690\n",
      "epoch 43, batch 350, train_loss 0.689\n",
      "epoch 43, batch 360, train_loss 0.692\n",
      "epoch 43, batch 370, train_loss 0.688\n",
      "epoch 43, batch 380, train_loss 0.690\n",
      "epoch 43, batch 390, train_loss 0.697\n",
      "epoch 43, batch 400, train_loss 0.686\n",
      "epoch 43, batch 410, train_loss 0.699\n",
      "epoch 43, batch 420, train_loss 0.689\n",
      "epoch 43, batch 430, train_loss 0.696\n",
      "epoch 43, batch 440, train_loss 0.694\n",
      "epoch 43, batch 450, train_loss 0.682\n",
      "epoch 43, batch 460, train_loss 0.687\n",
      "epoch 43, batch 470, train_loss 0.697\n",
      "epoch 43, batch 480, train_loss 0.695\n",
      "epoch 43, batch 490, train_loss 0.692\n",
      "epoch 43, batch 500, train_loss 0.689\n",
      "epoch 43, batch 510, train_loss 0.690\n",
      "epoch 43, batch 520, train_loss 0.684\n",
      "epoch 43, batch 530, train_loss 0.692\n",
      "epoch 43, batch 540, train_loss 0.693\n",
      "epoch 43, batch 550, train_loss 0.701\n",
      "epoch 43, batch 560, train_loss 0.698\n",
      "epoch 43, batch 570, train_loss 0.691\n",
      "epoch 43, batch 580, train_loss 0.689\n",
      "epoch 43, batch 590, train_loss 0.702\n",
      "epoch 43, batch 600, train_loss 0.688\n",
      "epoch 43, batch 610, train_loss 0.694\n",
      "epoch 43, batch 620, train_loss 0.693\n",
      "epoch 43, batch 630, train_loss 0.696\n",
      "epoch 43, batch 640, train_loss 0.686\n",
      "epoch 43, batch 650, train_loss 0.683\n",
      "epoch 43, batch 660, train_loss 0.691\n",
      "epoch 43, batch 670, train_loss 0.694\n",
      "epoch 43, batch 680, train_loss 0.694\n",
      "epoch 43, batch 690, train_loss 0.697\n",
      "epoch 43, batch 700, train_loss 0.687\n",
      "epoch 43, batch 710, train_loss 0.697\n",
      "epoch 43, batch 720, train_loss 0.687\n",
      "epoch 43, batch 730, train_loss 0.681\n",
      "epoch 43, batch 740, train_loss 0.691\n",
      "epoch 43, batch 750, train_loss 0.693\n",
      "epoch 43, batch 760, train_loss 0.688\n",
      "epoch 43, batch 770, train_loss 0.688\n",
      "epoch 43, batch 780, train_loss 0.696\n",
      "epoch 43, batch 790, train_loss 0.690\n",
      "epoch 43, batch 800, train_loss 0.681\n",
      "epoch 43, batch 810, train_loss 0.694\n",
      "epoch 43, batch 820, train_loss 0.701\n",
      "epoch 43, batch 830, train_loss 0.699\n",
      "epoch 43, batch 840, train_loss 0.696\n",
      "epoch 43, batch 850, train_loss 0.697\n",
      "epoch 43, batch 860, train_loss 0.688\n",
      "epoch 43, batch 870, train_loss 0.687\n",
      "epoch 43, batch 880, train_loss 0.703\n",
      "epoch 43, batch 890, train_loss 0.696\n",
      "epoch 43, batch 900, train_loss 0.698\n",
      "epoch 43, batch 910, train_loss 0.691\n",
      "epoch 43, batch 920, train_loss 0.690\n",
      "epoch 43, batch 930, train_loss 0.682\n",
      "epoch 43, batch 940, train_loss 0.687\n",
      "epoch 43, batch 950, train_loss 0.696\n",
      "epoch 43, batch 960, train_loss 0.692\n",
      "epoch 43, batch 970, train_loss 0.680\n",
      "epoch 43, batch 980, train_loss 0.692\n",
      "epoch 43, batch 990, train_loss 0.698\n",
      "epoch 43, batch 1000, train_loss 0.695\n",
      "epoch 43, batch 1010, train_loss 0.692\n",
      "epoch 43, batch 1020, train_loss 0.692\n",
      "epoch 43, batch 1030, train_loss 0.693\n",
      "epoch 43, batch 1040, train_loss 0.695\n",
      "epoch 43, batch 1050, train_loss 0.682\n",
      "epoch 43, batch 1060, train_loss 0.691\n",
      "epoch 43, batch 1070, train_loss 0.690\n",
      "epoch 43, batch 1080, train_loss 0.689\n",
      "epoch 43, batch 1090, train_loss 0.705\n",
      "epoch 43, batch 1100, train_loss 0.690\n",
      "epoch 43, batch 1110, train_loss 0.698\n",
      "epoch 43, batch 1120, train_loss 0.688\n",
      "epoch 43, batch 1130, train_loss 0.690\n",
      "epoch 43, batch 1140, train_loss 0.686\n",
      "epoch 43, batch 1150, train_loss 0.689\n",
      "epoch 43, batch 1160, train_loss 0.690\n",
      "epoch 43, batch 1170, train_loss 0.698\n",
      "epoch 43, batch 1180, train_loss 0.686\n",
      "epoch 43, batch 1190, train_loss 0.695\n",
      "epoch 43, train_loss 0.691, valid_loss 0.723, train_accuracy  70.29%, valid_accuracy  68.56%\n",
      "epoch 44, batch 0, train_loss 0.688\n",
      "epoch 44, batch 10, train_loss 0.686\n",
      "epoch 44, batch 20, train_loss 0.685\n",
      "epoch 44, batch 30, train_loss 0.677\n",
      "epoch 44, batch 40, train_loss 0.698\n",
      "epoch 44, batch 50, train_loss 0.697\n",
      "epoch 44, batch 60, train_loss 0.694\n",
      "epoch 44, batch 70, train_loss 0.698\n",
      "epoch 44, batch 80, train_loss 0.690\n",
      "epoch 44, batch 90, train_loss 0.693\n",
      "epoch 44, batch 100, train_loss 0.694\n",
      "epoch 44, batch 110, train_loss 0.696\n",
      "epoch 44, batch 120, train_loss 0.693\n",
      "epoch 44, batch 130, train_loss 0.692\n",
      "epoch 44, batch 140, train_loss 0.692\n",
      "epoch 44, batch 150, train_loss 0.685\n",
      "epoch 44, batch 160, train_loss 0.690\n",
      "epoch 44, batch 170, train_loss 0.692\n",
      "epoch 44, batch 180, train_loss 0.691\n",
      "epoch 44, batch 190, train_loss 0.690\n",
      "epoch 44, batch 200, train_loss 0.681\n",
      "epoch 44, batch 210, train_loss 0.693\n",
      "epoch 44, batch 220, train_loss 0.696\n",
      "epoch 44, batch 230, train_loss 0.694\n",
      "epoch 44, batch 240, train_loss 0.700\n",
      "epoch 44, batch 250, train_loss 0.689\n",
      "epoch 44, batch 260, train_loss 0.699\n",
      "epoch 44, batch 270, train_loss 0.691\n",
      "epoch 44, batch 280, train_loss 0.683\n",
      "epoch 44, batch 290, train_loss 0.698\n",
      "epoch 44, batch 300, train_loss 0.688\n",
      "epoch 44, batch 310, train_loss 0.695\n",
      "epoch 44, batch 320, train_loss 0.703\n",
      "epoch 44, batch 330, train_loss 0.689\n",
      "epoch 44, batch 340, train_loss 0.691\n",
      "epoch 44, batch 350, train_loss 0.695\n",
      "epoch 44, batch 360, train_loss 0.693\n",
      "epoch 44, batch 370, train_loss 0.706\n",
      "epoch 44, batch 380, train_loss 0.689\n",
      "epoch 44, batch 390, train_loss 0.701\n",
      "epoch 44, batch 400, train_loss 0.694\n",
      "epoch 44, batch 410, train_loss 0.697\n",
      "epoch 44, batch 420, train_loss 0.694\n",
      "epoch 44, batch 430, train_loss 0.686\n",
      "epoch 44, batch 440, train_loss 0.685\n",
      "epoch 44, batch 450, train_loss 0.694\n",
      "epoch 44, batch 460, train_loss 0.693\n",
      "epoch 44, batch 470, train_loss 0.697\n",
      "epoch 44, batch 480, train_loss 0.699\n",
      "epoch 44, batch 490, train_loss 0.693\n",
      "epoch 44, batch 500, train_loss 0.678\n",
      "epoch 44, batch 510, train_loss 0.683\n",
      "epoch 44, batch 520, train_loss 0.692\n",
      "epoch 44, batch 530, train_loss 0.692\n",
      "epoch 44, batch 540, train_loss 0.688\n",
      "epoch 44, batch 550, train_loss 0.686\n",
      "epoch 44, batch 560, train_loss 0.685\n",
      "epoch 44, batch 570, train_loss 0.692\n",
      "epoch 44, batch 580, train_loss 0.690\n",
      "epoch 44, batch 590, train_loss 0.692\n",
      "epoch 44, batch 600, train_loss 0.689\n",
      "epoch 44, batch 610, train_loss 0.693\n",
      "epoch 44, batch 620, train_loss 0.690\n",
      "epoch 44, batch 630, train_loss 0.690\n",
      "epoch 44, batch 640, train_loss 0.694\n",
      "epoch 44, batch 650, train_loss 0.694\n",
      "epoch 44, batch 660, train_loss 0.691\n",
      "epoch 44, batch 670, train_loss 0.701\n",
      "epoch 44, batch 680, train_loss 0.692\n",
      "epoch 44, batch 690, train_loss 0.698\n",
      "epoch 44, batch 700, train_loss 0.692\n",
      "epoch 44, batch 710, train_loss 0.698\n",
      "epoch 44, batch 720, train_loss 0.687\n",
      "epoch 44, batch 730, train_loss 0.693\n",
      "epoch 44, batch 740, train_loss 0.695\n",
      "epoch 44, batch 750, train_loss 0.688\n",
      "epoch 44, batch 760, train_loss 0.698\n",
      "epoch 44, batch 770, train_loss 0.690\n",
      "epoch 44, batch 780, train_loss 0.695\n",
      "epoch 44, batch 790, train_loss 0.691\n",
      "epoch 44, batch 800, train_loss 0.687\n",
      "epoch 44, batch 810, train_loss 0.688\n",
      "epoch 44, batch 820, train_loss 0.697\n",
      "epoch 44, batch 830, train_loss 0.688\n",
      "epoch 44, batch 840, train_loss 0.693\n",
      "epoch 44, batch 850, train_loss 0.690\n",
      "epoch 44, batch 860, train_loss 0.689\n",
      "epoch 44, batch 870, train_loss 0.699\n",
      "epoch 44, batch 880, train_loss 0.696\n",
      "epoch 44, batch 890, train_loss 0.690\n",
      "epoch 44, batch 900, train_loss 0.700\n",
      "epoch 44, batch 910, train_loss 0.695\n",
      "epoch 44, batch 920, train_loss 0.688\n",
      "epoch 44, batch 930, train_loss 0.695\n",
      "epoch 44, batch 940, train_loss 0.696\n",
      "epoch 44, batch 950, train_loss 0.693\n",
      "epoch 44, batch 960, train_loss 0.688\n",
      "epoch 44, batch 970, train_loss 0.691\n",
      "epoch 44, batch 980, train_loss 0.692\n",
      "epoch 44, batch 990, train_loss 0.698\n",
      "epoch 44, batch 1000, train_loss 0.686\n",
      "epoch 44, batch 1010, train_loss 0.694\n",
      "epoch 44, batch 1020, train_loss 0.701\n",
      "epoch 44, batch 1030, train_loss 0.691\n",
      "epoch 44, batch 1040, train_loss 0.683\n",
      "epoch 44, batch 1050, train_loss 0.692\n",
      "epoch 44, batch 1060, train_loss 0.682\n",
      "epoch 44, batch 1070, train_loss 0.703\n",
      "epoch 44, batch 1080, train_loss 0.694\n",
      "epoch 44, batch 1090, train_loss 0.690\n",
      "epoch 44, batch 1100, train_loss 0.689\n",
      "epoch 44, batch 1110, train_loss 0.693\n",
      "epoch 44, batch 1120, train_loss 0.694\n",
      "epoch 44, batch 1130, train_loss 0.685\n",
      "epoch 44, batch 1140, train_loss 0.702\n",
      "epoch 44, batch 1150, train_loss 0.690\n",
      "epoch 44, batch 1160, train_loss 0.694\n",
      "epoch 44, batch 1170, train_loss 0.696\n",
      "epoch 44, batch 1180, train_loss 0.687\n",
      "epoch 44, batch 1190, train_loss 0.702\n",
      "epoch 44, train_loss 0.691, valid_loss 0.723, train_accuracy  70.30%, valid_accuracy  68.60%\n",
      "epoch 45, batch 0, train_loss 0.700\n",
      "epoch 45, batch 10, train_loss 0.694\n",
      "epoch 45, batch 20, train_loss 0.694\n",
      "epoch 45, batch 30, train_loss 0.690\n",
      "epoch 45, batch 40, train_loss 0.693\n",
      "epoch 45, batch 50, train_loss 0.676\n",
      "epoch 45, batch 60, train_loss 0.696\n",
      "epoch 45, batch 70, train_loss 0.686\n",
      "epoch 45, batch 80, train_loss 0.686\n",
      "epoch 45, batch 90, train_loss 0.694\n",
      "epoch 45, batch 100, train_loss 0.687\n",
      "epoch 45, batch 110, train_loss 0.687\n",
      "epoch 45, batch 120, train_loss 0.688\n",
      "epoch 45, batch 130, train_loss 0.698\n",
      "epoch 45, batch 140, train_loss 0.698\n",
      "epoch 45, batch 150, train_loss 0.694\n",
      "epoch 45, batch 160, train_loss 0.689\n",
      "epoch 45, batch 170, train_loss 0.692\n",
      "epoch 45, batch 180, train_loss 0.706\n",
      "epoch 45, batch 190, train_loss 0.695\n",
      "epoch 45, batch 200, train_loss 0.685\n",
      "epoch 45, batch 210, train_loss 0.692\n",
      "epoch 45, batch 220, train_loss 0.696\n",
      "epoch 45, batch 230, train_loss 0.690\n",
      "epoch 45, batch 240, train_loss 0.698\n",
      "epoch 45, batch 250, train_loss 0.690\n",
      "epoch 45, batch 260, train_loss 0.693\n",
      "epoch 45, batch 270, train_loss 0.683\n",
      "epoch 45, batch 280, train_loss 0.694\n",
      "epoch 45, batch 290, train_loss 0.678\n",
      "epoch 45, batch 300, train_loss 0.680\n",
      "epoch 45, batch 310, train_loss 0.691\n",
      "epoch 45, batch 320, train_loss 0.691\n",
      "epoch 45, batch 330, train_loss 0.690\n",
      "epoch 45, batch 340, train_loss 0.685\n",
      "epoch 45, batch 350, train_loss 0.692\n",
      "epoch 45, batch 360, train_loss 0.689\n",
      "epoch 45, batch 370, train_loss 0.690\n",
      "epoch 45, batch 380, train_loss 0.696\n",
      "epoch 45, batch 390, train_loss 0.699\n",
      "epoch 45, batch 400, train_loss 0.697\n",
      "epoch 45, batch 410, train_loss 0.694\n",
      "epoch 45, batch 420, train_loss 0.690\n",
      "epoch 45, batch 430, train_loss 0.683\n",
      "epoch 45, batch 440, train_loss 0.694\n",
      "epoch 45, batch 450, train_loss 0.691\n",
      "epoch 45, batch 460, train_loss 0.685\n",
      "epoch 45, batch 470, train_loss 0.695\n",
      "epoch 45, batch 480, train_loss 0.692\n",
      "epoch 45, batch 490, train_loss 0.699\n",
      "epoch 45, batch 500, train_loss 0.700\n",
      "epoch 45, batch 510, train_loss 0.693\n",
      "epoch 45, batch 520, train_loss 0.687\n",
      "epoch 45, batch 530, train_loss 0.689\n",
      "epoch 45, batch 540, train_loss 0.693\n",
      "epoch 45, batch 550, train_loss 0.682\n",
      "epoch 45, batch 560, train_loss 0.685\n",
      "epoch 45, batch 570, train_loss 0.702\n",
      "epoch 45, batch 580, train_loss 0.684\n",
      "epoch 45, batch 590, train_loss 0.681\n",
      "epoch 45, batch 600, train_loss 0.687\n",
      "epoch 45, batch 610, train_loss 0.692\n",
      "epoch 45, batch 620, train_loss 0.686\n",
      "epoch 45, batch 630, train_loss 0.690\n",
      "epoch 45, batch 640, train_loss 0.700\n",
      "epoch 45, batch 650, train_loss 0.703\n",
      "epoch 45, batch 660, train_loss 0.698\n",
      "epoch 45, batch 670, train_loss 0.683\n",
      "epoch 45, batch 680, train_loss 0.691\n",
      "epoch 45, batch 690, train_loss 0.694\n",
      "epoch 45, batch 700, train_loss 0.695\n",
      "epoch 45, batch 710, train_loss 0.695\n",
      "epoch 45, batch 720, train_loss 0.699\n",
      "epoch 45, batch 730, train_loss 0.698\n",
      "epoch 45, batch 740, train_loss 0.693\n",
      "epoch 45, batch 750, train_loss 0.697\n",
      "epoch 45, batch 760, train_loss 0.698\n",
      "epoch 45, batch 770, train_loss 0.689\n",
      "epoch 45, batch 780, train_loss 0.695\n",
      "epoch 45, batch 790, train_loss 0.693\n",
      "epoch 45, batch 800, train_loss 0.697\n",
      "epoch 45, batch 810, train_loss 0.686\n",
      "epoch 45, batch 820, train_loss 0.697\n",
      "epoch 45, batch 830, train_loss 0.696\n",
      "epoch 45, batch 840, train_loss 0.693\n",
      "epoch 45, batch 850, train_loss 0.689\n",
      "epoch 45, batch 860, train_loss 0.695\n",
      "epoch 45, batch 870, train_loss 0.679\n",
      "epoch 45, batch 880, train_loss 0.683\n",
      "epoch 45, batch 890, train_loss 0.679\n",
      "epoch 45, batch 900, train_loss 0.709\n",
      "epoch 45, batch 910, train_loss 0.698\n",
      "epoch 45, batch 920, train_loss 0.685\n",
      "epoch 45, batch 930, train_loss 0.690\n",
      "epoch 45, batch 940, train_loss 0.682\n",
      "epoch 45, batch 950, train_loss 0.692\n",
      "epoch 45, batch 960, train_loss 0.682\n",
      "epoch 45, batch 970, train_loss 0.691\n",
      "epoch 45, batch 980, train_loss 0.693\n",
      "epoch 45, batch 990, train_loss 0.688\n",
      "epoch 45, batch 1000, train_loss 0.689\n",
      "epoch 45, batch 1010, train_loss 0.695\n",
      "epoch 45, batch 1020, train_loss 0.697\n",
      "epoch 45, batch 1030, train_loss 0.698\n",
      "epoch 45, batch 1040, train_loss 0.693\n",
      "epoch 45, batch 1050, train_loss 0.692\n",
      "epoch 45, batch 1060, train_loss 0.689\n",
      "epoch 45, batch 1070, train_loss 0.687\n",
      "epoch 45, batch 1080, train_loss 0.688\n",
      "epoch 45, batch 1090, train_loss 0.682\n",
      "epoch 45, batch 1100, train_loss 0.691\n",
      "epoch 45, batch 1110, train_loss 0.694\n",
      "epoch 45, batch 1120, train_loss 0.703\n",
      "epoch 45, batch 1130, train_loss 0.688\n",
      "epoch 45, batch 1140, train_loss 0.690\n",
      "epoch 45, batch 1150, train_loss 0.684\n",
      "epoch 45, batch 1160, train_loss 0.693\n",
      "epoch 45, batch 1170, train_loss 0.682\n",
      "epoch 45, batch 1180, train_loss 0.693\n",
      "epoch 45, batch 1190, train_loss 0.692\n",
      "epoch 45, train_loss 0.691, valid_loss 0.723, train_accuracy  70.31%, valid_accuracy  68.58%\n",
      "epoch 46, batch 0, train_loss 0.688\n",
      "epoch 46, batch 10, train_loss 0.685\n",
      "epoch 46, batch 20, train_loss 0.685\n",
      "epoch 46, batch 30, train_loss 0.692\n",
      "epoch 46, batch 40, train_loss 0.687\n",
      "epoch 46, batch 50, train_loss 0.695\n",
      "epoch 46, batch 60, train_loss 0.694\n",
      "epoch 46, batch 70, train_loss 0.701\n",
      "epoch 46, batch 80, train_loss 0.693\n",
      "epoch 46, batch 90, train_loss 0.682\n",
      "epoch 46, batch 100, train_loss 0.694\n",
      "epoch 46, batch 110, train_loss 0.684\n",
      "epoch 46, batch 120, train_loss 0.696\n",
      "epoch 46, batch 130, train_loss 0.697\n",
      "epoch 46, batch 140, train_loss 0.706\n",
      "epoch 46, batch 150, train_loss 0.697\n",
      "epoch 46, batch 160, train_loss 0.691\n",
      "epoch 46, batch 170, train_loss 0.688\n",
      "epoch 46, batch 180, train_loss 0.692\n",
      "epoch 46, batch 190, train_loss 0.694\n",
      "epoch 46, batch 200, train_loss 0.692\n",
      "epoch 46, batch 210, train_loss 0.694\n",
      "epoch 46, batch 220, train_loss 0.697\n",
      "epoch 46, batch 230, train_loss 0.695\n",
      "epoch 46, batch 240, train_loss 0.691\n",
      "epoch 46, batch 250, train_loss 0.683\n",
      "epoch 46, batch 260, train_loss 0.691\n",
      "epoch 46, batch 270, train_loss 0.689\n",
      "epoch 46, batch 280, train_loss 0.694\n",
      "epoch 46, batch 290, train_loss 0.690\n",
      "epoch 46, batch 300, train_loss 0.698\n",
      "epoch 46, batch 310, train_loss 0.695\n",
      "epoch 46, batch 320, train_loss 0.696\n",
      "epoch 46, batch 330, train_loss 0.698\n",
      "epoch 46, batch 340, train_loss 0.696\n",
      "epoch 46, batch 350, train_loss 0.694\n",
      "epoch 46, batch 360, train_loss 0.698\n",
      "epoch 46, batch 370, train_loss 0.689\n",
      "epoch 46, batch 380, train_loss 0.693\n",
      "epoch 46, batch 390, train_loss 0.693\n",
      "epoch 46, batch 400, train_loss 0.692\n",
      "epoch 46, batch 410, train_loss 0.685\n",
      "epoch 46, batch 420, train_loss 0.687\n",
      "epoch 46, batch 430, train_loss 0.702\n",
      "epoch 46, batch 440, train_loss 0.694\n",
      "epoch 46, batch 450, train_loss 0.683\n",
      "epoch 46, batch 460, train_loss 0.691\n",
      "epoch 46, batch 470, train_loss 0.686\n",
      "epoch 46, batch 480, train_loss 0.699\n",
      "epoch 46, batch 490, train_loss 0.691\n",
      "epoch 46, batch 500, train_loss 0.696\n",
      "epoch 46, batch 510, train_loss 0.705\n",
      "epoch 46, batch 520, train_loss 0.693\n",
      "epoch 46, batch 530, train_loss 0.686\n",
      "epoch 46, batch 540, train_loss 0.694\n",
      "epoch 46, batch 550, train_loss 0.688\n",
      "epoch 46, batch 560, train_loss 0.697\n",
      "epoch 46, batch 570, train_loss 0.700\n",
      "epoch 46, batch 580, train_loss 0.692\n",
      "epoch 46, batch 590, train_loss 0.696\n",
      "epoch 46, batch 600, train_loss 0.697\n",
      "epoch 46, batch 610, train_loss 0.694\n",
      "epoch 46, batch 620, train_loss 0.685\n",
      "epoch 46, batch 630, train_loss 0.693\n",
      "epoch 46, batch 640, train_loss 0.690\n",
      "epoch 46, batch 650, train_loss 0.694\n",
      "epoch 46, batch 660, train_loss 0.689\n",
      "epoch 46, batch 670, train_loss 0.693\n",
      "epoch 46, batch 680, train_loss 0.695\n",
      "epoch 46, batch 690, train_loss 0.692\n",
      "epoch 46, batch 700, train_loss 0.700\n",
      "epoch 46, batch 710, train_loss 0.691\n",
      "epoch 46, batch 720, train_loss 0.692\n",
      "epoch 46, batch 730, train_loss 0.687\n",
      "epoch 46, batch 740, train_loss 0.688\n",
      "epoch 46, batch 750, train_loss 0.681\n",
      "epoch 46, batch 760, train_loss 0.692\n",
      "epoch 46, batch 770, train_loss 0.689\n",
      "epoch 46, batch 780, train_loss 0.694\n",
      "epoch 46, batch 790, train_loss 0.687\n",
      "epoch 46, batch 800, train_loss 0.685\n",
      "epoch 46, batch 810, train_loss 0.684\n",
      "epoch 46, batch 820, train_loss 0.691\n",
      "epoch 46, batch 830, train_loss 0.693\n",
      "epoch 46, batch 840, train_loss 0.694\n",
      "epoch 46, batch 850, train_loss 0.695\n",
      "epoch 46, batch 860, train_loss 0.690\n",
      "epoch 46, batch 870, train_loss 0.695\n",
      "epoch 46, batch 880, train_loss 0.694\n",
      "epoch 46, batch 890, train_loss 0.680\n",
      "epoch 46, batch 900, train_loss 0.696\n",
      "epoch 46, batch 910, train_loss 0.687\n",
      "epoch 46, batch 920, train_loss 0.687\n",
      "epoch 46, batch 930, train_loss 0.697\n",
      "epoch 46, batch 940, train_loss 0.685\n",
      "epoch 46, batch 950, train_loss 0.701\n",
      "epoch 46, batch 960, train_loss 0.692\n",
      "epoch 46, batch 970, train_loss 0.693\n",
      "epoch 46, batch 980, train_loss 0.696\n",
      "epoch 46, batch 990, train_loss 0.691\n",
      "epoch 46, batch 1000, train_loss 0.697\n",
      "epoch 46, batch 1010, train_loss 0.693\n",
      "epoch 46, batch 1020, train_loss 0.702\n",
      "epoch 46, batch 1030, train_loss 0.683\n",
      "epoch 46, batch 1040, train_loss 0.697\n",
      "epoch 46, batch 1050, train_loss 0.695\n",
      "epoch 46, batch 1060, train_loss 0.691\n",
      "epoch 46, batch 1070, train_loss 0.687\n",
      "epoch 46, batch 1080, train_loss 0.702\n",
      "epoch 46, batch 1090, train_loss 0.700\n",
      "epoch 46, batch 1100, train_loss 0.695\n",
      "epoch 46, batch 1110, train_loss 0.692\n",
      "epoch 46, batch 1120, train_loss 0.686\n",
      "epoch 46, batch 1130, train_loss 0.697\n",
      "epoch 46, batch 1140, train_loss 0.696\n",
      "epoch 46, batch 1150, train_loss 0.692\n",
      "epoch 46, batch 1160, train_loss 0.692\n",
      "epoch 46, batch 1170, train_loss 0.693\n",
      "epoch 46, batch 1180, train_loss 0.692\n",
      "epoch 46, batch 1190, train_loss 0.699\n",
      "epoch 46, train_loss 0.692, valid_loss 0.723, train_accuracy  70.28%, valid_accuracy  68.56%\n",
      "epoch 47, batch 0, train_loss 0.684\n",
      "epoch 47, batch 10, train_loss 0.694\n",
      "epoch 47, batch 20, train_loss 0.692\n",
      "epoch 47, batch 30, train_loss 0.692\n",
      "epoch 47, batch 40, train_loss 0.691\n",
      "epoch 47, batch 50, train_loss 0.693\n",
      "epoch 47, batch 60, train_loss 0.696\n",
      "epoch 47, batch 70, train_loss 0.673\n",
      "epoch 47, batch 80, train_loss 0.702\n",
      "epoch 47, batch 90, train_loss 0.693\n",
      "epoch 47, batch 100, train_loss 0.693\n",
      "epoch 47, batch 110, train_loss 0.697\n",
      "epoch 47, batch 120, train_loss 0.687\n",
      "epoch 47, batch 130, train_loss 0.698\n",
      "epoch 47, batch 140, train_loss 0.685\n",
      "epoch 47, batch 150, train_loss 0.696\n",
      "epoch 47, batch 160, train_loss 0.691\n",
      "epoch 47, batch 170, train_loss 0.689\n",
      "epoch 47, batch 180, train_loss 0.691\n",
      "epoch 47, batch 190, train_loss 0.692\n",
      "epoch 47, batch 200, train_loss 0.695\n",
      "epoch 47, batch 210, train_loss 0.696\n",
      "epoch 47, batch 220, train_loss 0.688\n",
      "epoch 47, batch 230, train_loss 0.697\n",
      "epoch 47, batch 240, train_loss 0.696\n",
      "epoch 47, batch 250, train_loss 0.682\n",
      "epoch 47, batch 260, train_loss 0.688\n",
      "epoch 47, batch 270, train_loss 0.685\n",
      "epoch 47, batch 280, train_loss 0.690\n",
      "epoch 47, batch 290, train_loss 0.688\n",
      "epoch 47, batch 300, train_loss 0.700\n",
      "epoch 47, batch 310, train_loss 0.691\n",
      "epoch 47, batch 320, train_loss 0.688\n",
      "epoch 47, batch 330, train_loss 0.690\n",
      "epoch 47, batch 340, train_loss 0.689\n",
      "epoch 47, batch 350, train_loss 0.694\n",
      "epoch 47, batch 360, train_loss 0.699\n",
      "epoch 47, batch 370, train_loss 0.698\n",
      "epoch 47, batch 380, train_loss 0.690\n",
      "epoch 47, batch 390, train_loss 0.692\n",
      "epoch 47, batch 400, train_loss 0.697\n",
      "epoch 47, batch 410, train_loss 0.696\n",
      "epoch 47, batch 420, train_loss 0.701\n",
      "epoch 47, batch 430, train_loss 0.683\n",
      "epoch 47, batch 440, train_loss 0.692\n",
      "epoch 47, batch 450, train_loss 0.700\n",
      "epoch 47, batch 460, train_loss 0.685\n",
      "epoch 47, batch 470, train_loss 0.688\n",
      "epoch 47, batch 480, train_loss 0.694\n",
      "epoch 47, batch 490, train_loss 0.686\n",
      "epoch 47, batch 500, train_loss 0.686\n",
      "epoch 47, batch 510, train_loss 0.696\n",
      "epoch 47, batch 520, train_loss 0.696\n",
      "epoch 47, batch 530, train_loss 0.690\n",
      "epoch 47, batch 540, train_loss 0.682\n",
      "epoch 47, batch 550, train_loss 0.688\n",
      "epoch 47, batch 560, train_loss 0.692\n",
      "epoch 47, batch 570, train_loss 0.684\n",
      "epoch 47, batch 580, train_loss 0.690\n",
      "epoch 47, batch 590, train_loss 0.700\n",
      "epoch 47, batch 600, train_loss 0.689\n",
      "epoch 47, batch 610, train_loss 0.692\n",
      "epoch 47, batch 620, train_loss 0.700\n",
      "epoch 47, batch 630, train_loss 0.695\n",
      "epoch 47, batch 640, train_loss 0.693\n",
      "epoch 47, batch 650, train_loss 0.704\n",
      "epoch 47, batch 660, train_loss 0.690\n",
      "epoch 47, batch 670, train_loss 0.695\n",
      "epoch 47, batch 680, train_loss 0.694\n",
      "epoch 47, batch 690, train_loss 0.696\n",
      "epoch 47, batch 700, train_loss 0.690\n",
      "epoch 47, batch 710, train_loss 0.690\n",
      "epoch 47, batch 720, train_loss 0.688\n",
      "epoch 47, batch 730, train_loss 0.702\n",
      "epoch 47, batch 740, train_loss 0.693\n",
      "epoch 47, batch 750, train_loss 0.689\n",
      "epoch 47, batch 760, train_loss 0.693\n",
      "epoch 47, batch 770, train_loss 0.696\n",
      "epoch 47, batch 780, train_loss 0.702\n",
      "epoch 47, batch 790, train_loss 0.689\n",
      "epoch 47, batch 800, train_loss 0.691\n",
      "epoch 47, batch 810, train_loss 0.690\n",
      "epoch 47, batch 820, train_loss 0.688\n",
      "epoch 47, batch 830, train_loss 0.702\n",
      "epoch 47, batch 840, train_loss 0.685\n",
      "epoch 47, batch 850, train_loss 0.695\n",
      "epoch 47, batch 860, train_loss 0.690\n",
      "epoch 47, batch 870, train_loss 0.689\n",
      "epoch 47, batch 880, train_loss 0.700\n",
      "epoch 47, batch 890, train_loss 0.697\n",
      "epoch 47, batch 900, train_loss 0.695\n",
      "epoch 47, batch 910, train_loss 0.688\n",
      "epoch 47, batch 920, train_loss 0.688\n",
      "epoch 47, batch 930, train_loss 0.694\n",
      "epoch 47, batch 940, train_loss 0.681\n",
      "epoch 47, batch 950, train_loss 0.706\n",
      "epoch 47, batch 960, train_loss 0.691\n",
      "epoch 47, batch 970, train_loss 0.690\n",
      "epoch 47, batch 980, train_loss 0.691\n",
      "epoch 47, batch 990, train_loss 0.687\n",
      "epoch 47, batch 1000, train_loss 0.697\n",
      "epoch 47, batch 1010, train_loss 0.699\n",
      "epoch 47, batch 1020, train_loss 0.699\n",
      "epoch 47, batch 1030, train_loss 0.694\n",
      "epoch 47, batch 1040, train_loss 0.695\n",
      "epoch 47, batch 1050, train_loss 0.698\n",
      "epoch 47, batch 1060, train_loss 0.697\n",
      "epoch 47, batch 1070, train_loss 0.697\n",
      "epoch 47, batch 1080, train_loss 0.697\n",
      "epoch 47, batch 1090, train_loss 0.690\n",
      "epoch 47, batch 1100, train_loss 0.698\n",
      "epoch 47, batch 1110, train_loss 0.685\n",
      "epoch 47, batch 1120, train_loss 0.700\n",
      "epoch 47, batch 1130, train_loss 0.702\n",
      "epoch 47, batch 1140, train_loss 0.698\n",
      "epoch 47, batch 1150, train_loss 0.696\n",
      "epoch 47, batch 1160, train_loss 0.701\n",
      "epoch 47, batch 1170, train_loss 0.693\n",
      "epoch 47, batch 1180, train_loss 0.693\n",
      "epoch 47, batch 1190, train_loss 0.692\n",
      "epoch 47, train_loss 0.692, valid_loss 0.724, train_accuracy  70.28%, valid_accuracy  68.55%\n",
      "epoch 48, batch 0, train_loss 0.675\n",
      "epoch 48, batch 10, train_loss 0.702\n",
      "epoch 48, batch 20, train_loss 0.690\n",
      "epoch 48, batch 30, train_loss 0.692\n",
      "epoch 48, batch 40, train_loss 0.686\n",
      "epoch 48, batch 50, train_loss 0.683\n",
      "epoch 48, batch 60, train_loss 0.690\n",
      "epoch 48, batch 70, train_loss 0.689\n",
      "epoch 48, batch 80, train_loss 0.696\n",
      "epoch 48, batch 90, train_loss 0.685\n",
      "epoch 48, batch 100, train_loss 0.690\n",
      "epoch 48, batch 110, train_loss 0.695\n",
      "epoch 48, batch 120, train_loss 0.690\n",
      "epoch 48, batch 130, train_loss 0.697\n",
      "epoch 48, batch 140, train_loss 0.695\n",
      "epoch 48, batch 150, train_loss 0.683\n",
      "epoch 48, batch 160, train_loss 0.696\n",
      "epoch 48, batch 170, train_loss 0.704\n",
      "epoch 48, batch 180, train_loss 0.673\n",
      "epoch 48, batch 190, train_loss 0.697\n",
      "epoch 48, batch 200, train_loss 0.684\n",
      "epoch 48, batch 210, train_loss 0.685\n",
      "epoch 48, batch 220, train_loss 0.689\n",
      "epoch 48, batch 230, train_loss 0.687\n",
      "epoch 48, batch 240, train_loss 0.699\n",
      "epoch 48, batch 250, train_loss 0.693\n",
      "epoch 48, batch 260, train_loss 0.689\n",
      "epoch 48, batch 270, train_loss 0.692\n",
      "epoch 48, batch 280, train_loss 0.681\n",
      "epoch 48, batch 290, train_loss 0.692\n",
      "epoch 48, batch 300, train_loss 0.693\n",
      "epoch 48, batch 310, train_loss 0.690\n",
      "epoch 48, batch 320, train_loss 0.690\n",
      "epoch 48, batch 330, train_loss 0.694\n",
      "epoch 48, batch 340, train_loss 0.694\n",
      "epoch 48, batch 350, train_loss 0.686\n",
      "epoch 48, batch 360, train_loss 0.696\n",
      "epoch 48, batch 370, train_loss 0.693\n",
      "epoch 48, batch 380, train_loss 0.695\n",
      "epoch 48, batch 390, train_loss 0.692\n",
      "epoch 48, batch 400, train_loss 0.695\n",
      "epoch 48, batch 410, train_loss 0.697\n",
      "epoch 48, batch 420, train_loss 0.689\n",
      "epoch 48, batch 430, train_loss 0.695\n",
      "epoch 48, batch 440, train_loss 0.690\n",
      "epoch 48, batch 450, train_loss 0.686\n",
      "epoch 48, batch 460, train_loss 0.692\n",
      "epoch 48, batch 470, train_loss 0.698\n",
      "epoch 48, batch 480, train_loss 0.698\n",
      "epoch 48, batch 490, train_loss 0.690\n",
      "epoch 48, batch 500, train_loss 0.697\n",
      "epoch 48, batch 510, train_loss 0.697\n",
      "epoch 48, batch 520, train_loss 0.689\n",
      "epoch 48, batch 530, train_loss 0.686\n",
      "epoch 48, batch 540, train_loss 0.692\n",
      "epoch 48, batch 550, train_loss 0.681\n",
      "epoch 48, batch 560, train_loss 0.691\n",
      "epoch 48, batch 570, train_loss 0.693\n",
      "epoch 48, batch 580, train_loss 0.696\n",
      "epoch 48, batch 590, train_loss 0.680\n",
      "epoch 48, batch 600, train_loss 0.697\n",
      "epoch 48, batch 610, train_loss 0.688\n",
      "epoch 48, batch 620, train_loss 0.702\n",
      "epoch 48, batch 630, train_loss 0.692\n",
      "epoch 48, batch 640, train_loss 0.692\n",
      "epoch 48, batch 650, train_loss 0.693\n",
      "epoch 48, batch 660, train_loss 0.691\n",
      "epoch 48, batch 670, train_loss 0.688\n",
      "epoch 48, batch 680, train_loss 0.696\n",
      "epoch 48, batch 690, train_loss 0.690\n",
      "epoch 48, batch 700, train_loss 0.687\n",
      "epoch 48, batch 710, train_loss 0.693\n",
      "epoch 48, batch 720, train_loss 0.695\n",
      "epoch 48, batch 730, train_loss 0.686\n",
      "epoch 48, batch 740, train_loss 0.687\n",
      "epoch 48, batch 750, train_loss 0.692\n",
      "epoch 48, batch 760, train_loss 0.695\n",
      "epoch 48, batch 770, train_loss 0.687\n",
      "epoch 48, batch 780, train_loss 0.693\n",
      "epoch 48, batch 790, train_loss 0.699\n",
      "epoch 48, batch 800, train_loss 0.703\n",
      "epoch 48, batch 810, train_loss 0.698\n",
      "epoch 48, batch 820, train_loss 0.696\n",
      "epoch 48, batch 830, train_loss 0.695\n",
      "epoch 48, batch 840, train_loss 0.698\n",
      "epoch 48, batch 850, train_loss 0.689\n",
      "epoch 48, batch 860, train_loss 0.692\n",
      "epoch 48, batch 870, train_loss 0.687\n",
      "epoch 48, batch 880, train_loss 0.692\n",
      "epoch 48, batch 890, train_loss 0.694\n",
      "epoch 48, batch 900, train_loss 0.691\n",
      "epoch 48, batch 910, train_loss 0.696\n",
      "epoch 48, batch 920, train_loss 0.689\n",
      "epoch 48, batch 930, train_loss 0.698\n",
      "epoch 48, batch 940, train_loss 0.696\n",
      "epoch 48, batch 950, train_loss 0.691\n",
      "epoch 48, batch 960, train_loss 0.692\n",
      "epoch 48, batch 970, train_loss 0.696\n",
      "epoch 48, batch 980, train_loss 0.701\n",
      "epoch 48, batch 990, train_loss 0.696\n",
      "epoch 48, batch 1000, train_loss 0.688\n",
      "epoch 48, batch 1010, train_loss 0.689\n",
      "epoch 48, batch 1020, train_loss 0.681\n",
      "epoch 48, batch 1030, train_loss 0.697\n",
      "epoch 48, batch 1040, train_loss 0.688\n",
      "epoch 48, batch 1050, train_loss 0.687\n",
      "epoch 48, batch 1060, train_loss 0.691\n",
      "epoch 48, batch 1070, train_loss 0.695\n",
      "epoch 48, batch 1080, train_loss 0.698\n",
      "epoch 48, batch 1090, train_loss 0.692\n",
      "epoch 48, batch 1100, train_loss 0.690\n",
      "epoch 48, batch 1110, train_loss 0.688\n",
      "epoch 48, batch 1120, train_loss 0.696\n",
      "epoch 48, batch 1130, train_loss 0.688\n",
      "epoch 48, batch 1140, train_loss 0.690\n",
      "epoch 48, batch 1150, train_loss 0.693\n",
      "epoch 48, batch 1160, train_loss 0.686\n",
      "epoch 48, batch 1170, train_loss 0.687\n",
      "epoch 48, batch 1180, train_loss 0.693\n",
      "epoch 48, batch 1190, train_loss 0.685\n",
      "epoch 48, train_loss 0.691, valid_loss 0.723, train_accuracy  70.29%, valid_accuracy  68.59%\n",
      "epoch 49, batch 0, train_loss 0.690\n",
      "epoch 49, batch 10, train_loss 0.694\n",
      "epoch 49, batch 20, train_loss 0.690\n",
      "epoch 49, batch 30, train_loss 0.685\n",
      "epoch 49, batch 40, train_loss 0.690\n",
      "epoch 49, batch 50, train_loss 0.681\n",
      "epoch 49, batch 60, train_loss 0.696\n",
      "epoch 49, batch 70, train_loss 0.702\n",
      "epoch 49, batch 80, train_loss 0.685\n",
      "epoch 49, batch 90, train_loss 0.686\n",
      "epoch 49, batch 100, train_loss 0.686\n",
      "epoch 49, batch 110, train_loss 0.693\n",
      "epoch 49, batch 120, train_loss 0.688\n",
      "epoch 49, batch 130, train_loss 0.692\n",
      "epoch 49, batch 140, train_loss 0.691\n",
      "epoch 49, batch 150, train_loss 0.691\n",
      "epoch 49, batch 160, train_loss 0.696\n",
      "epoch 49, batch 170, train_loss 0.692\n",
      "epoch 49, batch 180, train_loss 0.688\n",
      "epoch 49, batch 190, train_loss 0.690\n",
      "epoch 49, batch 200, train_loss 0.686\n",
      "epoch 49, batch 210, train_loss 0.691\n",
      "epoch 49, batch 220, train_loss 0.702\n",
      "epoch 49, batch 230, train_loss 0.694\n",
      "epoch 49, batch 240, train_loss 0.695\n",
      "epoch 49, batch 250, train_loss 0.700\n",
      "epoch 49, batch 260, train_loss 0.691\n",
      "epoch 49, batch 270, train_loss 0.690\n",
      "epoch 49, batch 280, train_loss 0.699\n",
      "epoch 49, batch 290, train_loss 0.698\n",
      "epoch 49, batch 300, train_loss 0.693\n",
      "epoch 49, batch 310, train_loss 0.686\n",
      "epoch 49, batch 320, train_loss 0.689\n",
      "epoch 49, batch 330, train_loss 0.690\n",
      "epoch 49, batch 340, train_loss 0.698\n",
      "epoch 49, batch 350, train_loss 0.692\n",
      "epoch 49, batch 360, train_loss 0.685\n",
      "epoch 49, batch 370, train_loss 0.684\n",
      "epoch 49, batch 380, train_loss 0.683\n",
      "epoch 49, batch 390, train_loss 0.684\n",
      "epoch 49, batch 400, train_loss 0.696\n",
      "epoch 49, batch 410, train_loss 0.696\n",
      "epoch 49, batch 420, train_loss 0.688\n",
      "epoch 49, batch 430, train_loss 0.692\n",
      "epoch 49, batch 440, train_loss 0.691\n",
      "epoch 49, batch 450, train_loss 0.693\n",
      "epoch 49, batch 460, train_loss 0.696\n",
      "epoch 49, batch 470, train_loss 0.690\n",
      "epoch 49, batch 480, train_loss 0.691\n",
      "epoch 49, batch 490, train_loss 0.700\n",
      "epoch 49, batch 500, train_loss 0.691\n",
      "epoch 49, batch 510, train_loss 0.693\n",
      "epoch 49, batch 520, train_loss 0.696\n",
      "epoch 49, batch 530, train_loss 0.701\n",
      "epoch 49, batch 540, train_loss 0.694\n",
      "epoch 49, batch 550, train_loss 0.687\n",
      "epoch 49, batch 560, train_loss 0.687\n",
      "epoch 49, batch 570, train_loss 0.693\n",
      "epoch 49, batch 580, train_loss 0.693\n",
      "epoch 49, batch 590, train_loss 0.701\n",
      "epoch 49, batch 600, train_loss 0.685\n",
      "epoch 49, batch 610, train_loss 0.687\n",
      "epoch 49, batch 620, train_loss 0.689\n",
      "epoch 49, batch 630, train_loss 0.696\n",
      "epoch 49, batch 640, train_loss 0.690\n",
      "epoch 49, batch 650, train_loss 0.689\n",
      "epoch 49, batch 660, train_loss 0.692\n",
      "epoch 49, batch 670, train_loss 0.692\n",
      "epoch 49, batch 680, train_loss 0.699\n",
      "epoch 49, batch 690, train_loss 0.695\n",
      "epoch 49, batch 700, train_loss 0.694\n",
      "epoch 49, batch 710, train_loss 0.688\n",
      "epoch 49, batch 720, train_loss 0.684\n",
      "epoch 49, batch 730, train_loss 0.691\n",
      "epoch 49, batch 740, train_loss 0.696\n",
      "epoch 49, batch 750, train_loss 0.695\n",
      "epoch 49, batch 760, train_loss 0.678\n",
      "epoch 49, batch 770, train_loss 0.698\n",
      "epoch 49, batch 780, train_loss 0.692\n",
      "epoch 49, batch 790, train_loss 0.685\n",
      "epoch 49, batch 800, train_loss 0.689\n",
      "epoch 49, batch 810, train_loss 0.692\n",
      "epoch 49, batch 820, train_loss 0.681\n",
      "epoch 49, batch 830, train_loss 0.700\n",
      "epoch 49, batch 840, train_loss 0.693\n",
      "epoch 49, batch 850, train_loss 0.689\n",
      "epoch 49, batch 860, train_loss 0.696\n",
      "epoch 49, batch 870, train_loss 0.688\n",
      "epoch 49, batch 880, train_loss 0.693\n",
      "epoch 49, batch 890, train_loss 0.681\n",
      "epoch 49, batch 900, train_loss 0.690\n",
      "epoch 49, batch 910, train_loss 0.693\n",
      "epoch 49, batch 920, train_loss 0.697\n",
      "epoch 49, batch 930, train_loss 0.691\n",
      "epoch 49, batch 940, train_loss 0.698\n",
      "epoch 49, batch 950, train_loss 0.696\n",
      "epoch 49, batch 960, train_loss 0.706\n",
      "epoch 49, batch 970, train_loss 0.692\n",
      "epoch 49, batch 980, train_loss 0.690\n",
      "epoch 49, batch 990, train_loss 0.684\n",
      "epoch 49, batch 1000, train_loss 0.686\n",
      "epoch 49, batch 1010, train_loss 0.682\n",
      "epoch 49, batch 1020, train_loss 0.692\n",
      "epoch 49, batch 1030, train_loss 0.691\n",
      "epoch 49, batch 1040, train_loss 0.690\n",
      "epoch 49, batch 1050, train_loss 0.696\n",
      "epoch 49, batch 1060, train_loss 0.693\n",
      "epoch 49, batch 1070, train_loss 0.683\n",
      "epoch 49, batch 1080, train_loss 0.681\n",
      "epoch 49, batch 1090, train_loss 0.692\n",
      "epoch 49, batch 1100, train_loss 0.692\n",
      "epoch 49, batch 1110, train_loss 0.686\n",
      "epoch 49, batch 1120, train_loss 0.691\n",
      "epoch 49, batch 1130, train_loss 0.695\n",
      "epoch 49, batch 1140, train_loss 0.690\n",
      "epoch 49, batch 1150, train_loss 0.694\n",
      "epoch 49, batch 1160, train_loss 0.697\n",
      "epoch 49, batch 1170, train_loss 0.692\n",
      "epoch 49, batch 1180, train_loss 0.690\n",
      "epoch 49, batch 1190, train_loss 0.689\n",
      "epoch 49, train_loss 0.691, valid_loss 0.723, train_accuracy  70.30%, valid_accuracy  68.60%\n"
     ]
    }
   ],
   "source": [
    "## re-initialize the model \n",
    "key = jr.PRNGKey(100)\n",
    "model = NeuralNetwork(key)\n",
    "\n",
    "\n",
    "## initialize the optimizer\n",
    "## the learning rate is set to 0.001\n",
    "optim = optax.adamw(0.001)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "\n",
    "## you can change the number of epochs and batch size as needed\n",
    "num_epochs = 50\n",
    "batch_size = 1024 * 16\n",
    "\n",
    "highest_valid_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    idx = np.arange(len(train_xs))\n",
    "    np.random.shuffle(idx)\n",
    "    train_xs = train_xs[idx]\n",
    "    train_ys = train_ys[idx]\n",
    "\n",
    "    num_batches = len(train_xs) // batch_size\n",
    "    for idx_batch in range(num_batches):\n",
    "        start_idx = idx_batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_xs = train_xs[start_idx:end_idx]\n",
    "        batch_ys = train_ys[start_idx:end_idx]\n",
    "\n",
    "        model, opt_state, loss_value = make_step(model, batch_xs, batch_ys, opt_state)\n",
    "\n",
    "        if idx_batch % 10 == 0:\n",
    "            print(f\"epoch {epoch:5>d}, batch {idx_batch:5>d}, train_loss {loss_value:5.3f}\")\n",
    "\n",
    "    \n",
    "    train_loss = compute_average_loss(model, train_xs, train_ys)\n",
    "    train_accuracy = compute_accuracy(model, train_xs, train_ys)\n",
    "\n",
    "    valid_loss = compute_average_loss(model, valid_xs, valid_ys)\n",
    "    valid_accuracy = compute_accuracy(model, valid_xs, valid_ys)\n",
    "    \n",
    "    print(f\"epoch {epoch:5>d}, train_loss {train_loss:5.3f}, valid_loss {valid_loss:5.3f}, train_accuracy {train_accuracy:7.2%}, valid_accuracy {valid_accuracy:7.2%}\")\n",
    "\n",
    "    if valid_accuracy > highest_valid_accuracy:\n",
    "        highest_valid_accuracy = valid_accuracy        \n",
    "        eqx.tree_serialise_leaves(\"../output/model-with-highest-valid-accuracy.eqx\", model)\n",
    "        print(f\"model saved with highest valid accuracy: {highest_valid_accuracy:7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test data\n",
    "After the model is trained, you make predictions on the test sequences using saved model that has the highest validation accuracy. The test sequences are provided in the file `test.txt`. You need to use the trained model to predict the secondary structure of all residues in the test sequences. The predictions should be written to a text file named `predictions.txt` and its format should be the same as the file `train.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model with the highest validation accuracy\n",
    "key = jr.PRNGKey(100)\n",
    "model = NeuralNetwork(key)\n",
    "model = eqx.tree_deserialise_leaves(\"../output/model-with-highest-valid-accuracy.eqx\", model)\n",
    "\n",
    "###########################################################################\n",
    "#### write your code here for the task described above (10 points) ########\n",
    "###########################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "1. You need to create a folder named `assignment-3-protein-secondary-structure-nn` under the OneDrive folder that has been shared with you.\n",
    "2. Complete the code and answer the questions as described above in this Jupyter notebook. Make sure to save your work and name your Jupyter notebook as `main.ipynb`.\n",
    "3. Upload the `main.ipynb`, `pyproject.toml`, and `test_prediction.txt` files to the `assignment-3-protein-secondary-structure-nn` folder that you created in step 1.\n",
    "\n",
    "**Note:** Please make sure the names of the folder and files are exactly as instructed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
